[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/taylorglass/Documents/MADA /taylorglass-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Race                  0             1 FALSE          4\n  top_counts              \n1 M: 4, F: 3, O: 2        \n2 A: 3, B: 3, W: 2, AIA: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  42.8 20.2  16  23  45  51   77 ▆▁▇▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable2.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n Boxplot of Height variable by Race. \n\np5 &lt;- ggplot() + geom_boxplot(data = mydata, aes(x = Race, y = Height)) + \n  labs(title = \"Boxplot summary of height by race\")\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-race-stratified.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\n Note about boxplot of Race variable against Height: there is only one observation of AIAN factor level, reflected by single horizontal line for its boxplot. \n Scatterplot of Age variable against Weight variable. \n\np6 &lt;- ggplot() + geom_point(data = mydata, aes(x = Weight, y = Age)) + geom_smooth(data = mydata, aes(x = Weight, y = Age), method = 'lm') + labs(title = \"Plot of age by weight\")\nplot(p6)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"age-weight.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "",
    "text": "Brian McKay1, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nMark Ebell, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nAriella Perry, Colorado Department of Public Health and Environment, Denver, CO, USA\nYe Shen, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nAndreas Handel1, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\n\n1 Corresponding Authors: Brian McKay and Andreas Handel\nAddress: 101 Buck Rd, Miller Hall, Athens, Georgia 30606\nEmail: bmckay52@uga.edu or ahandel@uga.edu\nPROCEEDINGS OF THE ROYAL SOCIETY B\nDOI: 10.1098/rspb.2020.0496"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#getting-the-files",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#getting-the-files",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Getting the files",
    "text": "Getting the files\nThe files described below are available from Dryad https://doi.org/10.5061/dryad.51c59zw4v.\n\n“Virulence_Trade-off.Rproj” This file lets R know the relative file paths for loading and saving files.\n“SymptomActivity.bib” This file has all of the citation saved as a bibTex.\n“Symptom Questionnaire_Redacted.pdf”: This is a copy of the electronic questionnaire patients with an upper respiratory symptoms were required to fill out. All identifying information has been redacted.\n“DataDictionary_VirulenceTradeOff.xlsx”: This document provides a description of all the variables included in the analysis.\n“1 Anonymized Data” folder contains the de-identified data for the analyses\n\nAn R script that merges all of the individual data sets and creates and saves “Data.Rda” and “Data.csv” in the “1 Anonymized Data” Folder. This script is not included since the raw data sets are not included to protect patient privacy.\n\n“2 Data Cleaning Script” folder contains one R script that cleans the data for the analyses\n\n“Data Merging Script.R”: R script that merges all of the individual data sets and creates and saves “Data.Rda” in the “1 Anonymized Data” Folder. This script is not included since the raw data sets are not included to protect patient privacy.\n“Data Cleaning.R”: This R script does all of the data preparation, creating all the required variables for the analysis. This script also produces the data sets used for the analyses and saves them in the “3 Clean Data” folder.\n\n“4 Analysis Scripts” folder has 4 R scripts that analyze the clean data and produce the results presented in the main text and supplement.\n\n“Flu Symptoms Activity Models.R” This script creates the univariate/multivariate linear regression table, the Spearman rank correlation, and the CMH trend tests. Results are saved in “5 Results”\n“Flu Symptoms Activity Plots.R” This script creates all of the plots. Results are saved in “5 Results”\n“Flu Symptoms Activity Tables.R” This script creates all of the tables. Results are saved in “5 Results”\n“Multivariate Subset Selection.R” This script does the variable selection for the multivariate model. Results are saved in “5 Results”\n\n“6 Manuscript” folder has 2 files in it used to create the manuscript.\n\n“Manuscript.Rmd” This Rmd file creates the basic manuscript word document (formatting will not be identical)\n“proceedings-of-the-royal-society-b.csl” is a style file to format the citations in the manuscript\n\n“7 Supplemental Material” folder has 2 files used to create this document\n\n“Supplemental Material.Rmd” This Rmd file creates the basic supplemental material word document (formatting will not be identical)\n“proceedings-of-the-royal-society-b.csl” is a style file to format the citations in the supplement"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#reproducing-results",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#reproducing-results",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Reproducing results",
    "text": "Reproducing results\nThe files required to reproduce the results are 2 R Markdown files, 4 R script, and one anonymized data file. These files allow the reproduction of all results shown in the main text and SM. To reproduce the results follow these steps.\nFirst, install R, Rstudio, and Pandoc (when you install Rstudio Pandoc should automatically install). Microsoft Word or Open Office Word is also required.\nSecond, save the zip file from Dryad on your local computer. Open the folder and double click “Virulence_Trade-off”. This should open Rstudio (if prompted, select Rstudio as the app to open this file type). Then open and run the files below in the specified order.\n\nR script “Data Cleaning.R” in the “2 Data Cleaning Script” folder uses “Data.Rda” and produces two clean data sets used for all further analyses. The data sets are all saved in the “3 Clean Data” folder and include:\n\n“SympAct_Any_Pos.Rda” Contains data for all influenza patients regardless of diagnosis method.\n“SympAct_Lab_Pos.Rda” Contains data for influenza patients diagnosed based on a PCR or rapid antigen test.\n\n\nIt is important to note that “SympAct_Lab_Pos.Rda” is a subset of “SympAct_Any_Pos.Rda” based on the method of diagnosis.\n\nFour R scripts in the “4 Analysis Scripts” folder (“Flu Symptoms Activity Univariate Model.R”, “Flu Symptoms Activity Univariate Plots.R”, and “Flu Symptoms Activity Univariate Tables.R”, “Multivariate Subset Selection.R”). The order you run these scripts does not matter. Results of each script are automatically saved in the “5 Results” folder.\nR Markdown “Manuscript.Rmd” is in the “6 Manuscript” folder. This combines all the relevant results and creates the main text as a Word document (some reformatting is required).\nR Markdown “Supplemental Material.Rmd” in the “7 Supplemental Material” folder generates the supplementary material as Word document."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#histogram-of-reported-activity-levels",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#histogram-of-reported-activity-levels",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Histogram of reported activity levels",
    "text": "Histogram of reported activity levels\nReported activity levels ranging from 0 to 10 with a median of 4 for those patients with a lab diagnosis of influenza (SM Figure @ref(fig:ActivityLabBarChart))\n\n\n\n\n\nHistogram of reported activity levels for patients with a lab diagnosis of influenza."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#table-of-symptoms",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#table-of-symptoms",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Table of symptoms",
    "text": "Table of symptoms\nSM Table @ref(tab:SympLabTable) shows the symptoms among patients with a lab-based diagnosis.\n\n\n\nSymptoms of the 324 patients with laboratory based flu diagnosis. The table shows the number of patients who reported having the following symptoms and the corresponding percentage.\n\n\n\nOverall\n\n\n\n\nn\n324\n\n\nAbdominal Pain = Yes (%)\n38 (11.7)\n\n\nBreathlessness = Yes (%)\n128 (39.5)\n\n\nChest Congestion = Yes (%)\n194 (59.9)\n\n\nChest Pain = Yes (%)\n110 (34.0)\n\n\nChills/Sweats = Yes (%)\n287 (88.6)\n\n\nCough = Yes (%)\n306 (94.4)\n\n\nDiarrhea = Yes (%)\n40 (12.3)\n\n\nEar Pain = Yes (%)\n59 (18.2)\n\n\nEye Pain = Yes (%)\n48 (14.8)\n\n\nFatigue = Yes (%)\n304 (93.8)\n\n\nHeadache = Yes (%)\n273 (84.3)\n\n\nItchy Eyes = Yes (%)\n75 (23.1)\n\n\nMyalgia = Yes (%)\n290 (89.5)\n\n\nNasal Congestion = Yes (%)\n255 (78.7)\n\n\nNausea = Yes (%)\n119 (36.7)\n\n\nRunny Nose = Yes (%)\n234 (72.2)\n\n\nSleeplessness = Yes (%)\n183 (56.5)\n\n\nSneeze = Yes (%)\n177 (54.6)\n\n\nSore Throat = Yes (%)\n265 (81.8)\n\n\nSubjective Fever = Yes (%)\n242 (74.7)\n\n\nSwollen Lymph Nodes = Yes (%)\n127 (39.2)\n\n\nTooth Pain = Yes (%)\n60 (18.5)\n\n\nVomiting = Yes (%)\n43 (13.3)\n\n\nWeakness = Yes (%)\n306 (94.4)\n\n\nWheezing = Yes (%)\n105 (32.4)"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Univariate and subset selection",
    "text": "Univariate and subset selection\nCorrelations between activity level and each symptom from the univariate linear analysis and the multivariate regression model selected using cross-validation (SM Table @ref(tab:LmActvSympLab)).\n\n\nResults of the univariate and multivariate linear regression of symptoms and activity. The coefficients are the estimated effect on activity when the symptom is present. The multivariate model was selected with a sequential forward floating selection, minimizing the RMSE on test data through a 5-fold cross validation (20 times repeated). 95%CI = The 95% confidence interval for the coefficient.\n\n\n\n\n\n\n\n\n\nDependent: Activity Level\n\nMean (sd)\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nAbdominal Pain\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n3.4 (2.7)\n-1.01 (-1.90 to -0.12, p=0.026)\n-\n\n\nBreathlessness\nNo\n4.3 (2.7)\n-\n-\n\n\n\nYes\n4.1 (2.5)\n-0.21 (-0.81 to 0.38, p=0.477)\n-\n\n\nChest Congestion\nNo\n4.7 (2.9)\n-\n-\n\n\n\nYes\n4.0 (2.4)\n-0.65 (-1.24 to -0.07, p=0.029)\n-\n\n\nChest Pain\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-0.38 (-0.99 to 0.23, p=0.217)\n-\n\n\nChills/Sweats\nNo\n5.8 (2.8)\n-\n-\n\n\n\nYes\n4.1 (2.6)\n-1.75 (-2.64 to -0.86, p&lt;0.001)\n-0.90 (-1.79 to -0.02, p=0.046)\n\n\nCough\nNo\n4.2 (3.2)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n0.10 (-1.16 to 1.36, p=0.875)\n-\n\n\nDiarrhea\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n3.6 (2.6)\n-0.73 (-1.60 to 0.15, p=0.103)\n-\n\n\nEar Pain\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n3.7 (2.7)\n-0.69 (-1.44 to 0.05, p=0.068)\n-\n\n\nEye Pain\nNo\n4.2 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.9)\n0.21 (-0.61 to 1.02, p=0.619)\n-\n\n\nFatigue\nNo\n5.7 (2.5)\n-\n-\n\n\n\nYes\n4.2 (2.6)\n-1.53 (-2.72 to -0.34, p=0.012)\n-\n\n\nHeadache\nNo\n5.6 (2.8)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-1.57 (-2.35 to -0.80, p&lt;0.001)\n-1.23 (-1.97 to -0.49, p=0.001)\n\n\nSleeplessness\nNo\n4.9 (2.7)\n-\n-\n\n\n\nYes\n3.8 (2.5)\n-1.14 (-1.71 to -0.57, p&lt;0.001)\n-0.91 (-1.44 to -0.37, p=0.001)\n\n\nItchy Eyes\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n3.7 (2.5)\n-0.72 (-1.40 to -0.04, p=0.038)\n-\n\n\nMyalgia\nNo\n5.4 (2.9)\n-\n-\n\n\n\nYes\n4.1 (2.6)\n-1.32 (-2.25 to -0.38, p=0.006)\n-\n\n\nNasal Congestion\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.2 (2.6)\n-0.22 (-0.93 to 0.49, p=0.542)\n-\n\n\nNausea\nNo\n4.7 (2.7)\n-\n-\n\n\n\nYes\n3.6 (2.4)\n-1.08 (-1.67 to -0.49, p&lt;0.001)\n-\n\n\nSore Throat\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.7)\n-0.20 (-0.95 to 0.55, p=0.605)\n-\n\n\nRunny Nose\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n4.1 (2.7)\n-0.50 (-1.14 to 0.15, p=0.129)\n-\n\n\nSneeze\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-0.67 (-1.24 to -0.09, p=0.024)\n-\n\n\nSubjective Fever\nNo\n5.2 (2.5)\n-\n-\n\n\n\nYes\n3.9 (2.6)\n-1.25 (-1.90 to -0.60, p&lt;0.001)\n-0.67 (-1.32 to -0.01, p=0.047)\n\n\nSwollen Lymph Nodes\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.46 (-1.05 to 0.13, p=0.128)\n-\n\n\nTooth Pain\nNo\n4.3 (2.6)\n-\n-\n\n\n\nYes\n4.1 (2.7)\n-0.24 (-0.98 to 0.50, p=0.526)\n-\n\n\nVomiting\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n2.8 (2.2)\n-1.64 (-2.48 to -0.81, p&lt;0.001)\n-1.39 (-2.18 to -0.60, p=0.001)\n\n\nWeakness\nNo\n6.6 (2.4)\n-\n-\n\n\n\nYes\n4.1 (2.6)\n-2.49 (-3.72 to -1.25, p&lt;0.001)\n-1.50 (-2.69 to -0.31, p=0.014)\n\n\nWheezing\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.46 (-1.07 to 0.16, p=0.144)\n-"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#distribution-of-scores",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#distribution-of-scores",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Distribution of scores",
    "text": "Distribution of scores\nThe infectiousness score is skewed left with more than half of the patients having a score of 4 or 5 (SM Figure @ref(fig:IandMLabBarChart)A). The morbidity score is more centered with no patient having a score of 0,1,19 or 20 (SM Figure @ref(fig:IandMLabBarChart)B).\n\n\n\n\n\n(A) Histogram of infectiousness score for patients with a lab diagnosis of influenza. (B) Histogram of morbidity score for patients with a lab diagnosis of influenza."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#correlation-between-number-and-severity-of-symptoms",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#correlation-between-number-and-severity-of-symptoms",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Correlation between number and severity of symptoms",
    "text": "Correlation between number and severity of symptoms\nThe data we have available reports most of the symptoms as absence or presence. If more symptoms correlate positively with stronger symptoms, using absence/presence data only is a valid, albeit less powerful approach. More problematically is a situation in which the number and strength of symptoms correlate negatively. For example, a person having fewer symptoms might “make up” for the lower number of symptoms by having a more severe form. Such a relation would invalidate our approach of adding presence/absence symptoms to arrive at a given score. While we cannot thoroughly test potential correlations between the quantity and strength of symptoms on all our data, we do have severity information on the cough, weakness, and body aches symptoms. This allows us to evaluate the relation between the number of symptoms and the severity of those symptoms.\nWe find that as the number of infectiousness-related symptoms (i.e., our infectiousness scores) increases, there is an increase in cough intensity (SM Figure @ref(fig:Infect1CoughFig)). The same relationship is found between the number of morbidity-related symptoms (i.e., our morbidity scores) and the severity of weakness and body aches (SM Figures @ref(fig:Morbidity1WeaknessFig) and @ref(fig:Morbidity1BodyAchesFig)). While not conclusive, this evidence suggests that our assumption that symptom absence/presence also captures symptom severity seems to be defensible.\n\n\n\n\n\nRelationship between cough intensity and the infectiousness score from main text using all Y/N variables plausibly related to infectiousness.\n\n\n\n\n\n\n\n\n\nRelationship between weakness intensity and the morbidity score from main text using all Y/N variables related to morbidity.\n\n\n\n\n\n\n\n\n\nRelationship between body ache intensity and the morbidity score from main text using all Y/N variables related to morbidity."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-the-infectiousness-score",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-the-infectiousness-score",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Alternative approaches to calculate the infectiousness score",
    "text": "Alternative approaches to calculate the infectiousness score\nIn the main text, we use all of the available yes/no symptoms that can feasibly be related to infectiousness. Since there is no consensus about which symptoms or routes of transmission are the most important, we also consider how robust the results are using several different versions of infectiousness scores.\nWhile chest congestion might be a proxy for pathogen load, i.e., more congestion could indicate higher levels of a pathogen, in which case it could impact infectiousness. However, it is true that chest congestion alone without coughing or sneezing might not lead to increased infectiousness (though some recent studies suggest that breathing alone accounts for a large fraction of expelled influenza virions [1]). To explore if inclusion or exclusion of chest congestion in the infectiousness scores made a difference, we created a score excluding it to see if the overall conclusions would change. The new score values ranged from 0 to 4 (SM Figure @ref(fig:InfectScore2Fig)A).\nThe same overall relationship is observed between the new infectiousness score and activity. There is still a curve. Spearman’s rank correlation indicates a negative relationship with (\\(r=\\) -0.11 (95% CI: -0.22, -0.00)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 3.772, \\(df =\\) 1, \\(p\\) 0.05) (SM Figure @ref(fig:InfectScore2Fig)B).\nWhen compared to the morbidity score, the same overall trends from the primary analysis were observed. Spearman’s rank correlation (\\(r=\\) 0.17 (95% CI: 0.06, 0.27)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 11.323, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.5 fold increase in the new infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:InfectScore2Fig)C).\n\n\n\n\n\nInfectiousness score without chest congestion\n\n\n\n\nHow patients define congestion symptoms is very subjective [2], so we created a score excluding both of the congestion-related variables to see if the overall conclusions would change. The new score could have a value of 0 to 3 (SM Figure @ref(fig:InfectScore3Fig)A).\nThe same overall relationship is observed between the new infectiousness score and activity. The curved relationship is not as clear but still present. Spearman’s rank correlation indicates negative relationship with (\\(r=\\) -0.11 (95% CI: -0.22, -0.00)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 4.599, \\(df =\\) 1, \\(p\\) 0.03) (SM Figure @ref(fig:InfectScore3Fig)B).\nWhen compared to the morbidity score again, the same trends were observed, and Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.16 (95% CI: 0.05, 0.26)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 10.023, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.5 fold increase in the new infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:InfectScore3Fig)C).\n\n\n\n\n\nInfectiousness score without chest and nasal congestion variables\n\n\n\n\nFor the score used in the main text, we included all relevant symptoms even if they can be considered strongly related. This might lead to potential double-counting of some symptoms. To evaluate if this might cause problems, we created an alternative score that removed highly correlated variables based on two cut off values of Yule’s Q [3]. There is no commonly used value to define a cut-off at which correlated variables should be removed. We decided to use an absolute correlation of 0.9 or 0.75 for our cut off. When using the 0.9 cut off only very strongly correlated variables were removed, while for the 0.75 even intermediately strong correlations lead to the removal of one of the variables [4]. For any pair of symptoms with a positive or negative correlation of 0.9 or greater, the more informative symptom (having the proportion Yes/No closest to 50%) was kept[5]. For our data, cough and chest congestion had a Yule’s Q of 0.933, and chest congestion had the best balance between yes and no responses, so it was kept. When relaxing the correlation to 0.75, there is no change in the variables included in the score.\nThe score values range from 0 to 4 (SM Figure @ref(fig:InfectScore4Fig)A). The same overall relationship is observed between the new infectiousness score and activity. The curve is present, Spearman’s rank correlation indicates negative relationship with (\\(r=\\) -0.16 (95% CI: -0.26, -0.05)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 7.138, \\(df =\\) 1, \\(p\\) &lt; 0.01) (SM Figure @ref(fig:InfectScore4Fig)B).\nWhen compared to the morbidity score again the same overall trends were observed with Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.28 (95% CI: 0.18, 0.38)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 25.818, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 2.1 fold increase in the new infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:InfectScore4Fig)C).\n\n\n\n\n\nInfectiousness score removing variables determined to be redundant based on Yule’s Q"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-morbidity-score",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-morbidity-score",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Alternative approaches to calculate morbidity score",
    "text": "Alternative approaches to calculate morbidity score\nTo see the impact of removing correlated variables from the morbidity score, we used the same methods applied to the infectiousness score above. Among the morbidity symptoms, only vomiting and weakness correlated greater than 0.9. Vomiting was included in the score since it was more balanced then weakness, which was present in 94% of patients. This score using 0.9 cut off had a possible range of 0 to 19. There are no patients with a morbidity score of 0,1,18,19 (SM Figure @ref(fig:ImpactScore2Fig)A). The same relationships observed in the main text are seen when the morbidity score is compared to activity and infectiousness scores.\nThere is a negative correlation between the morbidity score, and the patient’s self-reported activity level suggests that higher morbidity score is associated with reduced activity levels. Spearman’s rank correlation indicates negative relationship (\\(r=\\) -0.31 (95% CI: -0.41, -0.21)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 36.004, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 4.3 fold decrease in activity level going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore2Fig)B).\nThere is a positive correlation between the morbidity and infectiousness scores show a positive correlation. Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.28 (95% CI: 0.18, 0.38)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 25.942, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.7 fold in the infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore2Fig)C).\n\n\n\n\n\nMorbidity score removing variables determined to be redundant using the absolute value of Yule’s Q of 0.9 or more\n\n\n\n\nThe score created using the 0.75 cut off was different, with a total of 6 symptoms being excluded. Starting with the highest correlations first: Weakness/Vomit (Q=1) keep vomit, Tooth pain/Headache (Q=.86) keep Tooth pain, Chills Sweats/SoreThroat (Q=-.8) keep Chills Sweats, Fatigue/BodyAches (Q=.80) keep BodyAches, Vomit/Nausea (Q=.79) keep Nausea, SubjectiveFever/ChillsSweats (Q=.76) keep SubjectiveFever. The 0.75 cut off morbidity score included Subjective Fever, Myalgia, Sleeplessness, Breathlessness, Wheezing, Chest pain, Abdominal Pain, Diarrhea, Nausea, Ear Pain, Tooth pain, Eye pain, Itchy Eyes, and Swollen Lymph Nodes. This morbidity score has a possible value of 0 to 14. Compared to the morbidity score used in the main text and 0.9 cut off there are now 12 patients with a score of 0 or 1, but there are still no patients with a score of 13 or 14. (SM Figure @ref(fig:ImpactScore3Fig)A). Again the overall results remain the same despite using a different version of the score.\nThere is a negative correlation between the morbidity score and the patient’s self-reported activity level. Spearman’s rank correlation indicates negative relationship (\\(r=\\) -0.26 (95% CI: -0.36, -0.16)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 24.987, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 2.9 fold reduction in activity level going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore3Fig)B).\nThe relationship between the morbidity and infectiousness scores show a positive correlation. Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.29 (95% CI: 0.18, 0.38)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 25.942, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.7 fold increase in the infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore3Fig)C).\n\n\n\n\n\nMorbidity score removing variables determined to be redundant using the absolute value of Yule’s Q of 0.75 or more"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#description-of-the-population",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#description-of-the-population",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Description of the population",
    "text": "Description of the population\nInfluenza diagnosis for our population is determined using three different methods a rapid antigen test, a PCR test, or an empirical diagnosis by a physician. In the main text, we considered any person who was diagnosed by either a rapid antigen or PCR test as having influenza. Here we repeat the analyses completed in the main text with the addition of patients with a diagnosis of influenza empirically based on symptoms. Patients with an empirical diagnosis are generally defined as having influenza-like illness (ILI). In total, there are 735 patients with any diagnosis of influenza. Among these patients, the activity levels ranged from 0 to 10, with a median of 4. All of the patients had symptoms of disease. The most common symptom is weakness, and the least common symptom is vomiting (SM Table @ref(tab:SympAnyTable)).\n\n\n\nOut of the 735 patients included the table shows the number of patients who reported having the following symptoms and the corresponding percentage.\n\n\n\nOverall\n\n\n\n\nn\n735\n\n\nAbdominal Pain = Yes (%)\n93 (12.7)\n\n\nBreathlessness = Yes (%)\n297 (40.4)\n\n\nChest Congestion = Yes (%)\n409 (55.6)\n\n\nChest Pain = Yes (%)\n234 (31.8)\n\n\nChills/Sweats = Yes (%)\n604 (82.2)\n\n\nCough = Yes (%)\n660 (89.8)\n\n\nDiarrhea = Yes (%)\n99 (13.5)\n\n\nEar Pain = Yes (%)\n162 (22.0)\n\n\nEye Pain = Yes (%)\n113 (15.4)\n\n\nFatigue = Yes (%)\n671 (91.3)\n\n\nHeadache = Yes (%)\n620 (84.4)\n\n\nItchy Eyes = Yes (%)\n182 (24.8)\n\n\nMyalgia = Yes (%)\n656 (89.3)\n\n\nNasal Congestion = Yes (%)\n565 (76.9)\n\n\nNausea = Yes (%)\n258 (35.1)\n\n\nRunny Nose = Yes (%)\n524 (71.3)\n\n\nSleeplessness = Yes (%)\n419 (57.0)\n\n\nSneeze = Yes (%)\n395 (53.7)\n\n\nSore Throat = Yes (%)\n614 (83.5)\n\n\nSubjective Fever = Yes (%)\n505 (68.7)\n\n\nSwollen Lymph Nodes = Yes (%)\n314 (42.7)\n\n\nTooth Pain = Yes (%)\n166 (22.6)\n\n\nVomiting = Yes (%)\n79 (10.7)\n\n\nWeakness = Yes (%)\n686 (93.3)\n\n\nWheezing = Yes (%)\n221 (30.1)"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection-1",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection-1",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Univariate and subset selection",
    "text": "Univariate and subset selection\nWe explored the univariate correlations between activity level and each symptom. All of the symptoms that were statistically significantly related to activity showed a negative correlation with activity level (SM Table @ref(tab:LmActvSympAny)). Based on the cross-validated variable selection, we found that a model that included chills/sweats, subjective fever, headache, weakness, sleeplessness, and vomiting creates the most predictive model (SM Table @ref(tab:LmActvSympAny)).\n\n\nResults of the univariate and multivariate linear regression of symptoms and activity. The coefficients are the estimated effect on activity when the symptom is present. The multivariate model was selected with a sequential forward floating selection, minimizing the root mean square error on test data through a 5-fold cross validation (20 times repeated). 95%CI = The 95% confidence interval for the coefficient.\n\n\n\n\n\n\n\n\n\nDependent: Activity Level\n\nMean (sd)\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nAbdominal Pain\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n3.8 (2.7)\n-0.78 (-1.35 to -0.20, p=0.008)\n-\n\n\nBreathlessness\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.2 (2.6)\n-0.39 (-0.78 to 0.00, p=0.052)\n-\n\n\nChest Congestion\nNo\n4.7 (2.7)\n-\n-\n\n\n\nYes\n4.2 (2.5)\n-0.49 (-0.88 to -0.11, p=0.012)\n-\n\n\nChest Pain\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.8)\n-0.42 (-0.83 to -0.01, p=0.044)\n-\n\n\nChills/Sweats\nNo\n6.2 (2.5)\n-\n-\n\n\n\nYes\n4.1 (2.5)\n-2.08 (-2.56 to -1.61, p&lt;0.001)\n-1.30 (-1.79 to -0.81, p&lt;0.001)\n\n\nCough\nNo\n4.9 (2.8)\n-\n-\n\n\n\nYes\n4.4 (2.6)\n-0.48 (-1.11 to 0.15, p=0.137)\n-\n\n\nDiarrhea\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n3.7 (2.5)\n-0.85 (-1.41 to -0.29, p=0.003)\n-\n\n\nEar Pain\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.7)\n-0.34 (-0.80 to 0.12, p=0.149)\n-\n\n\nEye Pain\nNo\n4.5 (2.7)\n-\n-\n\n\n\nYes\n4.5 (2.6)\n0.05 (-0.48 to 0.58, p=0.855)\n-\n\n\nFatigue\nNo\n5.5 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.6)\n-1.15 (-1.83 to -0.48, p=0.001)\n-\n\n\nHeadache\nNo\n5.6 (2.6)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-1.34 (-1.86 to -0.82, p&lt;0.001)\n-0.91 (-1.39 to -0.43, p&lt;0.001)\n\n\nSleeplessness\nNo\n5.0 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.96 (-1.35 to -0.58, p&lt;0.001)\n-0.72 (-1.07 to -0.37, p&lt;0.001)\n\n\nItchy Eyes\nNo\n4.5 (2.7)\n-\n-\n\n\n\nYes\n4.4 (2.5)\n-0.07 (-0.52 to 0.37, p=0.742)\n-\n\n\nMyalgia\nNo\n5.5 (2.7)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-1.14 (-1.75 to -0.53, p&lt;0.001)\n-\n\n\nNasal Congestion\nNo\n4.8 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.7)\n-0.39 (-0.84 to 0.07, p=0.096)\n-\n\n\nNausea\nNo\n4.8 (2.7)\n-\n-\n\n\n\nYes\n3.8 (2.5)\n-0.99 (-1.39 to -0.60, p&lt;0.001)\n-\n\n\nSore Throat\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n4.5 (2.6)\n-0.02 (-0.54 to 0.50, p=0.939)\n-\n\n\nRunny Nose\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.4 (2.6)\n-0.19 (-0.61 to 0.23, p=0.382)\n-\n\n\nSneeze\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-0.27 (-0.66 to 0.11, p=0.164)\n-\n\n\nSubjective Fever\nNo\n5.6 (2.5)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-1.62 (-2.01 to -1.22, p&lt;0.001)\n-0.92 (-1.33 to -0.51, p&lt;0.001)\n\n\nSwollen Lymph Nodes\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.7)\n-0.13 (-0.52 to 0.25, p=0.495)\n-\n\n\nTooth Pain\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.7)\n-0.40 (-0.86 to 0.05, p=0.084)\n-\n\n\nVomiting\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n3.1 (2.3)\n-1.57 (-2.18 to -0.96, p&lt;0.001)\n-1.27 (-1.84 to -0.71, p&lt;0.001)\n\n\nWeakness\nNo\n6.3 (2.5)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-1.98 (-2.73 to -1.22, p&lt;0.001)\n-0.92 (-1.64 to -0.21, p=0.011)\n\n\nWheezing\nNo\n4.7 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.67 (-1.09 to -0.26, p=0.001)\n-"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#computation-of-infectiousness-and-morbidity-scores",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#computation-of-infectiousness-and-morbidity-scores",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Computation of infectiousness and morbidity scores",
    "text": "Computation of infectiousness and morbidity scores\nWe used the same symptom classification presented in the main text. The median infectiousness score is 4, and a skewed distribution is present with most of the patients having a score of 4 or 5 (SM Figure @ref(fig:InfectScoreFig)).\n\n\n\n\n\nDistribution of the infectiousness score.\n\n\n\n\nThe median morbidity score is 9, and no patients have a morbidity score of 0, 1, 19, 20 (SM Figure @ref(fig:MorbScoreFig)). Such a centered distribution is assumed to be a result of patients felling ill enough to seek medical care, but none were sick enough to require urgent care or hospitalization.\n\n\n\n\n\nDistribution of the morbidity score."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-infectiousness-score-on-activity",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-infectiousness-score-on-activity",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Impact of infectiousness score on activity",
    "text": "Impact of infectiousness score on activity\nAnalysis of the impact of the infectiousness score on activity suggests that the value of this score has a negative correlation with the activity level. Spearman’s rank correlation is \\(r =\\) -0.10 (95% CI: -0.17, -0.03) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 7.083, \\(df =\\) 1, \\(p =\\) &lt; 0.01) (SM Figure @ref(fig:AnyTvAFig)). This is different from the main analysis were we did not observe a clear relationship between activity and the infectiousness score.\n\n\n\n\n\nActivity level for each level of the infectiousness score. The red diamond is the mean. The solid blue line is the linear regression fit. The shaded area is the 95% confidence interval for the linear regression."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-activity",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-activity",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Impact of morbidity score on activity",
    "text": "Impact of morbidity score on activity\nAnalysis of the impact of the morbidity score on activity suggests that the value of this score is correlated with the activity level of a patient, with higher morbidity correlating with reduced activity. Spearman’s rank correlation indicates a negative relationship \\(r =\\) -0.32 (95% CI: -0.38, -0.25) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 78.501, \\(df =\\) 1, \\(p\\) &lt; 0.01) (SM Figure @ref(fig:AnyMvAFig)). The observed pattern is clear, with a mean 3.6 fold decrease in activity level going from the lowest to the highest morbidity score.\n\n\n\n\n\nActivity level for each level of the morbidity score. The red diamond is the mean. The solid blue line is the linear regression fit. The shaded area is the 95% confidence interval for the linear regression."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-infectiousness-score",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-infectiousness-score",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Impact of morbidity score on infectiousness score",
    "text": "Impact of morbidity score on infectiousness score\nAnalysis of the relationship between the morbidity and infectiousness scores show a positive correlation. Spearman’s rank correlation indicates a positive relationship ( \\(r =\\) 0.26 (95% CI: 0.19, 0.33)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 44.505, \\(df =\\) 1, \\(p\\) &lt; 0.01) (SM Figure @ref(fig:AnyMvTFig)). Apart from the values activity levels for low morbidity score (with small sample sizes), the pattern is consistent with a mean 1.7 fold increase in the infectiousness score going from the lowest to the highest morbidity score.\n\n\n\n\n\nInfectiousness score for each level of the morbidity score. The red diamond is the mean. The solid blue line is the linear regression fit. The shaded area is the 95% confidence interval for the linear regression."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "simulateddata.html",
    "href": "simulateddata.html",
    "title": "Synthetic Data Practice",
    "section": "",
    "text": "#synthetic data practice\n##setup for simple example\n##setup-load packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/taylorglass/Documents/MADA /taylorglass-MADA-portfolio\n\n##setup- set a seed for reproducibility\nset.seed(123)\n##setup-define the observations to generate\nn_patients &lt;- 100\n##generating data\n##create an emptydataframe with placeholders for variables\nsyn_data &lt;- data.frame(\n  PatientID = numeric(n_patients), \n  Age = numeric(n_patients),\n  Gender = character(n_patients),\n  TreatmentGroup = character(n_patients),\n  EnrollmentDate = lubridate::as_date(character(n_patients)),\n  BloodPressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  AdverseEvent = integer(n_patients)\n)\n# variable 1: patient ID\nsyn_data$PatientID &lt;- 1:n_patients\n# variable 2: age (numeric)\nsyn_data$Age &lt;- round(rnorm(n_patients, mean = 45, sd = 10), 1)\n# variable 3: gender (categorical)\nsyn_data$Gender &lt;- purrr::map_chr(sample(c(\"Male\", \"Female\"), n_patients, replace = TRUE), as.character)\n# variable 4: treatment group (categorical)\nsyn_data$TreatmentGroup &lt;- purrr::map_chr(sample(c(\"A\", \"B\", \"Placebo\"), n_patients, replace = TRUE), as.character)\n# variable 5: data of enrollment\nsyn_data$EnrollmentDate &lt;- lubridate::as_date(sample(seq(from = lubridate::as_date(\"2022-01-01\"), to = lubridate::as_date(\"2022-12-31\"), by=\"days\"), n_patients, replace = TRUE))\n# variable 6: blood pressure (numeric)\nsyn_data$BloodPressure &lt;- round(runif(n_patients, min = 90, max = 160),1)\n# variable 7: cholesterol (numeric & independent of treatment)\nsyn_data$Cholesterol &lt;- round(rnorm(n_patients, mean = 200, sd = 30), 1)\n# variable 8: adverse event \nsyn_data$AdverseEvent &lt;- purrr::map_int(sample(0:1, n_patients, replace = TRUE, prob = c(0.8, 0.2)), as.integer)\n# print the first few rows of the generated data\nhead(syn_data)\n\n  PatientID  Age Gender TreatmentGroup EnrollmentDate BloodPressure Cholesterol\n1         1 39.4 Female              B     2022-08-25         152.0       171.8\n2         2 42.7 Female              B     2022-06-14         128.7       201.6\n3         3 60.6 Female              A     2022-04-17         153.4       182.8\n4         4 45.7   Male              B     2022-02-02         131.1       206.6\n5         5 46.3 Female              A     2022-03-24         119.6       175.0\n6         6 62.2 Female              A     2022-12-20         156.5       196.1\n  AdverseEvent\n1            0\n2            0\n3            0\n4            0\n5            0\n6            1\n\n#save the simulated data to a CSV and rds file\nwrite.csv(syn_data, here(\"syn_data.csv\"), row.names = FALSE)\n##checking data\n## look at the generated data\nsummary(syn_data)\n\n   PatientID           Age           Gender          TreatmentGroup    \n Min.   :  1.00   Min.   :21.90   Length:100         Length:100        \n 1st Qu.: 25.75   1st Qu.:40.08   Class :character   Class :character  \n Median : 50.50   Median :45.60   Mode  :character   Mode  :character  \n Mean   : 50.50   Mean   :45.90                                        \n 3rd Qu.: 75.25   3rd Qu.:51.92                                        \n Max.   :100.00   Max.   :66.90                                        \n EnrollmentDate       BloodPressure    Cholesterol     AdverseEvent \n Min.   :2022-01-08   Min.   : 91.3   Min.   :108.9   Min.   :0.00  \n 1st Qu.:2022-04-06   1st Qu.:110.7   1st Qu.:174.2   1st Qu.:0.00  \n Median :2022-06-25   Median :130.8   Median :197.3   Median :0.00  \n Mean   :2022-06-30   Mean   :128.0   Mean   :195.3   Mean   :0.21  \n 3rd Qu.:2022-10-04   3rd Qu.:147.4   3rd Qu.:217.2   3rd Qu.:0.00  \n Max.   :2022-12-30   Max.   :159.5   Max.   :271.0   Max.   :1.00\ndplyr::glimpse(syn_data)\n\nRows: 100\nColumns: 8\n$ PatientID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ Age            &lt;dbl&gt; 39.4, 42.7, 60.6, 45.7, 46.3, 62.2, 49.6, 32.3, 38.1, 4…\n$ Gender         &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Male\", \"Female\", \"Female…\n$ TreatmentGroup &lt;chr&gt; \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"Placebo\", \"A\", \"Pla…\n$ EnrollmentDate &lt;date&gt; 2022-08-25, 2022-06-14, 2022-04-17, 2022-02-02, 2022-0…\n$ BloodPressure  &lt;dbl&gt; 152.0, 128.7, 153.4, 131.1, 119.6, 156.5, 139.6, 118.9,…\n$ Cholesterol    &lt;dbl&gt; 171.8, 201.6, 182.8, 206.6, 175.0, 196.1, 217.0, 236.3,…\n$ AdverseEvent   &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0…\n#frequency table for adverse events stratified by treatment\ntable(syn_data$AdverseEvent,syn_data$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 33 24      22\n  1 10  6       5\n## ggplot2 boxplot for cholesterol by treatment group\nggplot(syn_data, aes(x=TreatmentGroup, y=Cholesterol)) +\n  geom_boxplot() +\n  labs(x= \"Treatment Group\", y= \"Cholesterol Level\") +\n  theme_bw()"
  },
  {
    "objectID": "simulateddata.html#setup-for-complex-example",
    "href": "simulateddata.html#setup-for-complex-example",
    "title": "Synthetic Data Practice",
    "section": "setup for complex example",
    "text": "setup for complex example\n\n# make sure the packages are installed\n# load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(here)\n# Set seed for reproducibility\nset.seed(123)\n# Number of patients in each treatment group\nnum_patients &lt;- 20\n# Number of days and samples per patient\nnum_days &lt;- 7\nnum_samples_per_day &lt;- 1"
  },
  {
    "objectID": "simulateddata.html#generating-data",
    "href": "simulateddata.html#generating-data",
    "title": "Synthetic Data Practice",
    "section": "generating data",
    "text": "generating data\n\n# treatment group levels\ntreatment_groups &lt;- c(\"Low Dose\", \"High Dose\")\n# generate patient IDs\npatient_ids &lt;- rep(1:num_patients, each = num_days)\n# generate treatment group assignments for each patient\ntreatment_assignments &lt;- rep(sample(treatment_groups, num_patients, replace = TRUE), \n                             each = num_days)\n# generate day IDs for each patient\nday_ids &lt;- rep(1:num_days, times = num_patients)\n# function to generate drug concentrations with variability\ngenerate_drug_concentrations &lt;- function(day, dose_group, patient_id) {\n  baseline_concentration &lt;- ifelse(dose_group == \"Low Dose\", 8, 15)\n  patient_variation &lt;- rnorm(1, mean = 0, sd = 1)\n  time_variation &lt;- exp(-0.1*day)\n  baseline_concentration * time_variation + patient_variation \n}\n# generate drug concentrations for each sample\ndrug_concentrations &lt;- mapply(generate_drug_concentrations, \n                              day = rep(day_ids, each = num_samples_per_day), \n                              dose_group = treatment_assignments,\n                              patient_id = rep(1:num_patients, each = num_days))\n# flatten the matrix to a vector\ndrug_concentrations &lt;- as.vector(drug_concentrations)\n# generate cholesterol levels for each sample \n# (assuming a positive correlation with drug concentration)\ncholesterol_levels &lt;- drug_concentrations + \n  rnorm(num_patients * num_days * num_samples_per_day, mean = 0, sd = 5)\n# generate adverse events based on drug concentration \n# (assuming a higher chance of adverse events with higher concentration)\n# sigmoid function to map concentrations to probabilities\nadverse_events_prob &lt;- plogis(drug_concentrations / 10) \nadverse_events &lt;- rbinom(num_patients * num_days * num_samples_per_day, \n                         size = 1, prob = adverse_events_prob)\n# create a data frame\nsyn_dat2 &lt;- data.frame(\n  PatientID = rep(patient_ids, each = num_samples_per_day),\n  TreatmentGroup = rep(treatment_assignments, each = num_samples_per_day),\n  Day = rep(day_ids, each = num_samples_per_day),\n  DrugConcentration = drug_concentrations,\n  CholesterolLevel = cholesterol_levels,\n  AdverseEvent = adverse_events\n)"
  },
  {
    "objectID": "simulateddata.html#checking-data",
    "href": "simulateddata.html#checking-data",
    "title": "Synthetic Data Practice",
    "section": "checking data",
    "text": "checking data\n\n# print the first few rows of the generated dataset\nprint(head(syn_dat2))\n\n  PatientID TreatmentGroup Day DrugConcentration CholesterolLevel AdverseEvent\n1         1       Low Dose   1          8.462781        12.401475            0\n2         1       Low Dose   2          6.909660        10.754871            0\n3         1       Low Dose   3          6.327317         7.988330            0\n4         1       Low Dose   4          5.473243         0.431360            1\n5         1       Low Dose   5          4.296404         3.699141            0\n6         1       Low Dose   6          6.177406         4.775430            1\n\n\n\nsummary(syn_dat2)\n\n   PatientID     TreatmentGroup          Day    DrugConcentration\n Min.   : 1.00   Length:140         Min.   :1   Min.   : 2.081   \n 1st Qu.: 5.75   Class :character   1st Qu.:2   1st Qu.: 5.174   \n Median :10.50   Mode  :character   Median :4   Median : 7.056   \n Mean   :10.50                      Mean   :4   Mean   : 7.593   \n 3rd Qu.:15.25                      3rd Qu.:6   3rd Qu.: 9.738   \n Max.   :20.00                      Max.   :7   Max.   :14.933   \n CholesterolLevel  AdverseEvent   \n Min.   :-4.494   Min.   :0.0000  \n 1st Qu.: 3.844   1st Qu.:0.0000  \n Median : 7.234   Median :1.0000  \n Mean   : 7.855   Mean   :0.6571  \n 3rd Qu.:11.259   3rd Qu.:1.0000  \n Max.   :24.422   Max.   :1.0000  \n\n\n\ndplyr::glimpse(syn_dat2)\n\nRows: 140\nColumns: 6\n$ PatientID         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3…\n$ TreatmentGroup    &lt;chr&gt; \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low…\n$ Day               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4…\n$ DrugConcentration &lt;dbl&gt; 8.462781, 6.909660, 6.327317, 5.473243, 4.296404, 6.…\n$ CholesterolLevel  &lt;dbl&gt; 12.4014754, 10.7548711, 7.9883301, 0.4313600, 3.6991…\n$ AdverseEvent      &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1…"
  },
  {
    "objectID": "simulateddata.html#exploratory-plot",
    "href": "simulateddata.html#exploratory-plot",
    "title": "Synthetic Data Practice",
    "section": "exploratory plot",
    "text": "exploratory plot\n\np1 &lt;- ggplot(syn_dat2, aes(x = Day, y = DrugConcentration, \n                      group = as.factor(PatientID), color = TreatmentGroup)) +\n  geom_line() +\n  labs(title = \"Drug Concentrations Over Time\",\n       x = \"Day\",\n       y = \"Drug Concentration\",\n       color = \"TreatmentGroup\") +\n  theme_minimal()\nplot(p1)\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(syn_dat2, aes(x = as.factor(AdverseEvent), y = DrugConcentration, \n                           fill = TreatmentGroup)) +\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8), color = \"black\") +\n  geom_point(aes(color = TreatmentGroup), position = position_dodge(width = 0.8), \n             size = 3, shape = 16) +  # Overlay raw data points\n  labs(\n    x = \"Adverse Events\",\n    y = \"Drug Concentration\",\n    title = \"Boxplot of Drug Concentration by Adverse Events and Treatment\"\n  ) +\n  scale_color_manual(values = c(\"A\" = \"blue\", \"B\" = \"red\")) +  # Customize color for each treatment\n  theme_minimal() +\n  theme(legend.position = \"top\")\nplot(p2)"
  },
  {
    "objectID": "simulateddata.html#save-data",
    "href": "simulateddata.html#save-data",
    "title": "Synthetic Data Practice",
    "section": "save data",
    "text": "save data\n\nwrite.csv(syn_dat2, here(\"syn_dat2.csv\"), row.names = FALSE)"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "Exploring CDC Data: Vaccination Coverage Among Pregnant Women\n\n\nDescription of the data\nI found this data set from the CDC’s data log under the pregnancy and vaccination section. The data was provided by the National Center for Immunization and Respiratory Diseases (NCIRD), and the information was collected through the Pregnancy Risk Assessment Monitoring System (PRAMS). Information includes vaccination rates at the state level for influenza and tetanus toxoid, reduced diptheria toxoid, and aceullar pertussis (Tdap) for women who recently gave birth. Variables include vaccine type, geography, survey year/influenza season, dimension (age or race), and estimation of vaccination coverage with 95% confidence interval. It was most recently updated on December 15, 2023 and contains 4,379 observations of 9 variables. Link: https://data.cdc.gov/Pregnancy-Vaccination/Vaccination-Coverage-among-Pregnant-Women/h7pm-wmjc/about_data\n\n\nRead and load the data\nI loaded two packages for this step: readr and here. I called the data ‘pregvacc’. After reading and loading the data, I confirmed that there were 4,379 observations of 9 variables.\n\n## load packages\nlibrary(readr)\nlibrary(here)\n\nhere() starts at /Users/taylorglass/Documents/MADA /taylorglass-MADA-portfolio\n\n##read in data and find dimensions\npregvacc &lt;- read_csv(here(\"cdcdata-exercise\", \"Vaccination_Coverage_among_Pregnant_Women_20240205.csv\"))\n\nRows: 4379 Columns: 9\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): Vaccine, Geography Type, Geography, Dimension Type, Dimension, Esti...\ndbl (2): Survey Year/Influenza Season, Sample Size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndim(pregvacc)\n\n[1] 4379    9\n\n\n\n\nProcessing the data\nUsing the naniar package to discover missingness of the data shows that this data set is pretty clean. Sample size is the only variable with missing data, and it has 215 missing observations. Sample size is an important variable for determining the reliability of the vaccination coverage estimation percentages. The ‘data quality’ section of the CDC page for this data set notes that observations marked with an asterisk are unreliable due to a sample size of less than 30. After exploring the variable, the minimum value is exactly 30, so none of the estimates are less than 30. I feel confident in removing this variable from the data set because all of the vaccination coverage estimates with complete data are reliable based on the sample size variable.\n\n## load packages\nlibrary(naniar)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\n## explore missingness of the data \ngg_miss_var(pregvacc)\n\n\n\n\n\n\n\nsum(is.na(pregvacc$`Sample Size`))\n\n[1] 215\n\nsummary(pregvacc$`Sample Size`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   30.0   167.0   320.5   797.2   655.2 43737.0     215 \n\n## create new dataset without the sample size variable\npregvacc2 &lt;- pregvacc %&gt;% select(-`Sample Size`)\n\nUsing the head() function, I saw “NR” in the estimate of vaccination coverage and confidence interval variables, which also represents missing data. With assistance from AI tools, I wrote a function to rid the data of any “NR” variables or special characters using the dplyr and stringr packages. I found that there are 442 missing observations of the estimate variable and confidence interval variables. Considering that the data set contains 4,379 observations, only 10% of the observations have a missing estimate value for vaccination coverage. I think it is okay to remove the missing observations in this scenario because each state has multiple observations across several years of PRAMS data. Information about vaccination among pregnant women is included for all 50 states based on various ages and races/ethnicities, so I think it is reasonable to remove 10% of the data set for this analysis.\n\n## examine data again \nhead(pregvacc2)\n\n# A tibble: 6 × 8\n  Vaccine   `Geography Type` Geography Survey Year/Influenza …¹ `Dimension Type`\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;           \n1 Influenza States           Alaska                        2012 Age             \n2 Influenza States           Alaska                        2020 Age             \n3 Influenza States           Alaska                        2020 Race and Ethnic…\n4 Influenza States           Alaska                        2012 Age             \n5 Influenza States           Alaska                        2012 Race and Ethnic…\n6 Influenza States           Alaska                        2020 Age             \n# ℹ abbreviated name: ¹​`Survey Year/Influenza Season`\n# ℹ 3 more variables: Dimension &lt;chr&gt;, `Estimate (%)` &lt;chr&gt;, `95% CI (%)` &lt;chr&gt;\n\n## create a function to clean the data\nclean_data &lt;- function(data) {\n  data[data == 'NR*'] &lt;- NA\n  data &lt;- data %&gt;% mutate_if(is.character, str_replace_all, pattern = \"[^a-zA-Z0-9\\\\s]\", replacement = \"\")\n  return(data)\n}\n\n## use the clean_data function to create the third version of the dataset\npregvacc3 &lt;- clean_data(pregvacc2)\n\n## explore the new NAs created by the clean_data function \ngg_miss_var(pregvacc3)\n\n\n\n\n\n\n\nsum(is.na(pregvacc3$`Estimate (%)`))\n\n[1] 442\n\nsum(is.na(pregvacc3$`95% CI (%)`))\n\n[1] 442\n\nmissing &lt;- 442/4379*100\nhead(pregvacc3)\n\n# A tibble: 6 × 8\n  Vaccine   `Geography Type` Geography Survey Year/Influenza …¹ `Dimension Type`\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;           \n1 Influenza States           Alaska                        2012 Age             \n2 Influenza States           Alaska                        2020 Age             \n3 Influenza States           Alaska                        2020 Race and Ethnic…\n4 Influenza States           Alaska                        2012 Age             \n5 Influenza States           Alaska                        2012 Race and Ethnic…\n6 Influenza States           Alaska                        2020 Age             \n# ℹ abbreviated name: ¹​`Survey Year/Influenza Season`\n# ℹ 3 more variables: Dimension &lt;chr&gt;, `Estimate (%)` &lt;chr&gt;, `95% CI (%)` &lt;chr&gt;\n\n\nAfter dropping the NA values, the data set contains 3,937 observations of 8 variables with no missing values. The head() function revealed that the estimate percentage is a character variable, which is incorrect because it should be a numeric variable. I mutated the variable to be numeric and divided by 100, so it will have the typical percentage format.\n\n## create final dataset with no NAs to be used for exploratory analyis \npregvacc4 &lt;- na.omit(pregvacc3)\ndim(pregvacc4)\n\n[1] 3937    8\n\nsum(is.na(pregvacc4))\n\n[1] 0\n\n## check that all variables appear to be complete\nhead(pregvacc4)\n\n# A tibble: 6 × 8\n  Vaccine   `Geography Type` Geography Survey Year/Influenza …¹ `Dimension Type`\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;           \n1 Influenza States           Alaska                        2012 Age             \n2 Influenza States           Alaska                        2020 Age             \n3 Influenza States           Alaska                        2012 Age             \n4 Influenza States           Alaska                        2020 Age             \n5 Influenza States           Alaska                        2020 Age             \n6 Influenza States           Alaska                        2020 Age             \n# ℹ abbreviated name: ¹​`Survey Year/Influenza Season`\n# ℹ 3 more variables: Dimension &lt;chr&gt;, `Estimate (%)` &lt;chr&gt;, `95% CI (%)` &lt;chr&gt;\n\ntail(pregvacc4)\n\n# A tibble: 6 × 8\n  Vaccine `Geography Type` Geography Survey Year/Influenza Se…¹ `Dimension Type`\n  &lt;chr&gt;   &lt;chr&gt;            &lt;chr&gt;                          &lt;dbl&gt; &lt;chr&gt;           \n1 Tdap    States           Minnesota                       2018 Race and Ethnic…\n2 Tdap    States           Minnesota                       2018 Age             \n3 Tdap    States           Minnesota                       2019 Age             \n4 Tdap    States           Minnesota                       2020 Age             \n5 Tdap    States           Minnesota                       2019 Age             \n6 Tdap    States           Minnesota                       2018 Age             \n# ℹ abbreviated name: ¹​`Survey Year/Influenza Season`\n# ℹ 3 more variables: Dimension &lt;chr&gt;, `Estimate (%)` &lt;chr&gt;, `95% CI (%)` &lt;chr&gt;\n\n## clean vaccination estimate variable to follow normal percentage format \npregvacc4 &lt;- pregvacc4 %&gt;% \n                mutate(`Estimate (%)` = as.numeric(`Estimate (%)`)) %&gt;%\n                mutate(`Estimate (%)` = `Estimate (%)` / 100)\nclass(pregvacc4$`Estimate (%)`) # check if class mutation worked\n\n[1] \"numeric\"\n\nsummary(pregvacc4$`Estimate (%)`) # check if format mutation worked \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.520   5.370   6.380   6.234   7.280   9.960 \n\n\nThe 95% confidence interval has the same issue of being a character variable instead of numeric variable, and the upper and lower bounds are separated by the word “to”, which prevents me from directly converting it to numeric. I separated the variable into upper and lower bounds using the separate() function from the tidyr package, so I could convert the variable to the numeric format. I also had to divide the values by 100 once they were numeric, so it would be in a normal percentage format.\n\n## load packages \nlibrary(tidyr)\n\n## separate the confidence interval variable to convert it to numeric in a typical percent format\npregvacc4 &lt;- pregvacc4 %&gt;% \n                separate(`95% CI (%)`, into = c(\"95% CI Lower\", \"95% CI Upper\"), sep = \"to\", convert = TRUE)\nhead(pregvacc4) # check if separation worked\n\n# A tibble: 6 × 9\n  Vaccine   `Geography Type` Geography Survey Year/Influenza …¹ `Dimension Type`\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;           \n1 Influenza States           Alaska                        2012 Age             \n2 Influenza States           Alaska                        2020 Age             \n3 Influenza States           Alaska                        2012 Age             \n4 Influenza States           Alaska                        2020 Age             \n5 Influenza States           Alaska                        2020 Age             \n6 Influenza States           Alaska                        2020 Age             \n# ℹ abbreviated name: ¹​`Survey Year/Influenza Season`\n# ℹ 4 more variables: Dimension &lt;chr&gt;, `Estimate (%)` &lt;dbl&gt;,\n#   `95% CI Lower` &lt;dbl&gt;, `95% CI Upper` &lt;int&gt;\n\npregvacc4 &lt;- pregvacc4 %&gt;% \n                mutate(`95% CI Lower` = as.numeric(`95% CI Lower`) / 100) %&gt;% \n                mutate(`95% CI Upper` = as.numeric(`95% CI Upper`) / 100)\nhead(pregvacc4) # check if mutation worked \n\n# A tibble: 6 × 9\n  Vaccine   `Geography Type` Geography Survey Year/Influenza …¹ `Dimension Type`\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;           \n1 Influenza States           Alaska                        2012 Age             \n2 Influenza States           Alaska                        2020 Age             \n3 Influenza States           Alaska                        2012 Age             \n4 Influenza States           Alaska                        2020 Age             \n5 Influenza States           Alaska                        2020 Age             \n6 Influenza States           Alaska                        2020 Age             \n# ℹ abbreviated name: ¹​`Survey Year/Influenza Season`\n# ℹ 4 more variables: Dimension &lt;chr&gt;, `Estimate (%)` &lt;dbl&gt;,\n#   `95% CI Lower` &lt;dbl&gt;, `95% CI Upper` &lt;dbl&gt;\n\n\n\n\nExploratory/Descriptive Analysis on the clean data\nAfter cleaning and processing the data, the final data set has 3,937 observations of 9 variables. The categorical variables include vaccine, geography type, geography, dimension type, and dimension. The vaccine variable has two categories: influenza and tdap. The influenza vaccine is twice as common as the tdap vaccine considering that it accounts for 66.6% of vaccine observations. I used AI tools to help create a simple table displaying what percent is in each category.\n\n## find basic facts about final clean dataset\ndim(pregvacc4)\n\n[1] 3937    9\n\nstr(pregvacc4)\n\ntibble [3,937 × 9] (S3: tbl_df/tbl/data.frame)\n $ Vaccine                     : chr [1:3937] \"Influenza\" \"Influenza\" \"Influenza\" \"Influenza\" ...\n $ Geography Type              : chr [1:3937] \"States\" \"States\" \"States\" \"States\" ...\n $ Geography                   : chr [1:3937] \"Alaska\" \"Alaska\" \"Alaska\" \"Alaska\" ...\n $ Survey Year/Influenza Season: num [1:3937] 2012 2020 2012 2020 2020 ...\n $ Dimension Type              : chr [1:3937] \"Age\" \"Age\" \"Age\" \"Age\" ...\n $ Dimension                   : chr [1:3937] \"18 Years\" \"18 Years\" \"35 Years\" \"35 Years\" ...\n $ Estimate (%)                : num [1:3937] 4.92 6.4 5.49 6.47 6.27 6.68 4.89 4.83 4.68 5.92 ...\n $ 95% CI Lower                : num [1:3937] 4.53 6.04 4.36 5.6 5.79 5.92 4.18 4.3 4.12 5.35 ...\n $ 95% CI Upper                : num [1:3937] 5.31 6.75 6.58 7.28 6.73 7.38 5.61 5.36 5.24 6.47 ...\n\n## create percentage table with categories of vaccine variable\nvacc_table &lt;- table(pregvacc4$Vaccine)\npercentage_table &lt;- prop.table(vacc_table) * 100\nprint(percentage_table)\n\n\nInfluenza      Tdap \n 66.67513  33.32487 \n\n\nUsing the code above as a template, I created a summary table for each categorical variable. The geography type variable is 95.9% states, with the other 4.1% of observations being at the national level. The geography categorical variable has 51 options with differentiation between New York City and the rest of New York state and inclusion of Puerto Rico. Wisconsin makes up the highest percentage of the geography variable with 4.06% of observations, and Indiana makes up the lowest percentage at 0.2% of observation. The two dimension types are split almost evenly with 55.65% of observations belonging to age and 44.35% of observations belonging to race and ethnicity. There are 4 categories for age: 18 years, 18-24 years, 25-34 years, and 35 years. Each of these age categories account for about 14% of dimension observations. There are 4 categories for race/ethnicity: Black Non-Hispanic, Hispanic, Other or Multiple Races Non-Hispanic, and White Non-Hispanic. White Non-Hispanic is the largest of the race and ethnicity categories with 13.86% of observations for the dimension variable.\n\n## create percentage table with categories of geography type variable\ngeotype_table &lt;- table(pregvacc4$`Geography Type`)\npercentage_table2 &lt;- prop.table(geotype_table) * 100\nprint(percentage_table2)\n\n\n National    States \n 4.064008 95.935992 \n\n## create percentage table with categories of geography variable\ngeo_table &lt;- table(pregvacc4$Geography)\npercentage_table3 &lt;- prop.table(geo_table) * 100\nprint(percentage_table3)\n\n\n             Alabama               Alaska              Arizona \n           1.2446025            1.5494031            0.1778004 \n            Arkansas             Colorado          Connecticut \n           1.9558039            3.0988062            1.6256033 \n            Delaware District of Columbia              Florida \n           4.0132080            0.7112014            0.3556007 \n             Georgia               Hawaii             Illinois \n           1.2192024            2.1082042            2.8194056 \n             Indiana                 Iowa               Kansas \n           0.2032004            2.3114046            0.8890018 \n            Kentucky            Louisiana                Maine \n           0.6096012            2.2606045            1.2954026 \n            Maryland        Massachusetts             Michigan \n           1.8288037            3.2512065            2.9464059 \n           Minnesota          Mississippi             Missouri \n           1.8288037            1.2192024            3.4798070 \n             Montana             Nebraska        New Hampshire \n           1.6256033            2.8448057            1.6510033 \n          New Jersey           New Mexico             New York \n           2.0320041            1.9558039            3.2004064 \n      North Carolina         North Dakota   NYCity of New York \n           0.6096012            0.8128016            3.4544069 \n     NYRest of state                 Ohio             Oklahoma \n           2.7178054            0.5588011            3.0226060 \n              Oregon         Pennsylvania          Puerto Rico \n           1.4224028            3.8608077            0.6350013 \n        Rhode Island         South Dakota            Tennessee \n           2.0320041            1.0160020            1.1176022 \n               Texas        United States                 Utah \n           0.8128016            4.0640081            2.7686055 \n             Vermont             Virginia           Washington \n           2.4384049            2.6416053            3.2512065 \n       West Virginia            Wisconsin              Wyoming \n           1.1430023            4.0640081            1.2446025 \n\n## create percentage table with categories of dimension type variable\ndimtype_table &lt;- table(pregvacc4$`Dimension Type`)\npercentage_table3 &lt;- prop.table(dimtype_table) * 100\nprint(percentage_table3)\n\n\n               Age Race and Ethnicity \n          55.65151           44.34849 \n\n## create percentage table with categories of dimension variable\ndim_table &lt;- table(pregvacc4$Dimension)\npercentage_table4 &lt;- prop.table(dim_table) * 100\nprint(percentage_table4)\n\n\n                           18 Years                          1824 Years \n                          14.020828                           13.944628 \n                         2534 Years                            35 Years \n                          14.020828                           13.665227 \n                  Black NonHispanic                            Hispanic \n                           9.728219                           10.820422 \nOther or Multiple Races NonHispanic                   White NonHispanic \n                           9.931420                           13.868428 \n\n\nTo explore the continuous variables, I will make plots to see if they are approximately normal. The mean of the vaccination estimate among pregnant women is 6.23%, and the standard deviation is 1.497. I had to convert the estimate variable to a factor to create a bar graph that displayed each of the unique discrete observations. The distribution looks approximately normal, but it is slightly skewed to the left.\n\n## load packages \nlibrary(ggplot2)\n\n## find summary values while variable is numeric \nmean(pregvacc4$`Estimate (%)`)\n\n[1] 6.233548\n\nsd(pregvacc4$`Estimate (%)`)\n\n[1] 1.496931\n\n## convert variable to factor to be visualized\npregvacc4 &lt;- pregvacc4 %&gt;% \n                mutate(`Estimate (%)` = as.factor(`Estimate (%)`))\nclass(pregvacc4$`Estimate (%)`) ## check if mutation worked \n\n[1] \"factor\"\n\n## create visualization of the distribution of estimate (%) variable\nggplot(pregvacc4, aes(x=`Estimate (%)`)) + \n    geom_bar(position = \"dodge\") + \n    labs(x = \"Estimate of Vaccination Percentage\",\n         y= \"Frequency\") \n\n\n\n\n\n\n\n\nI would expect the lower and upper bound of the 95% confidence interval (CI) to have the same distribution as the estimate variable because the estimate variable is used to calculate the CI. Before converting the variables to a factor type to create the bar graph, I found the mean and standard deviation for each one while the variable was still numeric. The mean for the lower bound of the 95% CI 5.56%, and the standard deviation was 1.53. The mean for the upper bound of the 95% CI is 6.86%, and the standard deviation is 1.46. After creating two more bar graphs using the variables as factors, my expectations were met, and each distribution for the bounds of the confidence interval are approximately normal with a slight skew to the left.\n\n## find summary values while variable is numeric \nmean(pregvacc4$`95% CI Lower`)\n\n[1] 5.555558\n\nsd(pregvacc4$`95% CI Lower`)\n\n[1] 1.531577\n\n## find summary values while variable is numeric\nmean(pregvacc4$`95% CI Upper`)\n\n[1] 6.86397\n\nsd(pregvacc4$`95% CI Upper`)\n\n[1] 1.463774\n\n## convert variable to factor to be visualized \npregvacc4 &lt;- pregvacc4 %&gt;% \n                mutate(`95% CI Lower` = as.factor(`95% CI Lower`))\nclass(pregvacc4$`95% CI Lower`) ## check if conversion worked\n\n[1] \"factor\"\n\n##create visualization of the distribution of 95% CI Lower variable\nggplot(pregvacc4, aes(x=`95% CI Lower`)) + \n    geom_bar(position = \"dodge\") + \n    labs(x = \"Lower Bound of 95% Confidence Interval for Estimate\",\n         y= \"Frequency\") \n\n\n\n\n\n\n\n## convert variable to factor to be visualized \npregvacc4 &lt;- pregvacc4 %&gt;% \n                mutate(`95% CI Upper` = as.factor(`95% CI Upper`))\nclass(pregvacc4$`95% CI Upper`) ## check if conversion worked\n\n[1] \"factor\"\n\n##create visualization of the distribution of 95% CI Upper variable\nggplot(pregvacc4, aes(x=`95% CI Upper`)) + \n    geom_bar(position = \"dodge\") + \n    labs(x = \"Upper Bound of 95% Confidence Interval for Estimate\",\n         y= \"Frequency\") \n\n\n\n\n\n\n\n\n\n\nCassia Roth created this section.\nFirst, lets make sure we have all necessary packages loaded.\n\n#Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ purrr     1.0.2\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:naniar':\n\n    n_complete\n\nlibrary(gtsummary)\n\nNext, I asked ChatGPT the following prompt: Can you write R code that generates a dataset of 3937 observations of vaccination status for women who recently gave birth. Women are between 18 and 35 years. The dataset should include seven variables (vaccine, state, year, age, race/ethnicity, estimate of vaccine coverage in percentage, 95% CI lower, 95% CI upper). 66% of the vaccination observations should be influenza vaccines and 33 percent should be Tdap vaccines. Then, given Table 4 Tessa created above, I put that into ChatGPT, so it should know the percentage of observations for each state. Then, I asked it to add code so that 50% of observations were in the age groups 18-24 and 50% were in the age groups 25-34; and that 21% of observations were Black-non Hispanic, 25% were Hispanic, 31% were White non-Hispanic, and 23% were Other. Last, I asked it to make sure the years for the data were between 2012 and 2021. I also asked it to make sure that the mean vaccination coverage was 6.23% with a standard deviation of 1.497. I finally asked it to calculate the 95% confidence interval for the mean vaccination coverage and display it. Calculating the confidence intervals took quite a bit of back and forth with ChatGPT.\nNote: I removed the variable geography type (state), since that wasn’t really necessary to duplicate the data. I also combined the age categories 18 and 18-24 and 25-34 and 35 for ease of use.\nThis is the fully workable code it produced.\n\n#Set seed for reproducibility\nset.seed(123)\n\n#Define number of observations to generate\nn_observations &lt;- 3937 #based on the observations of 9 variables stated above\n\n# Define the percentage of observations for each vaccine type\npercent_influenza &lt;- 0.66\npercent_tdap &lt;- 0.33\n\n# Generate vaccine types based on the specified percentages\nvaccine_types &lt;- sample(c(\"Influenza\", \"Tdap\"), size = n_observations, \n                        replace = TRUE, prob = c(percent_influenza, percent_tdap))\n\n# Define the states and their corresponding percentages of observations\nstates &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"Colorado\", \"Connecticut\",\n            \"Delaware\", \"District of Columbia\", \"Florida\", \"Georgia\", \"Hawaii\", \"Illinois\",\n            \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n            \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\",\n            \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\",\n            \"North Dakota\", \"NYCity of New York\", \"NYRest of state\", \"Ohio\", \"Oklahoma\", \"Oregon\",\n            \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Dakota\", \"Tennessee\", \"Texas\",\n            \"United States\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\",\n            \"Wisconsin\", \"Wyoming\")\n\npercentages &lt;- c(1.2446025, 1.5494031, 0.1778004, 1.9558039, 3.0988062, 1.6256033,\n                 4.0132080, 0.7112014, 0.3556007, 1.2192024, 2.1082042, 2.8194056,\n                 0.2032004, 2.3114046, 0.8890018, 0.6096012, 2.2606045, 1.2954026,\n                 1.8288037, 3.2512065, 2.9464059, 1.8288037, 1.2192024, 3.4798070,\n                 1.6256033, 2.8448057, 1.6510033, 2.0320041, 1.9558039, 3.2004064,\n                 0.6096012, 0.8128016, 3.4544069, 2.7178054, 0.5588011, 3.0226060,\n                 1.4224028, 3.8608077, 0.6350013, 2.0320041, 1.0160020, 1.1176022,\n                 0.8128016, 4.0640081, 2.7686055, 2.4384049, 2.6416053, 3.2512065,\n                 1.1430023, 4.0640081, 1.2446025)\n\n# Normalize percentages to sum to 1\npercentages &lt;- percentages / sum(percentages)\n\n# Generate state observations based on the specified percentages\nstate_obs &lt;- sample(states, size = n_observations, replace = TRUE, prob = percentages)\n\n# Generate random years between 2012 and 2021\nyears &lt;- sample(2012:2021, size = n_observations, replace = TRUE)\n\n# Generate random ages between 18 and 35\nage_groups &lt;- sample(c(\"18-24\", \"25-34\"), size = n_observations, replace = TRUE, prob = c(0.5, 0.5))\n\n# Generate random race/ethnicity\nrace_ethnicity &lt;- sample(c(\"Black-non Hispanic\", \"Hispanic\", \"White non-Hispanic\", \"Other\"), \n                         size = n_observations, replace = TRUE, prob = c(0.21, 0.25, 0.31, 0.23))\n\n# Generate random estimates of vaccine coverage in percentage\nvaccine_coverage &lt;- rnorm(n_observations, mean = 6.23, sd = 1.497)\n\n# Calculate the mean and standard deviation of the generated vaccine coverage data\nmean_coverage &lt;- mean(vaccine_coverage)\nsd_coverage &lt;- sd(vaccine_coverage)\n\n# Calculate the standard error (SE) of the mean\nse_mean &lt;- sd_coverage / sqrt(n_observations)\n\n# Calculate the margin of error (ME) for a 95% confidence level (assuming normal distribution)\n# For a 95% confidence interval, the critical z-value is approximately 1.96\nz_value &lt;- qnorm(0.975)  # Two-tailed\n\n# Initialize variables to store lower and upper bounds of the confidence intervals\nlower_ci &lt;- numeric(n_observations)\nupper_ci &lt;- numeric(n_observations)\n\n#Setting fixed CIs for each observation to avoid bootstrapping\nlower_ci &lt;- vaccine_coverage - z_value * se_mean\nupper_ci &lt;- vaccine_coverage + z_value * se_mean\n\n# Combine all variables into a data frame\nvaccination_data &lt;- data.frame(vaccine = vaccine_types,\n                               state = state_obs,\n                               year = years,\n                               age_group = age_groups,\n                               race_ethnicity = race_ethnicity,\n                               estimate_coverage_percentage = vaccine_coverage,\n                               lower_ci = lower_ci,\n                               upper_ci = upper_ci)\n\n# Display the first few rows of the dataset and get summary\nhead(vaccination_data)\n\n    vaccine              state year age_group     race_ethnicity\n1 Influenza          Wisconsin 2018     25-34              Other\n2      Tdap            Vermont 2012     18-24           Hispanic\n3 Influenza           Maryland 2019     25-34              Other\n4      Tdap            Alabama 2021     25-34 White non-Hispanic\n5      Tdap      United States 2021     18-24           Hispanic\n6 Influenza NYCity of New York 2020     25-34 Black-non Hispanic\n  estimate_coverage_percentage  lower_ci  upper_ci\n1                     8.470043  8.423233  8.516853\n2                     5.660458  5.613648  5.707267\n3                    10.149212 10.102402 10.196022\n4                     6.185569  6.138759  6.232379\n5                     4.994664  4.947854  5.041473\n6                     5.991605  5.944795  6.038414\n\nskim(vaccination_data)\n\n\nData summary\n\n\nName\nvaccination_data\n\n\nNumber of rows\n3937\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nvaccine\n0\n1\n4\n9\n0\n2\n0\n\n\nstate\n0\n1\n4\n20\n0\n51\n0\n\n\nage_group\n0\n1\n5\n5\n0\n2\n0\n\n\nrace_ethnicity\n0\n1\n5\n18\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2016.48\n2.9\n2012.00\n2014.00\n2016.00\n2019.00\n2021.00\n▇▇▇▇▇\n\n\nestimate_coverage_percentage\n0\n1\n6.23\n1.5\n1.22\n5.24\n6.25\n7.27\n10.87\n▁▃▇▅▁\n\n\nlower_ci\n0\n1\n6.19\n1.5\n1.17\n5.20\n6.20\n7.22\n10.83\n▁▃▇▅▁\n\n\nupper_ci\n0\n1\n6.28\n1.5\n1.27\n5.29\n6.29\n7.32\n10.92\n▁▃▇▅▁\n\n\n\n\n\nThen, I plotted the mean and standard distribution of the vaccine coverage variable. Although my tables are in a different format than Tessa’s above, they have a similar distribution.\n\n#Find summary values\nmean(vaccination_data$estimate_coverage_percentage)\n\n[1] 6.233056\n\nsd(vaccination_data$estimate_coverage_percentage)\n\n[1] 1.498551\n\nmean(vaccination_data$lower_ci)\n\n[1] 6.186246\n\nsd(vaccination_data$lower_ci)\n\n[1] 1.498551\n\nmean(vaccination_data$upper_ci)\n\n[1] 6.279866\n\nsd(vaccination_data$upper_ci)\n\n[1] 1.498551\n\n# Plotting the distribution of vaccine coverage percentage\nggplot(vaccination_data, aes(x = estimate_coverage_percentage)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Vaccine Coverage Percentage\",\n       x = \"Vaccine Coverage Percentage\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plotting the distribution of mean and standard deviation of vaccine coverage percentage\nggplot(vaccination_data, aes(x = estimate_coverage_percentage)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(estimate_coverage_percentage, na.rm = TRUE)), \n             color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = mean(estimate_coverage_percentage, na.rm = TRUE) + \n                   sd(estimate_coverage_percentage, na.rm = TRUE)), \n             color = \"green\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = mean(estimate_coverage_percentage, na.rm = TRUE) - \n                   sd(estimate_coverage_percentage, na.rm = TRUE)), \n             color = \"green\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Distribution of Vaccine Coverage Percentage with Mean and Standard Deviation\",\n       x = \"Vaccine Coverage Percentage\",\n       y = \"Frequency\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nFinally, I will plot the distribution of both the lower and upper bounds of the 95% CI. As you can see from the output, the distributions match those of the mean and standard deviation of the vaccine coverage variable.\n\n# Plotting the distribution of lower 95% CI\nggplot(vaccination_data, aes(x = lower_ci)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Distribution of Lower 95% Confidence Interval\",\n       x = \"Lower 95% CI\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plotting the distribution of upper 95% CI\nggplot(vaccination_data, aes(x = upper_ci)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Distribution of Upper 95% Confidence Interval\",\n       x = \"Upper 95% CI\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "The Basics\nI am currently in the graduate year for my Master of Public Health in Biostatistics, so I have been living in Athens for over 4 years now. I am a Double Dawg here at the University of Georgia, and I have a bachelor’s of science in Health Promotion, which allowed me to get a head start on my MPH. I took my first biostatistics class during my freshman year, and I have been excited to learn more about this topic ever since. I chose this area of emphasis because I love combining my affinity for math with real public health problems. I really enjoy data visualization and translating findings from research studies into a format that can be understood by everyone. I hope to have a career that allows me to use my data analysis skills on projects that make a positive difference in population health.\n\n\nResearch Interests\nI am currently on Dr. Lambert and Dr. Swartzendruber’s research team working on their Crisis Pregnancy Center map. I completed my undergraduate internship with this team, and I spent most of my time completing qualitative data collection forms and developing codebooks. I continued my work in the fall of 2023 as the co-team leader for the content analysis team, which required me to manage 6 research assistants and complete weekly quality assurance checks of the data collection surveys. I love working in reproductive health research, and I am also interested in exploring research in the antimicrobial resistance and clinical trials for drug development spaces in the future.\n\n\nData Analysis Experience\nMost of my data analysis training has been hands-on with the CPC map project through using tools such as REDCap. I took the EPID 7500 elective in the fall of 2023, which gave me a great foundation for working with R studio. I also took BIOS 8110 in the same semester, which is a categorical data analysis class that allowed me to explore more advanced statistical methods. Both of these courses included a final project that required using a real-world data set to formula research questions, clean the data to answer these questions, and create a report with the findings.\n\n\nGoals For The Course\nI hope to become more comfortable with learning new data analysis tools using the resources provided during this course. I am often initially intimidated when it comes to learning new software and techniques, so I want to become better at adapting to new analysis tools. I am new to Github, so I am excited to create a presence in that space while learning how to utilize Gitkracken. While I have used AI tools sparingly, I am also looking forward to understanding more about how they function and how they can help me in my future career.\n\n\nFun Facts\nI am a huge foodie, so I spend the majority of my day planning meals, brainstorming ideas for the next party spread, or planning vacations to hot spots with new restaurants. Some of my favorite restaurants in Athens are Last Resort Grill, Thai Spoon, and Maepole. I love watching the Dawgs play, and I think game days are my favorite time to be in Athens. One unique thing about me is that I have the same birthday as my best friend, and we have spent the past 16 years celebrating together. Something curious about me: I actually grew up as an Auburn fan because my dad went there, but I will be cheering for the Dawgs from now on.\n\n\n\nMe in NYC at my favorite restaurant, Via Carota\n\n\n\n\nInteresting Data Analysis Predictions\nUpside is a website run by The Data Warehousing Institute with the majority of articles written by professional technology authors and some contributions from those within the data analytics community. A recently published article predicted the upcoming trends for generative AI in 2024. The most interesting prediction suggests an expansion of generative AI to support customer service with only meager large-scale investments in AI due to data security concerns. Additional topics discussed in the article include data lakehouses, Microsoft Fabric, and cloud analytics. I find the new platforms and companies involved in each of these data analysis topics to be really interesting because involvement in AI seems to be the hottest topic in current events recently."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Data from the ds labs package will be utilized for this practice. The renv package is a great resource for keeping track of all the packages used within a project. I chose to install it here, so I can use it on class exercises and the final project. I loaded tidyverse to practice processing data and ggplot2 to visualize the processed data later in this exercise. I loaded naniar to determine missingness of the data.\n\nlibrary(dslabs)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(naniar)"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploring-gapminder",
    "href": "coding-exercise/coding-exercise.html#exploring-gapminder",
    "title": "R Coding Exercise",
    "section": "Exploring Gapminder",
    "text": "Exploring Gapminder\nI will explore the gapminder dataset included in this pacakge with the help(), str(), summary(), and class() functions.\n\nhelp(gapminder)\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nclass(gapminder)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing Data",
    "text": "Processing Data\nTo create a dataset with only African countries, I need to view the dataset using head() to determine a variable name that I can use to filter the dataset. Once I found “continent”, I filtered for Africa. The str() function shows me there are 2,907 observations of 9 variables, which means 2,907 African countries are included in the gapminder dataset. The summary() function allowed me to see what Dr. Handle meant by R keeping all the continent categories but plugging in a zero as a placeholder.\n\nhead(gapminder) ##I see a variable name called continent that I will use to filter the dataset\n\n              country year infant_mortality life_expectancy fertility\n1             Albania 1960           115.40           62.87      6.19\n2             Algeria 1960           148.20           47.50      7.65\n3              Angola 1960           208.00           35.98      7.32\n4 Antigua and Barbuda 1960               NA           62.97      4.43\n5           Argentina 1960            59.87           65.39      3.11\n6             Armenia 1960               NA           66.86      4.55\n  population          gdp continent          region\n1    1636054           NA    Europe Southern Europe\n2   11124892  13828152297    Africa Northern Africa\n3    5270844           NA    Africa   Middle Africa\n4      54681           NA  Americas       Caribbean\n5   20619075 108322326649  Americas   South America\n6    1867396           NA      Asia    Western Asia\n\nafricadata &lt;- gapminder %&gt;% \n                filter(continent == \"Africa\")  ##I created a new dataset that only includes African countries\nstr(africadata) \n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nTo create two new objects from the ‘africadata’ object with only two columns each, I used the select() function to choose the columns I wanted to keep. The first object, ‘IMLE’, contains the infant mortality rates and life expectancies for each African country. I used the str() function to see there are 2907 observations of 2 variables, and the summary() function provides 5 data points for each numeric variable. The second object, “PLE’, contains the population and life expectancies for each African county. Similarly, I used str() to confirm there are 2907 observations of 2 variables, and the summary() function to learn about the range of each variable.\n\nIMLE &lt;- africadata %&gt;%  ##store a new object using the arrow notation\n        select(\"infant_mortality\", \"life_expectancy\") ##pipe in the filtered dataset and select specific columns \nstr(IMLE) ##confirm there are only 2 variables \n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(IMLE) ##observe the range of each numerical variable\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nPLE &lt;- africadata %&gt;%  ##store a new object using the arrow notation\n          select(\"population\", \"life_expectancy\") ##pipe in the filtered dataset and select specific columns \nstr(PLE) ##confirm there are only 2 variables \n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(PLE) ##observe the range of each numerical variable\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting",
    "href": "coding-exercise/coding-exercise.html#plotting",
    "title": "R Coding Exercise",
    "section": "Plotting",
    "text": "Plotting\nTo visualize the relationship between the two variables included in each of these objects, I will plot them using ggplot(). Using the IMLE object, I plotted infant mortality on the x axis and life expenctancy on the y axis using the aesthetics filter with ggplot. I added geom-point() to plot the data as points. I also chose to include a title, which I centered on the graph, to make the contents of the plot even clearer. This plot shows the inverse relationship between life expectancy and infant mortality. The graph shows a strong inverse relationship with only 3 major outliers. The majority of observations fall between an infant mortality rate of 30 to 150.\n\nIMLE %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Life Expectancy as a Function of Infant Mortality\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) ##center the title on the graph\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nI followed a similar process to plot life expectancy based on population using the PLE object. I used ggplot() again along with geom_point() to plot the data points. I used the same technique to create a title centered on the graph. I also added the scale_x_log10() function to convert the numeric population variable to the logarithmic scale, which helps to create a cleaner visualization. The graph shows the most observations around a population of 1 million with a life expectancy ranging from 40 to 60. There is one major outlier on the graph, but most observations are clustered to create easily definable streaks. The streaks of data in both of these graphs correspond to observations in each African country over the years included in the dataset, which range from 1960 to 2016. As time passes, life expectancy increases which causes the population size to increase. Since infant mortality has decreased over this time period, life expectancy is increasing. These trends show up as streaks in the data because each country’s observations are grouped together over time.\n\nPLE %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Life Expectancy as a Function of Population\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) + ##center the title\n  scale_x_log10() ##change the x-axis to the log scale\n\nWarning: Removed 51 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-data-processing",
    "href": "coding-exercise/coding-exercise.html#more-data-processing",
    "title": "R Coding Exercise",
    "section": "More data processing",
    "text": "More data processing\nFinding out which observations have missing data is an important part of data analysis. I used gg_miss_var() to visualize which variables were missing information. While GDP accounts for the most missing observations, infant_mortality is the variable with the second most missing values. To determine which years have missing infant_mortality data, I used AI to learn how to use dplyr functions to filter for the specific missing data I am looking for. I filtered for observations where infant mortality rate was missing, and I selected the year column since that is the variable I am interested in. After printing the data, I realized it showed all 226 observations by year, so I used the unique() function to more efficiently determine which years had at least 1 missing infant mortality rate observation. Infant mortality rate is missing from 1960 to 1981 and again in 2016.\n\ngg_miss_var(africadata) ##explore missingness of the data\n\n\n\n\n\n\n\nmissing_years &lt;- africadata %&gt;%\n                  filter(is.na(infant_mortality)) %&gt;% ##find observations of infant mortality that are missing \n                  select(year) ##keep only the year column of the observations with missing data\nprint(missing_years) ##view the years with missing data\n\n    year\n1   1960\n2   1960\n3   1960\n4   1960\n5   1960\n6   1960\n7   1960\n8   1960\n9   1960\n10  1960\n11  1961\n12  1961\n13  1961\n14  1961\n15  1961\n16  1961\n17  1961\n18  1961\n19  1961\n20  1961\n21  1961\n22  1961\n23  1961\n24  1961\n25  1961\n26  1961\n27  1961\n28  1962\n29  1962\n30  1962\n31  1962\n32  1962\n33  1962\n34  1962\n35  1962\n36  1962\n37  1962\n38  1962\n39  1962\n40  1962\n41  1962\n42  1962\n43  1962\n44  1963\n45  1963\n46  1963\n47  1963\n48  1963\n49  1963\n50  1963\n51  1963\n52  1963\n53  1963\n54  1963\n55  1963\n56  1963\n57  1963\n58  1963\n59  1963\n60  1964\n61  1964\n62  1964\n63  1964\n64  1964\n65  1964\n66  1964\n67  1964\n68  1964\n69  1964\n70  1964\n71  1964\n72  1964\n73  1964\n74  1964\n75  1965\n76  1965\n77  1965\n78  1965\n79  1965\n80  1965\n81  1965\n82  1965\n83  1965\n84  1965\n85  1965\n86  1965\n87  1965\n88  1965\n89  1966\n90  1966\n91  1966\n92  1966\n93  1966\n94  1966\n95  1966\n96  1966\n97  1966\n98  1966\n99  1966\n100 1966\n101 1966\n102 1967\n103 1967\n104 1967\n105 1967\n106 1967\n107 1967\n108 1967\n109 1967\n110 1967\n111 1967\n112 1967\n113 1968\n114 1968\n115 1968\n116 1968\n117 1968\n118 1968\n119 1968\n120 1968\n121 1968\n122 1968\n123 1968\n124 1969\n125 1969\n126 1969\n127 1969\n128 1969\n129 1969\n130 1969\n131 1970\n132 1970\n133 1970\n134 1970\n135 1970\n136 1971\n137 1971\n138 1971\n139 1971\n140 1971\n141 1971\n142 1972\n143 1972\n144 1972\n145 1972\n146 1972\n147 1972\n148 1973\n149 1973\n150 1973\n151 1973\n152 1973\n153 1973\n154 1974\n155 1974\n156 1974\n157 1974\n158 1974\n159 1975\n160 1975\n161 1975\n162 1975\n163 1975\n164 1976\n165 1976\n166 1976\n167 1977\n168 1977\n169 1977\n170 1978\n171 1978\n172 1979\n173 1979\n174 1980\n175 1981\n176 2016\n177 2016\n178 2016\n179 2016\n180 2016\n181 2016\n182 2016\n183 2016\n184 2016\n185 2016\n186 2016\n187 2016\n188 2016\n189 2016\n190 2016\n191 2016\n192 2016\n193 2016\n194 2016\n195 2016\n196 2016\n197 2016\n198 2016\n199 2016\n200 2016\n201 2016\n202 2016\n203 2016\n204 2016\n205 2016\n206 2016\n207 2016\n208 2016\n209 2016\n210 2016\n211 2016\n212 2016\n213 2016\n214 2016\n215 2016\n216 2016\n217 2016\n218 2016\n219 2016\n220 2016\n221 2016\n222 2016\n223 2016\n224 2016\n225 2016\n226 2016\n\nunique(missing_years) ##organize the years with missing data\n\n    year\n1   1960\n11  1961\n28  1962\n44  1963\n60  1964\n75  1965\n89  1966\n102 1967\n113 1968\n124 1969\n131 1970\n136 1971\n142 1972\n148 1973\n154 1974\n159 1975\n164 1976\n167 1977\n170 1978\n172 1979\n174 1980\n175 1981\n176 2016\n\n\nAfter choosing 2000 for the year to study infant mortality rate based on missingness in the data, I filtered for observations made during 2000 only. I used the dim() function to confirm that I have 51 observations of 9 variables. The str() function provides the variable type for each variable, and the summary() function provides more details for each variable.\n\nyear2000 &lt;- africadata %&gt;% \n              filter(year == \"2000\") ##create a new object with only observations from the year 2000\ndim(year2000) ##confirm dimensions of year2000\n\n[1] 51  9\n\nstr(year2000) ##observe structure of year2000\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(year2000) ##view summaries of variables included in year 2000\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting",
    "href": "coding-exercise/coding-exercise.html#more-plotting",
    "title": "R Coding Exercise",
    "section": "More plotting",
    "text": "More plotting\nI made the same plots as above, excpet there is only data from the year 2000 this time. I made the same design choices to create a clear plot using labs() and theme() to visualize Infant Morality and Life Expectancy in 2000 and Population and Life Expectancy in 2000. Infant mortality and life expectancy still show an inverse relationship, but the relationship between population and life expectancy is no longer visible.\n\nyear2000 %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Infant Mortality and Life Expectancy in 2000\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) ##center the title\n\n\n\n\n\n\n\nyear2000 %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Infant Mortality and Life Expectancy in 2000\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) + ##center the title\n  scale_x_log10() ##convert the x axis to log sccale"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-model-fits",
    "href": "coding-exercise/coding-exercise.html#simple-model-fits",
    "title": "R Coding Exercise",
    "section": "Simple model fits",
    "text": "Simple model fits\nTo explore the relationship between population and life expectancy in 2000, I will use the lm() function with the year2000 object to create a simple model comparing life expectancy to infant mortality in fit 1 and to population in fit 2. Based on the summary() function output for fit1, we can conclude that infant mortality rate has a statistically significant effect on life expectancy because the p-value for the coefficient estimate is 2.83e-08, which is much smaller than 0.05. For every 1 unit increase in infant mortality rate, life expectancy decreases by 0.18916 years. Based on the summary() function output for fit2, we can conclude that there is not a statistically significant relationship between life expectancy and population because the p-value for the coefficient estimate is 0.616, which is much greater than 0.05.\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = year2000) ##create a linear model with life expectancy as outcome and infant mortality as predictor and save it to an object called fit1\nsummary(fit1) ##generate information about linear regression equation with p-values\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = year2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nfit2 &lt;- lm(life_expectancy ~ population, data = year2000) ##create a linear model with life expectancy as outcome and population as predictor and save it to an object called fit2\nsummary(fit2) ##generate information about linear regression equation with p-values\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = year2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nRachel Robertson contributed to this portion of Taylor’s portfolio.\n\nlibrary(dplyr)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\n\nThe following object is masked from 'package:naniar':\n\n    n_complete\n\nlibrary(tidyr) ##I begin by opening the additional libraries that I will need for this analysis"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploratory-data-analysis",
    "href": "coding-exercise/coding-exercise.html#exploratory-data-analysis",
    "title": "R Coding Exercise",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nI used the dslabs reference manual on CRAN to identify a dataset that intersted me. I chose the dataset us_contagious_diseases, which contains the variables disease (factor), state (factor), year(num), weeks_reporting(num), count(num), and population(num).I will begin by exploring this dataset with the dim(), str(), and summary() functions. The libraries needed for this analysis include tidyverse, dslabs, and ggplot2, and have been opened by running the the first code chunk.\n\ndim(us_contagious_diseases)\n\n[1] 16065     6\n\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n\nI found that there are 6 columns and 16065 rows in this dataset. These disease factors included are: Hepatitis A, Measles, Mumps, Pertussis, Polio, Rubella, and Smallpox.\nFor the purposes of this project, I will analyze one disease, Measels. This is because, although the U.S. is trying to erradicate the virus, vaccine undercoverage in recent years has lead to sporatic outbreaks in the U.S. in naive pockets.\nstr() reveals that there are 51 states listed, so I am curious what the 51st state is. To see all of the states listed I use the function, levels().\n\nlevels(us_contagious_diseases$state) ##Examine the levels of the factor called states\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District Of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n\n\nI found that the 51st state listed is District of Columbia. Since I am unsure if the Maryland values include or exclude District of Columbia, I will leave this state in the analysis but note this in the final figure.\nsummary() reveals that the years range from 1928 - 2011 and weeks reporting accounts for missing weeks. weeks reporting ranges from 0 - 52 which indicates some years with no data. I would like to find which years have missing data. A case count of 0 for any of the diseases is not necessarily missing data unless the there were 0 weeks_reporting for that year.\nBefore cleaning I will use gg_miss_var() to explore if there are any additional missing variables.\n\ngg_miss_var(us_contagious_diseases) ##find missing values\n\n\n\n\n\n\n\n\nIt seems that some population data might be missing as well so I will also have to filter the population data that has “NA” or missing data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#cleaning-the-data",
    "href": "coding-exercise/coding-exercise.html#cleaning-the-data",
    "title": "R Coding Exercise",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nFirst, I will remove the rows with missing population data. I will do this by using the filter function.\n\nus_contagious_diseases2 &lt;- us_contagious_diseases %&gt;% ##create a new data frame\n  drop_na(population) ##drop NA from the population factor\ngg_miss_var(us_contagious_diseases2) ##check that there are no longer missing variables\n\n\n\n\n\n\n\n\nNow that the NA values for population have been dropped, we may continue to clean the data.\nI will now remove the years that are missing data for weeks_reporting.I will find the identify the years that have a 0 value for weeks_reporting by using the filter() function and drop_na() function. I will then check the number of data rows using the skim() function.\n\nus_contagious_diseases3 &lt;- us_contagious_diseases2 %&gt;% ## creating new data frame\n  dplyr::filter(weeks_reporting != 0) %&gt;% ##finding the weeks_reporting 0 values with dplyr and setting then to NA\n  tidyr::drop_na(weeks_reporting) ##dropping the NA in weeks_reporting and dropping them\nskimr::skim(us_contagious_diseases3) ##checking the number of rows\n\n\nData summary\n\n\nName\nus_contagious_diseases3\n\n\nNumber of rows\n14228\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndisease\n0\n1\nFALSE\n7\nMea: 3319, Per: 2709, Hep: 2327, Pol: 1844\n\n\nstate\n0\n1\nFALSE\n51\nCal: 312, Tex: 311, Ill: 306, Mic: 306\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1969.97\n23.07\n1928\n1948\n1973\n1989\n2011\n▆▅▆▇▅\n\n\nweeks_reporting\n0\n1\n42.11\n12.16\n1\n38\n47\n51\n52\n▁▁▁▂▇\n\n\ncount\n0\n1\n1682.64\n6226.76\n0\n17\n110\n677\n132342\n▇▁▁▁▁\n\n\npopulation\n0\n1\n4254904.47\n4792670.73\n86853\n1089254\n2827642\n5162987\n37607525\n▇▁▁▁▁\n\n\n\n\n\nThe data is now clean enough for additional processing."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-the-data",
    "href": "coding-exercise/coding-exercise.html#processing-the-data",
    "title": "R Coding Exercise",
    "section": "Processing the Data",
    "text": "Processing the Data\nI will find a flat incidence rate by diving count by population for each state.I will use the mutate function to create a new column called incidence.\n\nus_contagious_diseases4 &lt;- us_contagious_diseases3 %&gt;% \n  mutate(incidence = count/population) ##create variable called incidence based on count/population\nsummary(us_contagious_diseases4) ##confirm variable\n\n        disease               state            year      weeks_reporting\n Hepatitis A:2327   California   :  312   Min.   :1928   Min.   : 1.00  \n Measles    :3319   Texas        :  311   1st Qu.:1948   1st Qu.:38.00  \n Mumps      :1576   Illinois     :  306   Median :1973   Median :47.00  \n Pertussis  :2709   Michigan     :  306   Mean   :1970   Mean   :42.11  \n Polio      :1844   Florida      :  305   3rd Qu.:1989   3rd Qu.:51.00  \n Rubella    :1374   Massachusetts:  302   Max.   :2011   Max.   :52.00  \n Smallpox   :1079   (Other)      :12386                                 \n     count          population         incidence        \n Min.   :     0   Min.   :   86853   Min.   :0.000e+00  \n 1st Qu.:    17   1st Qu.: 1089254   1st Qu.:7.152e-06  \n Median :   110   Median : 2827642   Median :3.905e-05  \n Mean   :  1683   Mean   : 4254904   Mean   :5.678e-04  \n 3rd Qu.:   677   3rd Qu.: 5162987   3rd Qu.:2.538e-04  \n Max.   :132342   Max.   :37607525   Max.   :2.964e-02  \n                                                        \n\n\nI am going to change raw incidence rate into incidence per 100,000, which is a standard measure and allows for easier visualization of the incidence rate.\n\nus_contagious_diseases5 &lt;- us_contagious_diseases4 %&gt;% \n  mutate(incidenceper100000 = incidence * 100000) ##create new variable incidenceper100000 based on incidence times 100000\nsummary(us_contagious_diseases4) ##confirm variable\n\n        disease               state            year      weeks_reporting\n Hepatitis A:2327   California   :  312   Min.   :1928   Min.   : 1.00  \n Measles    :3319   Texas        :  311   1st Qu.:1948   1st Qu.:38.00  \n Mumps      :1576   Illinois     :  306   Median :1973   Median :47.00  \n Pertussis  :2709   Michigan     :  306   Mean   :1970   Mean   :42.11  \n Polio      :1844   Florida      :  305   3rd Qu.:1989   3rd Qu.:51.00  \n Rubella    :1374   Massachusetts:  302   Max.   :2011   Max.   :52.00  \n Smallpox   :1079   (Other)      :12386                                 \n     count          population         incidence        \n Min.   :     0   Min.   :   86853   Min.   :0.000e+00  \n 1st Qu.:    17   1st Qu.: 1089254   1st Qu.:7.152e-06  \n Median :   110   Median : 2827642   Median :3.905e-05  \n Mean   :  1683   Mean   : 4254904   Mean   :5.678e-04  \n 3rd Qu.:   677   3rd Qu.: 5162987   3rd Qu.:2.538e-04  \n Max.   :132342   Max.   :37607525   Max.   :2.964e-02  \n                                                        \n\n\nI will add a weight to this value to account for missing weeks reported. To do this I will multiply each incidence rate by (weeks_reporting/52), where weeks_reporting is the total number of weeks where cases are counted and 52 is the total number of weeks in a year.I will call this new variable IRweighted.\n\nus_contagious_diseases6 &lt;- us_contagious_diseases5 %&gt;% \n  mutate(IRweighted = incidenceper100000*(weeks_reporting/52)) ##add weight to incidence rate per 100000 to account for differing numbers of weeks that case counts are captured\nsummary(us_contagious_diseases6) ##confirm variable\n\n        disease               state            year      weeks_reporting\n Hepatitis A:2327   California   :  312   Min.   :1928   Min.   : 1.00  \n Measles    :3319   Texas        :  311   1st Qu.:1948   1st Qu.:38.00  \n Mumps      :1576   Illinois     :  306   Median :1973   Median :47.00  \n Pertussis  :2709   Michigan     :  306   Mean   :1970   Mean   :42.11  \n Polio      :1844   Florida      :  305   3rd Qu.:1989   3rd Qu.:51.00  \n Rubella    :1374   Massachusetts:  302   Max.   :2011   Max.   :52.00  \n Smallpox   :1079   (Other)      :12386                                 \n     count          population         incidence         incidenceper100000 \n Min.   :     0   Min.   :   86853   Min.   :0.000e+00   Min.   :   0.0000  \n 1st Qu.:    17   1st Qu.: 1089254   1st Qu.:7.152e-06   1st Qu.:   0.7152  \n Median :   110   Median : 2827642   Median :3.905e-05   Median :   3.9055  \n Mean   :  1683   Mean   : 4254904   Mean   :5.678e-04   Mean   :  56.7780  \n 3rd Qu.:   677   3rd Qu.: 5162987   3rd Qu.:2.538e-04   3rd Qu.:  25.3760  \n Max.   :132342   Max.   :37607525   Max.   :2.964e-02   Max.   :2964.4269  \n                                                                            \n   IRweighted       \n Min.   :   0.0000  \n 1st Qu.:   0.5211  \n Median :   3.1708  \n Mean   :  53.0719  \n 3rd Qu.:  22.7544  \n Max.   :2518.9571  \n                    \n\n\nNow that I have the weighted measure of incidence rate (per 100000), I will make some exploratory figures to examine changes in Measels incidence over time for each state."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploratory-figures",
    "href": "coding-exercise/coding-exercise.html#exploratory-figures",
    "title": "R Coding Exercise",
    "section": "Exploratory Figures",
    "text": "Exploratory Figures\nI will plot weighted incidence rate over time in a scatter plot and add color by state to examine potential state differences.I will make a separate figure for each disease. I had to ask ChatGPT how to plot a certain category of data from a column without making a new dataframe. this is where I found the grep1(value, dataframe$var) function and structure.\n\nMeasles_plot &lt;- ggplot(data = us_contagious_diseases6[grepl('Measles', us_contagious_diseases6$disease), ], aes(x = year, y = IRweighted, color = state)) +\n  geom_line() +\n  labs(x = 'Year', y = 'IRweighted', title = 'Measles Incidence Rates (per 100000) in the U.S. from 1928 to 2011') ##add line plot of measles in the U.S. from 1928 to 2011, depending the state\nprint(Measles_plot) ##display the line plot\n\n\n\n\n\n\n\n\nI had to open this figure to view the lines. It seems that every several years, there is a measels outbreak that hits many states in the U.S. at once witha rapid peaking incidence and then rapidly declines as the population gains immunity. The peaks are particularly high for the state in the pink/ purple shade. Between 1980 to 2000 the measels IRweighted seems to diminish to near 0.\nIt is difficult to distinguish between states, so I change the data to show an IRweighted total for all of the states. ChatGPT said that the aggregate function can be used to sum the IRweighted for the state values that are equal.\n\nagg_data_measles &lt;- aggregate(IRweighted ~ year + state, data = subset(us_contagious_diseases6, disease == 'Measles'), sum) ##This aggregate function and structure was found by using ChatGPT. The subset function was used to aggregate only the measels disease data.\n\nUSmeasles_plot &lt;- ggplot(data = agg_data_measles, aes(x = year, y = IRweighted)) +\n   geom_line() +\n   labs(x = 'Year', y = 'Total IRweighted (Measles)', title = 'Total Measels Incidence Rate (per 100000) in the U.S. from 1928 to 2011') ##Another line plot is made with ggplot, but from the aggregated data frame\n\nprint(USmeasles_plot) ##display the cumulative plot\n\n\n\n\n\n\n\n\nA seasonal epi curve can be observed for the national sum of incidence rate of measels in the U.S. There are yearly peaks from 1928 until they begin to rapidly decrease around 1965, eventually reaching close to 0 in 1980. This might be do to the MMR development and distribution in the US in 1971.\nMMR elicits protection against Measels, Mumps, and Rubella. Because of the vaccine distribution, we should also see a decrease in Mumps incidence. To determine whether this is the case, I will also plot the Mumps incidence rate over time in comparison to the latest plot.\n\nagg_data_mumps &lt;- aggregate(IRweighted ~ year + state, data = subset(us_contagious_diseases6, disease == 'Mumps'), sum) ##Making a new aggregated data frame from the Mumps disease data\n\n## Plot the aggregated data for Mumps instead of measels\nUSmumps_plot &lt;- ggplot(data = agg_data_mumps, aes(x = year, y = IRweighted)) +\n   geom_line() +\n   labs(x = 'Year', y = 'Total IRweighted (Mumps)', title = 'Total Mumps Incidence Rate (per 100000) in the U.S. from 1928 to 2011')\n\nprint(USmumps_plot) ##Display the Mumps IR plot\n\n\n\n\n\n\n\n\nThough with much lower incidence rates, the mumps data set also shows a rapid decrease in incidence rates from 1970 to 1980. There is no incidence rate data prior to around 1965. For this reason, I will also plot Rubella.\n\nagg_data_rubella &lt;- aggregate(IRweighted ~ year + state, data = subset(us_contagious_diseases6, disease == 'Rubella'), sum) ##Making a new aggregated data frame from the Mumps disease data\n\n## Plot the aggregated data for Mumps instead of measels\nUSrubella_plot &lt;- ggplot(data = agg_data_rubella, aes(x = year, y = IRweighted)) +\n   geom_line() +\n   labs(x = 'Year', y = 'Total IRweighted (Rubella)', title = 'Total Rubella Incidence Rate (per 100000) in the U.S. from 1928 to 2011')\n\nprint(USrubella_plot) ## Display the rubella IR plot\n\n\n\n\n\n\n\n\nThe Rubella data shows a similar decrease from 1970 to 1980 but does not include data before around 1965. This makes it unclear whether the decrease in all three disease incidence rates is due to the MMR vaccine."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-statistical-models",
    "href": "coding-exercise/coding-exercise.html#simple-statistical-models",
    "title": "R Coding Exercise",
    "section": "Simple Statistical Models",
    "text": "Simple Statistical Models\nI am interested in looking at the MMR incidence rates decrease over time. For this, I can use the total IRweighted aggregate data as the outcome and compare it to the year as the predictor.\nI will start with Measels\n\nMeasels_fit &lt;- lm(IRweighted~ year, data = agg_data_measles) ##create a linear model with Measels as outcome and year as predictor and save it to an object called Measels_fit\nMeasels_fit_table &lt;- broom::tidy(Measels_fit) ##Adding a simplified table of the linear model with tidy\nprint(Measels_fit_table) ##print table of the linear regression equation with p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 13793.     400.         34.4 1.66e-222\n2 year           -6.94     0.204     -34.0 1.32e-217\n\n\nMeasels incidence rate (per 100000) decreased by 6.9 units per year in the U.S. This is a significant decrease with a p-value that is alpha&lt;0.05.\nI will now use the lm() function to produce a linear model Mumps over year\n\nMumps_fit &lt;- lm(IRweighted~ year, data = agg_data_mumps) ##create a linear model with Mumps as outcome and year as predictor and save it to an object called Mumps_fit\nMumps_fit_table &lt;- broom::tidy(Mumps_fit) ##Adding a simplified table of the linear model with tidy\nprint(Mumps_fit_table) ##print table of the linear regression equation with p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  3596.    164.          21.9 3.91e-93\n2 year           -1.80    0.0827     -21.8 1.77e-92\n\n\nMumps incidence rates (per 100000) decreased by 1.8 per one year. This is a smaller unit change, but still a significant decrease with a p-value of alpha&lt;0.005.\nLastly, I will confirm this relationship with the Rubella aggregate data in the U.S.\n\nRubella_fit &lt;- lm(IRweighted~ year, data = agg_data_rubella) ##create a linear model with rubella as outcome and year as predictor and save it to an object called Rubella_fit\nRubella_fit_table &lt;- broom::tidy(Rubella_fit) ##Adding a simplified table of the linear model with tidy\nprint(Rubella_fit_table) ##print table of the linear regression equation with p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 1646.      83.6         19.7 3.49e-76\n2 year          -0.827    0.0422     -19.6 1.46e-75\n\n\nThere is decrease in Rubella incidence rates (per 100000) of 0.83 over one year. This decrease, although the smallest, is significant with a p-value of alpha&lt;0.05.\nOverall, these exploratory analysis, figures, and models displayed a significant decrease in Measels, Mumps, and Rubella incident rates in the U.S. following the introduction of the MMR vaccine in 1971."
  },
  {
    "objectID": "data-exercise/data-exercise.html#find-text-data-to-analyze",
    "href": "data-exercise/data-exercise.html#find-text-data-to-analyze",
    "title": "Data Exercise",
    "section": "Find text data to analyze",
    "text": "Find text data to analyze\nAfter reading the text mining chapter in the IDS textbook, I learned about Project Gutenberg, which is a digital archive of books that are publicly available. I installed the Gutenbergr package, loaded the library, and searched through some of the options. I chose to analyze Alice’s Adventures in Wonderland which has a Gutenberg ID of 11. I downloaded it from Project Gutennberg and saved it as ‘book’.\n\n## install.packages(\"gutenbergr\")\nlibrary(gutenbergr)\nlibrary(stringr)\ngutenberg_metadata\n\n# A tibble: 72,569 × 8\n   gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n          &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              \n 1            1 \"The De… Jeffe…                1638 en       \"Politics/American…\n 2            2 \"The Un… Unite…                   1 en       \"Politics/American…\n 3            3 \"John F… Kenne…                1666 en       \"\"                 \n 4            4 \"Lincol… Linco…                   3 en       \"US Civil War\"     \n 5            5 \"The Un… Unite…                   1 en       \"United States/Pol…\n 6            6 \"Give M… Henry…                   4 en       \"American Revoluti…\n 7            7 \"The Ma… &lt;NA&gt;                    NA en       \"\"                 \n 8            8 \"Abraha… Linco…                   3 en       \"US Civil War\"     \n 9            9 \"Abraha… Linco…                   3 en       \"US Civil War\"     \n10           10 \"The Ki… &lt;NA&gt;                    NA en       \"Banned Books List…\n# ℹ 72,559 more rows\n# ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;\n\ngutenberg_works\n\nfunction (..., languages = \"en\", only_text = TRUE, rights = c(\"Public domain in the USA.\", \n    \"None\"), distinct = TRUE, all_languages = FALSE, only_languages = TRUE) \n{\n    dots &lt;- lazyeval::lazy_dots(...)\n    if (length(dots) &gt; 0 && any(names(dots) != \"\")) {\n        cli::cli_abort(c(x = \"We detected a named input.\", i = \"Use == expressions, not named arguments.\", \n            i = \"For example, use gutenberg_works(author == 'Dickens, Charles'),\", \n            i = \"not gutenberg_works(author = 'Dickens, Charles').\"))\n    }\n    ret &lt;- filter(gutenberg_metadata, ...)\n    if (!is.null(languages)) {\n        lang_filt &lt;- gutenberg_languages %&gt;% filter(language %in% \n            languages) %&gt;% count(gutenberg_id, total_languages)\n        if (all_languages) {\n            lang_filt &lt;- lang_filt %&gt;% filter(n &gt;= length(languages))\n        }\n        if (only_languages) {\n            lang_filt &lt;- lang_filt %&gt;% filter(total_languages &lt;= \n                n)\n        }\n        ret &lt;- ret %&gt;% filter(gutenberg_id %in% lang_filt$gutenberg_id)\n    }\n    if (!is.null(rights)) {\n        .rights &lt;- rights\n        ret &lt;- filter(ret, rights %in% .rights)\n    }\n    if (only_text) {\n        ret &lt;- filter(ret, has_text)\n    }\n    if (distinct) {\n        ret &lt;- distinct(ret, title, gutenberg_author_id, .keep_all = TRUE)\n        if (any(colnames(ret) == \".keep_all\")) {\n            ret$.keep_all &lt;- NULL\n        }\n    }\n    ret\n}\n&lt;bytecode: 0x7f82dcaaf970&gt;\n&lt;environment: namespace:gutenbergr&gt;\n\nbook &lt;- gutenberg_download(11)\n\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n\n\nUsing mirror http://aleph.gutenberg.org\n\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/11/11.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\n\nWarning: Unknown or uninitialised column: `text`.\n\nhead(book)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: gutenberg_id &lt;int&gt;, text &lt;chr&gt;"
  },
  {
    "objectID": "data-exercise/data-exercise.html#create-tidy-table-with-all-meaningful-words",
    "href": "data-exercise/data-exercise.html#create-tidy-table-with-all-meaningful-words",
    "title": "Data Exercise",
    "section": "Create tidy table with all meaningful words",
    "text": "Create tidy table with all meaningful words\nAfter installing and loading the tidytext package, I created a table with all the words in the book. I create another column to assign a word number to each word, so I can do sentiment analysis later in the exercise. I used the stop_words dataset to get rid of the stop words, so I could explore meaningful words that are most commonly included in the book. I used AI to assist me in removing all words that were numbers as well, which uses a function from the stringr package. I made another tibble with the top 50 most used words. ‘said’ is the most commonly used word with 460 uses, which is followed by ‘Alice’ being used 386 times and ‘little’ being used 127 times.\n\n## install.packages(\"tidytext\")\nlibrary(tidytext)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nwords &lt;- book %&gt;%\n            unnest_tokens(word, text) %&gt;% \n            mutate(wordID = row_number())\nhead(words)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: gutenberg_id &lt;int&gt;, word &lt;chr&gt;, wordID &lt;int&gt;\n\nstop_words &lt;- get_stopwords()\nwords_clean &lt;-  words %&gt;% \n                  anti_join(stop_words) %&gt;% \n                  filter(!str_detect(word, \"^\\\\d+$\"))\n\nJoining with `by = join_by(word)`\n\nhead(words_clean)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: gutenberg_id &lt;int&gt;, word &lt;chr&gt;, wordID &lt;int&gt;\n\ntop_words &lt;- words_clean %&gt;% \n                count(word) %&gt;% \n                top_n(50, n) %&gt;% \n                arrange(desc(n))\nhead(top_words)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: word &lt;chr&gt;, n &lt;int&gt;"
  },
  {
    "objectID": "data-exercise/data-exercise.html#create-a-sentiment-analysis",
    "href": "data-exercise/data-exercise.html#create-a-sentiment-analysis",
    "title": "Data Exercise",
    "section": "Create a sentiment analysis",
    "text": "Create a sentiment analysis\nTo complete sentiment analysis, I had to install another package called ‘textdata’ and load ‘tidytext’ to explore the ‘bing’ and ‘afinn’ lexicons. I used the “bing” lexicon to assign positive or negative values to the words, and I used the ‘afinn’ lexicon to assign a score or value to each word. There are 4781 positive and 2005 negative words included in the ‘bing’ lexicon. There are only 16 words that get assigned the most negative score of -5 in the ‘afinn’ lexicon with most words scoring between a negative 3 and positive 3. Only 1 word is considered neutral with a score of 0. I used the inner_join function with the clean words tibble to assign a sentiment to the meaningful words. The top five words are all negative with ‘pig’,‘mad’, ‘mock’, ‘stole’, and ‘tired’ being assigned negative sentiments. To explore this observation further, I used the ‘afinn’ option to find the scores for each word.For example, ‘mad’ was assigned a -3 and ‘mock’ wass assigned a -2, which is interesting because I would consider ‘mock’ to be more negative than ‘mad’ personally.\n\n## install.packages(\"textdata\")\nlibrary(textdata)\nlibrary(tidytext)\nget_sentiments(\"bing\") %&gt;% count(sentiment)\n\n# A tibble: 2 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 negative   4781\n2 positive   2005\n\nget_sentiments(\"afinn\") %&gt;% count(value)\n\n# A tibble: 11 × 2\n   value     n\n   &lt;dbl&gt; &lt;int&gt;\n 1    -5    16\n 2    -4    43\n 3    -3   264\n 4    -2   966\n 5    -1   309\n 6     0     1\n 7     1   208\n 8     2   448\n 9     3   172\n10     4    45\n11     5     5\n\nbing &lt;- get_sentiments(\"bing\") %&gt;% \n          select(word, sentiment)\nalice_bing &lt;- words_clean %&gt;% \n                inner_join(bing, by = \"word\", relationship = \"many-to-many\") \nhead(alice_bing)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: gutenberg_id &lt;int&gt;, word &lt;chr&gt;, wordID &lt;int&gt;, sentiment &lt;chr&gt;\n\nafinn &lt;- get_sentiments(\"afinn\") %&gt;%\n          select(word, value)\nalice_affin &lt;-words_clean %&gt;% \n                inner_join(afinn, by =\"word\", relationship = \"many-to-many\")\nhead(alice_affin)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: gutenberg_id &lt;int&gt;, word &lt;chr&gt;, wordID &lt;int&gt;, value &lt;dbl&gt;"
  },
  {
    "objectID": "data-exercise/data-exercise.html#visualize-the-sentiment-analysis",
    "href": "data-exercise/data-exercise.html#visualize-the-sentiment-analysis",
    "title": "Data Exercise",
    "section": "Visualize the sentiment analysis",
    "text": "Visualize the sentiment analysis\nI loaded the ggplot2 package to create visualizations of the sentiments throughout the book. I used the ‘alice_affin’ dataset because it already includes the sentiment value assigned to each word. The smoother line on the scatterplot shows that the overall sentiment throughout the book hovers around neutral with slightly higher levels of positive words in the beginning. Words with a score of 2 and -2 are consistently spread throughout the book, which makes sense given most words receive those mild scores. The extremely positive words receiving a 4 on the ‘afinn’ scale are spread sparsely through the book. The most negative words receiving a score of -3 are seen more frequently throughout the book with a cluster around word number 12,500, which corresponds to the midpoint of the book roughly. I loaded the ‘here’ library, so I could save this scatterplot in my portfolio.\n\nlibrary(ggplot2)\n\nsentiments &lt;- ggplot(alice_affin, aes(wordID, value)) + \n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"loess\", se = FALSE, color = \"pink\") +\n    labs(title = \"Alice's Adventure in Wonderland Sentiments\", \n         x= \"Word Number\", \n         y= \"Sentiment Value\")\nprint(sentiments)\n\n\n\n\n\n\n\nlibrary(here)\n\nhere() starts at /Users/taylorglass/Documents/MADA /taylorglass-MADA-portfolio\n\nfigure_file = here(\"data-exercise\",\"sentiments.png\")\nggsave(filename = figure_file, plot = sentiments)\n\nSaving 7 x 5 in image\n\n\nI used AI to assist me in writing the code to generate a visualization of the average sentiment for each page. I assumed there were 250 words per page and created a new variable assigning word number to a page number in ‘alice_affin’. I computed the average sentiment per page to create a variable for the y-axis of the visualization and created a new datset grouped by that page variable called ‘pagesentiment’. The scatterplot shows a random distribution of average sentiment scores across all the pages of the book. The smoother line shows the first 60 pages are slightly more positive on the ‘affin’ scale while pages 60 to 90 are balanced to be fairly neutral. Around page 120, we see an increase in the average sentiment score, which makes sense because most stories like Alice’s Adventure in Wonderland end on a positive note. I also saved this plot to my portfolio.\n\nwords_per_page &lt;- 250\n\nalice_affin &lt;- alice_affin %&gt;% \n              mutate(page = ceiling(wordID / words_per_page))\n\npagesentiment&lt;- alice_affin %&gt;%\n                  group_by(page) %&gt;%\n                  summarise(average_sentiment = mean(value))\n\navgsent &lt;- ggplot(pagesentiment, aes(x=page, y=average_sentiment)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se=FALSE, color = \"pink\") + \n  labs(title = \"Average Sentiment by Page\", \n       x= \"Page Number\", \n       y= \"Average Sentiment Score\")\nprint(avgsent)\n\n\n\n\n\n\n\nfigure_file &lt;- here(\"data-exercise\", \"avgsent.png\")\nggsave(filename = figure_file, plot = avgsent)\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Welcome!\n\nMy name is Taylor Glass.\nWelcome to my website and data analysis portfolio for the Modern Applied Data Analysis course at the University of Georgia.\n\nPlease use the Menu Bar above to look around.\nI have added information about me, and I plan to add examples of my work from this course as the semester progresses.\nStay tuned for more updates and content!"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "This is an exercise to understand tidymodels better. We will use data on a drug candidate called Mavoglurant. The data is store as a .csv file, and it was downloaded from the original Github repository.\n\n\n\nlibrary(readr) # to load in the data\nlibrary(here) # to load in the data \nlibrary(ggplot2) # to create visualizations \nlibrary(dplyr) # for data processing/wrangling\nlibrary(gtsummary) # to create summary tables \nlibrary(knitr) # to create summary tables\nlibrary(tidymodels) # to fit models\nlibrary(pROC) # to find the ROC curve \n\n\n\n\n\nmavo &lt;- read_csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\n\n\n\nWe can create a quick visual of the data by plotting DV, the outcome variable, over time. We will stratify by dose and use ID as a grouping factor. The line graph is busy and not particularly helpful. It could be useful to zoom in and limit the time to 25 hours because most of the variation occurs before that threshold.\n\n# plot DV as a function of TIME, stratified by DOSE and grouped by ID\nmavo %&gt;% \n  ggplot(aes(x=TIME, y=DV, group= as.factor(ID), color = as.factor(DOSE))) + \n  geom_line() +\n  facet_wrap( ~ DOSE, scales = \"free_y\")\n\n\n\n\nThis line graph is strongly skewed to the right with the highest drug concentration values DV being observed within the first 2 hours. As time progresses, the drug concentration sharply decreases until it reaches 0 around 48 hours.\nAs we look closer, some individuals received the drug more than once and have observations for both OCC=1 and OCC=2. We will only include participants who received the drug once to keep this analysis manageable.\n\n# filter for those with one drug dosage\nmavo2 &lt;- mavo %&gt;% \n          filter(OCC == 1)\n\n# check the filtering worked\nsummary(mavo2$OCC)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       1       1       1       1 \n\n\nAll individuals have an entry at time 0 that has DV=0 and a non-zero entry for the AMT variable. The dosing entry for everyone is located in the AMT variable. All other entries are the time-series values for the drug concentration. We will create a new value for the total amount of drug for each individual by adding all the DV values. This approach should not be used outside of this practice example because individuals may have various amounts of observations for this variable. A better approach would be using integration to get the area under the curve with a simple trapezoid rule, but we will keep things simple here. Once we create the new total drug concentration variable Y for those with time that does not equal zero and for those with time that does equal zero, we combine the datasets, so there is a single dataset with TIME=0 for everybody.\n\n# exclude individuals with TIME = 0 and create new variable Y as the sum of DV per individual \nmavo3 &lt;- mavo2 %&gt;% \n  filter(TIME != 0) %&gt;%\n  group_by(ID) %&gt;%  # this line allows the calculation to be done per individual \n  reframe(Y = sum(DV))\n\n# keep individuals with TIME = 0 only \nmavo4 &lt;- mavo2 %&gt;% \n  filter(TIME == 0) \n\n# combine the two datasets\nmavo5 &lt;- left_join(mavo4, mavo3, by = \"ID\")\n\nWe only need a few of these variables to create our model. We will slim down the dataset to only include Y, DOSE, AGE, SEX, RACE, WT, and HT. It will also be beneficial for the RACE and SEX variables to be factors.\n\nmavo5 &lt;- mavo5 %&gt;% \n            select(Y, DOSE, AGE, SEX, RACE, WT, HT) %&gt;% \n            mutate(SEX = as.factor(SEX),\n                   RACE = as.factor(RACE))\n\n\n\n\n\n\nTo get a better understanding of the data, I made a simple summary table to examine the distribution for each of the variables.\n\n# create simple summary table\nsummary &lt;- mavo5 %&gt;% \n            tbl_summary()\nsummary\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 1201\n    \n  \n  \n    Y\n2,349 (1,701, 3,050)\n    DOSE\n\n        25\n59 (49%)\n        37.5\n12 (10%)\n        50\n49 (41%)\n    AGE\n31 (26, 40)\n    SEX\n\n        1\n104 (87%)\n        2\n16 (13%)\n    RACE\n\n        1\n74 (62%)\n        2\n36 (30%)\n        7\n2 (1.7%)\n        88\n8 (6.7%)\n    WT\n82 (73, 90)\n    HT\n1.77 (1.70, 1.81)\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\nThe average total dose concentration across the 120 observations is 2,349. Dosage is split into three discrete options: 25, 37.5, and 50. Almost 50% of the observations took the 25 unit dose and 41% took the 50 unit dose. The average age is 31 years, and the average weight is 82 units. The average height is 1.77, which must be stored in meters. Sex is very unevenly distributed with one gender accounting for 87% of observations. Race is also unevenly distributed with 62% of observations accounted for by one race.\nI created another table that summarized the numeric variables by SEX because I noticed that the distribution was so uneven. I wanted to examine the differences in the numeric variables by sex.\n\n# create summary table of numeric variables by SEX\nnumtable&lt;-  mavo5 %&gt;% \n              group_by(SEX) %&gt;%\n                summarize(across(c(Y:AGE, WT, HT), mean))\nkable(numtable)\n\n\n\n\nSEX\nY\nDOSE\nAGE\nWT\nHT\n\n\n\n\n1\n2477.643\n37.01923\n31.79808\n84.09808\n1.779333\n\n\n2\n2235.874\n32.81250\n40.81250\n72.51875\n1.626908\n\n\n\n\n\nFor whichever sex is denoted by 1, the average total drug concentration is higher at 2477.643 units compared to 2235.874 for sex 2. DOSE, HT, and WT also have higher averages among the SEX=1 group, so I hypothesize that 1 represents males and 2 represents females. It is interesting that the average age among the SEX=2 group is 9 years older compared to the SEX=1 group.\n\n\n\nI will create histograms to explore each of the numeric variables to understand their distributions better.\n\n# histogram of total drug concentration `Y`\nplot1 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= Y)) + \n              geom_histogram() + \n                labs(x=\"Total Drug Concentration\", y=\"Frequency\", title = \"Distribution of Total Drug Concentration\")\nplot1\n\n\n\n\nPlot 1 reveals that total drug concentration is almost normally distributed with a few trailing outliers that are greater than 4000 units. The lowest total drug concentration is around 750 units, and the highest outliter is about 5600 units. The majoirty of observations fall between 1000 and 4000 units.\n\n# histogram of DOSE\nplot2 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= DOSE)) + \n              geom_histogram() + \n                labs(x=\"Mavoglurant Dosage\", y=\"Frequency\", title = \"Distribution of Dosage\")\nplot2\n\n\n\n\nPlot 2 reveals that there are three distinct options for Mavoglurant dosage around 25, 37.5, and 50 units. Almost half of the observations take the 25 unit dosage, and the 37.5 unit dosage is the most uncommmon.\n\n# histogram of AGE \nplot3 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= AGE)) + \n              geom_histogram() + \n                labs(x=\"Age\", y=\"Frequency\", title = \"Distribution of Age\")\nplot3\n\n\n\n\nPlot 3 reveals that age is not normally distributed. The two ages accounting for the most observations are around 26 and 27 years. There are also peaks in the histogram at 37 years and 30 years. This graph shows random variation in the age of the 120 observations.\n\n# histogram of height `HT`\nplot4 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= HT)) + \n              geom_histogram() + \n                labs(x=\"Height\", y=\"Frequency\", title = \"Distribution of Height\")\nplot4\n\n\n\n\nPlot 4 reveals height to be approximately normally distributed with a peak around 1.8 units. There are a few low outliers between 1.5 and 1.6 units, but the majority of observations have a heigt between 1.62 and 1.91 unit.\n\n# histogram of weight `WT`\nplot5 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= WT)) + \n              geom_histogram() + \n                labs(x=\"Weight\", y=\"Frequency\", title = \"Distribution of Weight\")\nplot5\n\n\n\n\nPlot 5 reveals that weight is approximately normally distributed with a peak around 82 units. The lowest weight is about 55 units, and the highest weight is about 116 units. Most of the observations fall between 68 and 100 units.\nIt is also important to explore the distributions of the two categorical variables using bar graphs: RACE and SEX.\n\n# bar graph of race variable \nplot6 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= RACE)) + \n              geom_bar() + \n                labs(x=\"Race\", y=\"Frequency\", title = \"Distribution of Race\")\nplot6\n\n\n\n\nPlot 6 shows that race is denoted as four discrete numeric options: 1, 2, 7, and 88. Over 70 observations are accounted for in the RACE=1 category which is almost twice as large as the RACE=2 category. RACE=7 has the fewest observations, ann RACE=88 has about 8 observations. It is almost impossible to determine which race is represented by each category, but RACE=88 probably accounts for missing information.\n\n# bar graph of sex variable\nplot7 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= SEX)) + \n              geom_bar() + \n                labs(x=\"Sex\", y=\"Frequency\", title = \"Distribution of Sex\")\nplot7\n\n\n\n\nPlot 7 confirms what we already know about sex being unevenly distributed. SEX=1 accounts for over 100 observations, which is almost 5 times as many observations included in the SEX=2 category.\nI will move on to creating scatterplots and boxplots to examine the relationship between the main outcome of interest Y and predictor variables including DOSE, AGE, HT, WT, RACE, and SEX.\n\n# scatter plot of dosage and total drug concentration \nplot8 &lt;- mavo5 %&gt;% \n            mutate(DOSE = as.factor(DOSE)) %&gt;% \n              ggplot(aes(x= DOSE, y=Y)) + \n                geom_boxplot() + \n                  labs(x=\"Dose\", y=\"Total Drug Concentration\", title = \"Dosage Related to Total Drug Concentration\")\nplot8\n\n\n\n\nI converted the DOSAGE variable to a factor variable here because I felt like a boxplot would display the information better than a scatterplot. The boxplot can be used to determine that the 50 unit dose accounts for the highest average total drug concentration. Total drug concentration decreases as dosage decreases to 37.5 and 25 units, which makes sense intuitively. There are a few outliers in each dosage category, but they do not seem to affect the overall trend.\n\n# scatter plot of age and total drug concentration\nplot9 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= AGE, y=Y)) + \n              geom_point() + \n                labs(x=\"Age\", y=\"Total Drug Concentration\", title = \"Age Related to Total Drug Concentration\")\nplot9\n\n\n\n\nPlot 9 shows that there is no correlation between age and total drug concentration. The points appear to be randomly distributed without a clear trend of association.\n\n# scatter plot of height and total drug concentration\nplot10 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= HT, y=Y)) + \n              geom_point() + \n                labs(x=\"Height\", y=\"Total Drug Concentration\", title = \"Height Related to Total Drug Concentration\")\nplot10\n\n\n\n\nPlot 10 shows that height is not correlated with total drug concentration either. The points appear to be randomly distributed with no clear trend of association.\n\n# scatter plot of weight and total drug concentration\nplot11 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= WT, y=Y)) + \n              geom_point() + \n                labs(x=\"Weight\", y=\"Total Drug Concentration\", title = \"Weight Related to Total Drug Concentration\")\nplot11\n\n\n\n\nPlot 11 contains a lot of variation, but there appears to be weak correlation between these two variables. There seems to be a negative trend between weight and total drug concentration. As weight increases, total drug concentration decreases.\n\n# boxplot of race and total drug concentration\nplot12 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= RACE, y=Y)) + \n              geom_boxplot() + \n                labs(x=\"Race\", y=\"Total Drug Concentration\", title = \"Race Related to Total Drug Concentration\")\nplot12\n\n\n\n\nPlot 12 shows that there is minimal variation in total drug concentration across racial categories because the averages appear to be within 100 units for all 4 categories. The interquartile range for RACE=1 appears to be the largest, and this category has the highest maximum total drug concentration as well. The interquartile range for RACE=7 is the smallest, which means there is the least variation in total drug concentration in this category. There are only 2 outliers according to this plot, one in RACE=1 and another in RACE=88.\n\nplot13 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= SEX, y=Y)) + \n              geom_boxplot() + \n                labs(x=\"Sex\", y=\"Total Drug Concentration\", title = \"Sex Related to Total Drug Concentration\")\nplot13\n\n\n\n\nPlot 13 shows a difference in total drug concentration based on sex. The average total concentration for SEX=1 is higher than in SEX=2. The interquartile range apppears to be the same for both sexes, and there is one outlier in each category.\n\n\n\n\n\n\nThe first model we are interested in fitting is a linear model with the continuous outcome Y and the main predictor of interest DOSE. I will use simple linear regression with the default lm method for ordinary least squares.\n\n# create a linear model with main predictor only\nlm_mod &lt;- linear_reg() #specify the type of model\nmavo_fit1 &lt;- lm_mod %&gt;% \n            fit(Y ~ DOSE, data = mavo5) #estimate/train the model \ntidy(mavo_fit1) #generate clean output with estimates and p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    323.     199.        1.62 1.07e- 1\n2 DOSE            58.2      5.19     11.2  2.69e-20\n\n\nBased on the linear regression model, the total drug concentration increases by 58.21 units with every 1 unit increase in dosage. There is a statistically significant positive association between the two variables based on the tiny p-value of 2.69e-20.\nThe second model we are interested in is also a linear model, but we want to examine the continuous outcome with all six predictors. I will use multiple linear regression with the same default lm method.\n\n# create a linear model with all predictors\nlm_mod &lt;- linear_reg() #specify the type of model\nmavo_fit2 &lt;- lm_mod %&gt;% \n            fit(Y ~ DOSE + AGE + HT + WT + RACE + SEX, data = mavo5) #estimate/train the model \ntidy(mavo_fit2) #generate clean output with estimates and p-values\n\n# A tibble: 9 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  3387.     1835.       1.85  6.76e- 2\n2 DOSE           59.9       4.88    12.3   2.05e-22\n3 AGE             3.16      7.82     0.403 6.88e- 1\n4 HT           -748.     1104.      -0.678 4.99e- 1\n5 WT            -23.0       6.40    -3.60  4.71e- 4\n6 RACE2         155.      129.       1.21  2.31e- 1\n7 RACE7        -405.      448.      -0.904 3.68e- 1\n8 RACE88        -53.5     245.      -0.219 8.27e- 1\n9 SEX2         -358.      217.      -1.65  1.02e- 1\n\n\nThe multiple linear regression yields a slightly larger coefficient for the DOSE variable than the simple linear regression did. As dosage increases by 1 unit, total drug concentration increases by 59.934 units according to this model. This pattern makes sense because a higher dosage of the drug is expected to create a higher total drug concentration in the body. Age is positively associated with total drug concentration because as age increases by 1 year, total drug concentration increases by 3.155 years. This pattern is expected because metabolism slows as age increases, so it makes sense that the body would retain higher levels of drug concentration if it is metabolizing the drug at a slower rate. Height has a negative correlation with total drug concentration based on a coefficient estimate of -748.487. Weight is also negatively correlated, but it has a much smaller coefficient of -23.047. These trends are intuitive given that larger bodies are able to metabolize quicker than smaller bodies on average, so taller and heavier people would have lower total drug concentration. The model chose RACE=1 as the comparison category, so the total drug concentration increases by 155.034 units when RACE=2 compared to RACE=1. The total drug concentration decreases by 405.32 units when RACE=7 compared to RACE=1. There is also a negative association between RACE=88 and total drug concentration compared to RACE=1 based on the coefficient of -53.505. It is harder to interpet these trends since we do not know which race is associated with each category. Lastly, total drug concentration decreases by 357.734 units when SEX=2 compared to when SEX=1. Males and femaless have completely different metabolism rate, so it makes sense that this trend exists. All of these associations are statistically significant because the p-values are all less than 0.001.\n\n\n\nTo determine how well these models perform, we will compute the RMSE and R squared metrics. To find RMSE, I made predictions using each model to compare to the real data stored in mavo5. I selected the prediction column and bound it to the column with true values of Y from the original data to create a new data frame. I used the rmse() function from the yardstick package in tidymodels to find the RMSE values. R squared is calculated from the same prediction and truth values using the rsq() function.\n\n# create predictions using the simple linear regression model\npredictions1 &lt;- predict(mavo_fit1, new_data = mavo5) %&gt;% \n                  select(.pred)\n\n# create a data frame with the predictions and true values of Y\ndata1 &lt;- bind_cols(predictions1, mavo5$Y) %&gt;%\n            rename(Y = \"...2\")\n\n# find RMSE and R squared to determine model fit\nrmse1&lt;- rmse(data1, truth = Y, estimate = .pred)\nrsq1 &lt;- rsq(data1, truth = Y, estimate = .pred)\nprint(rmse1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        666.\n\nprint(rsq1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.516\n\n# create predictions using the multiple linear regression model\npredictions2 &lt;- predict(mavo_fit2, new_data = mavo5) %&gt;% \n                  select(.pred)\n\n# create a data frame with the predictions and true values of Y\ndata2 &lt;- bind_cols(predictions2, mavo5$Y) %&gt;%\n            rename(Y = \"...2\")\n\n# find RMSE and R squared to determine model fit\nrmse2&lt;- rmse(data2, truth = Y, estimate = .pred)\nrsq2&lt;- rsq(data2, truth = Y, estimate = .pred)\nprint(rmse2)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        591.\n\nprint(rsq2)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.619\n\n\nThe RMSE for the simple linear regression model is 666.4618, which is much higher than the RMSE of 590.8535 for the multiple linear regression model. The model with all of the predictors is a better fit for the data. The R squared value for the multiple linear regression model is higher than the simple linear regression model. All the predictors account for 61.93% of variation in the graph, while the DOSE variable alone only accounts for 51.56% of the variation. Overall, the multiple linear regression model is the better choice.\n\n\n\nWe will know consider SEX as the outcome of interest for the sake of practicing modeling with categorical outcomes. First, we will use logistic regression to model SEX using the main predictor, DOSE.\n\n# create a logistic regression model with main predictor only\nlog_mod &lt;- logistic_reg() #specify the type of model\nmavo_fit3 &lt;- log_mod %&gt;% \n                fit(SEX ~ DOSE, data = mavo5) #estimate/train the model \ntidy(mavo_fit3) #generate clean output with estimates and p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.765     0.854     -0.896   0.370\n2 DOSE         -0.0318    0.0243    -1.31    0.192\n\n#exponentiate coefficient estimate \nexp(-0.03175443)\n\n[1] 0.9687444\n\n\nThe logistic regression model shows a negative relationship between SEX and DOSE. The coefficient estimate is on the log odds scale, so it must be exponentiated before interpretation. The odds of SEX being SEX=2 increase multiplicatively by a factor of 0.9687 for every increase in dosage. This interpretation does not make a ton of sense logically, but it is important to remember that coefficient estimates are on the log odds scale.\nNext, we will fit a model for SEX with all of the predictors.\n\n# create a logistic regression model with all predictors\nlog_mod &lt;- logistic_reg() #specify the type of model\nmavo_fit4 &lt;- log_mod %&gt;% \n                fit(SEX ~ DOSE + Y + AGE + HT + WT + RACE, data = mavo5) #estimate/train the model \ntidy(mavo_fit4) #generate clean output with estimates and p-values\n\n# A tibble: 9 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  60.3     18.0         3.34   0.000824\n2 DOSE         -0.0308   0.0776     -0.396  0.692   \n3 Y            -0.00104  0.000963   -1.08   0.280   \n4 AGE           0.0834   0.0607      1.37   0.170   \n5 HT          -33.2     11.1        -3.00   0.00274 \n6 WT           -0.0628   0.0794     -0.791  0.429   \n7 RACE2        -1.93     1.37       -1.40   0.161   \n8 RACE7         0.118    3.85        0.0306 0.976   \n9 RACE88       -1.50     2.19       -0.683  0.494   \n\n# exponentiate coefficient estimates\nexp(-0.03075591)\n\n[1] 0.9697122\n\nexp(-0.00104044)\n\n[1] 0.9989601\n\nexp(-33.19601486)\n\n[1] 3.829605e-15\n\n\nThis model shows negative associations between most of the variables and the outcome of SEX. The dose coefficient estimate changed only slightly with the addition of the other predictors, and now dosage is associated with a 0.9697 multiplicative effect on the odds of SEX=2. Total drug concentration Y appears to have the smallest effect on the outcome, which is to be expected because the total concentration of the drug in your body will have no effect on your sex. Regardless, for every 1 unit increase in total drug concentration, the odds of SEX=2 increase multiplicatively by a factor of 0.9989, which is essentially null. Height has the strongest relationship with the outcome variable, and that trend is expected because males are taller on average than females, which makes height the best predictor of the binary SEX outcome. As height increases by 1 unit, the odds of SEX=2 increase multiplicatively by a factor of 3.829e-15. Weight has a significantly smaller effect on SEX that is not statistically significant, but the same logic as the height variable applies here. It makes sense that this trend would be weaker because weight fluctuates muchh more than height, regardless of sex. While it is difficult to interpret the change in odds in this situation, the overall trends make sense intuitively.\n\n\n\nFor both models, we will assess the fit using accuracy and ROC-AUC metrics. The setup is similar to calculating RMSE and R squared for linear models. I found the predictions for the SEX variable first. I created a new dataset with the predictions and the true value from the original dataset. The accuracy() function uses the dataset, truth, and estimate arguments.\n\n# make predictions using the logistic model with main predictor only \npredictions3 &lt;- predict(mavo_fit3, new_data = mavo5) %&gt;% \n                  select(.pred_class)\n\n# create a data frame with the predicted and true values \ndata3 &lt;- bind_cols(predictions3, mavo5$SEX) %&gt;% \n            rename(SEX = \"...2\")\n\n# find accuracy of the first logistic model\nacc1 &lt;- accuracy(data3, truth = SEX, estimate = .pred_class)\nacc1\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.867\n\n# make predictions using the logistic model with all predictors\npredictions4 &lt;- predict(mavo_fit4, new_data = mavo5) %&gt;%\n                    select(.pred_class)\n\n# create a data frame with the predicted and true values\ndata4 &lt;- bind_cols(predictions4, mavo5$SEX) %&gt;% \n            rename(SEX = \"...2\")\n\n# find accuracy of the second logistic model\nacc2 &lt;- accuracy(data4, truth = SEX, estimate = .pred_class)\nacc2\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.942\n\n\nThe accuracy for the logistic model with the main predictor only is 0.867, and the accuracy for the model with all predictors is 0.942. Once again,the model with all predictors is the better choice.\nIdeally, we would use the roc_auc() function from the tidymodels package to find the area under the curve. I received persistent errors when I tried this approach about not being able to rename variables in this context. I removed the variable names from my prediction data set and tried to call the columns in multiple formats, but I could not get this function to run. Instead, I used the pROC package to find the ROC curve first and the area under the curve second.\n\n# create data set with truth and prediction variables for model with main predictor only \ndata4 &lt;- data.frame(truth = mavo5$SEX,\n                    estimate = as.numeric(predictions3$.pred_class))\n\n# find the ROC curve\nroc_curve1 &lt;- roc(data4$truth, data4$estimate)\n\n# calculate area under the curve score\nroc_auc1 &lt;- auc(roc_curve1)\nroc_auc1\n\nArea under the curve: 0.5\n\n# create data set with truth and prediction variables for model with main predictor only \ndata5 &lt;- data.frame(truth = mavo5$SEX,\n                    estimate = as.numeric(predictions4$.pred_class))\n\n# find the ROC curve\nroc_curve2 &lt;- roc(data5$truth, data5$estimate)\n\n# calculate area under the curve score\nroc_auc2 &lt;- auc(roc_curve2)\nroc_auc2\n\nArea under the curve: 0.8606\n\n\nThe area under the curve for the logistic regression model with the main predictor only is 0.5 compared to an AOC of 0.861 for the second model containing all the predictors. The second model is the better choice based on this metric.\nAfter receiving feedback from Dr. Handel, he provided us with an example of how to find the area under the curve using the function that I attempted the first time.\n\n# fit the first logistic model \nlogfit1 &lt;- log_mod %&gt;% fit(SEX ~ DOSE, data = mavo5)\n\n# find area under the curve\nm1_auc &lt;-  logfit1 %&gt;%\n  predict(mavo5, type = \"prob\") %&gt;%\n  bind_cols(mavo5) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\nprint(m1_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.592\n\n# fit the second logistic model\nlogfit2 &lt;- log_mod %&gt;% fit(SEX ~ ., data = mavo5)\n\n# find the area under the curve\nm2_auc &lt;-  logfit2 %&gt;%\n  predict(mavo5, type = \"prob\") %&gt;%\n  bind_cols(mavo5) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\nprint(m2_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.980"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#exploratory-data-analysis",
    "href": "fitting-exercise/fitting-exercise.html#exploratory-data-analysis",
    "title": "Fitting Exercise",
    "section": "",
    "text": "To get a better understanding of the data, I made a simple summary table to examine the distribution for each of the variables.\n\n# create simple summary table\nsummary &lt;- mavo5 %&gt;% \n            tbl_summary()\nsummary\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 1201\n    \n  \n  \n    Y\n2,349 (1,701, 3,050)\n    DOSE\n\n        25\n59 (49%)\n        37.5\n12 (10%)\n        50\n49 (41%)\n    AGE\n31 (26, 40)\n    SEX\n\n        1\n104 (87%)\n        2\n16 (13%)\n    RACE\n\n        1\n74 (62%)\n        2\n36 (30%)\n        7\n2 (1.7%)\n        88\n8 (6.7%)\n    WT\n82 (73, 90)\n    HT\n1.77 (1.70, 1.81)\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\nThe average total dose concentration across the 120 observations is 2,349. Dosage is split into three discrete options: 25, 37.5, and 50. Almost 50% of the observations took the 25 unit dose and 41% took the 50 unit dose. The average age is 31 years, and the average weight is 82 units. The average height is 1.77, which must be stored in meters. Sex is very unevenly distributed with one gender accounting for 87% of observations. Race is also unevenly distributed with 62% of observations accounted for by one race.\nI created another table that summarized the numeric variables by SEX because I noticed that the distribution was so uneven. I wanted to examine the differences in the numeric variables by sex.\n\n# create summary table of numeric variables by SEX\nnumtable&lt;-  mavo5 %&gt;% \n              group_by(SEX) %&gt;%\n                summarize(across(c(Y:AGE, WT, HT), mean))\nkable(numtable)\n\n\n\n\nSEX\nY\nDOSE\nAGE\nWT\nHT\n\n\n\n\n1\n2477.643\n37.01923\n31.79808\n84.09808\n1.779333\n\n\n2\n2235.874\n32.81250\n40.81250\n72.51875\n1.626908\n\n\n\n\n\nFor whichever sex is denoted by 1, the average total drug concentration is higher at 2477.643 units compared to 2235.874 for sex 2. DOSE, HT, and WT also have higher averages among the SEX=1 group, so I hypothesize that 1 represents males and 2 represents females. It is interesting that the average age among the SEX=2 group is 9 years older compared to the SEX=1 group.\n\n\n\nI will create histograms to explore each of the numeric variables to understand their distributions better.\n\n# histogram of total drug concentration `Y`\nplot1 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= Y)) + \n              geom_histogram() + \n                labs(x=\"Total Drug Concentration\", y=\"Frequency\", title = \"Distribution of Total Drug Concentration\")\nplot1\n\n\n\n\nPlot 1 reveals that total drug concentration is almost normally distributed with a few trailing outliers that are greater than 4000 units. The lowest total drug concentration is around 750 units, and the highest outliter is about 5600 units. The majoirty of observations fall between 1000 and 4000 units.\n\n# histogram of DOSE\nplot2 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= DOSE)) + \n              geom_histogram() + \n                labs(x=\"Mavoglurant Dosage\", y=\"Frequency\", title = \"Distribution of Dosage\")\nplot2\n\n\n\n\nPlot 2 reveals that there are three distinct options for Mavoglurant dosage around 25, 37.5, and 50 units. Almost half of the observations take the 25 unit dosage, and the 37.5 unit dosage is the most uncommmon.\n\n# histogram of AGE \nplot3 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= AGE)) + \n              geom_histogram() + \n                labs(x=\"Age\", y=\"Frequency\", title = \"Distribution of Age\")\nplot3\n\n\n\n\nPlot 3 reveals that age is not normally distributed. The two ages accounting for the most observations are around 26 and 27 years. There are also peaks in the histogram at 37 years and 30 years. This graph shows random variation in the age of the 120 observations.\n\n# histogram of height `HT`\nplot4 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= HT)) + \n              geom_histogram() + \n                labs(x=\"Height\", y=\"Frequency\", title = \"Distribution of Height\")\nplot4\n\n\n\n\nPlot 4 reveals height to be approximately normally distributed with a peak around 1.8 units. There are a few low outliers between 1.5 and 1.6 units, but the majority of observations have a heigt between 1.62 and 1.91 unit.\n\n# histogram of weight `WT`\nplot5 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= WT)) + \n              geom_histogram() + \n                labs(x=\"Weight\", y=\"Frequency\", title = \"Distribution of Weight\")\nplot5\n\n\n\n\nPlot 5 reveals that weight is approximately normally distributed with a peak around 82 units. The lowest weight is about 55 units, and the highest weight is about 116 units. Most of the observations fall between 68 and 100 units.\nIt is also important to explore the distributions of the two categorical variables using bar graphs: RACE and SEX.\n\n# bar graph of race variable \nplot6 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= RACE)) + \n              geom_bar() + \n                labs(x=\"Race\", y=\"Frequency\", title = \"Distribution of Race\")\nplot6\n\n\n\n\nPlot 6 shows that race is denoted as four discrete numeric options: 1, 2, 7, and 88. Over 70 observations are accounted for in the RACE=1 category which is almost twice as large as the RACE=2 category. RACE=7 has the fewest observations, ann RACE=88 has about 8 observations. It is almost impossible to determine which race is represented by each category, but RACE=88 probably accounts for missing information.\n\n# bar graph of sex variable\nplot7 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= SEX)) + \n              geom_bar() + \n                labs(x=\"Sex\", y=\"Frequency\", title = \"Distribution of Sex\")\nplot7\n\n\n\n\nPlot 7 confirms what we already know about sex being unevenly distributed. SEX=1 accounts for over 100 observations, which is almost 5 times as many observations included in the SEX=2 category.\nI will move on to creating scatterplots and boxplots to examine the relationship between the main outcome of interest Y and predictor variables including DOSE, AGE, HT, WT, RACE, and SEX.\n\n# scatter plot of dosage and total drug concentration \nplot8 &lt;- mavo5 %&gt;% \n            mutate(DOSE = as.factor(DOSE)) %&gt;% \n              ggplot(aes(x= DOSE, y=Y)) + \n                geom_boxplot() + \n                  labs(x=\"Dose\", y=\"Total Drug Concentration\", title = \"Dosage Related to Total Drug Concentration\")\nplot8\n\n\n\n\nI converted the DOSAGE variable to a factor variable here because I felt like a boxplot would display the information better than a scatterplot. The boxplot can be used to determine that the 50 unit dose accounts for the highest average total drug concentration. Total drug concentration decreases as dosage decreases to 37.5 and 25 units, which makes sense intuitively. There are a few outliers in each dosage category, but they do not seem to affect the overall trend.\n\n# scatter plot of age and total drug concentration\nplot9 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= AGE, y=Y)) + \n              geom_point() + \n                labs(x=\"Age\", y=\"Total Drug Concentration\", title = \"Age Related to Total Drug Concentration\")\nplot9\n\n\n\n\nPlot 9 shows that there is no correlation between age and total drug concentration. The points appear to be randomly distributed without a clear trend of association.\n\n# scatter plot of height and total drug concentration\nplot10 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= HT, y=Y)) + \n              geom_point() + \n                labs(x=\"Height\", y=\"Total Drug Concentration\", title = \"Height Related to Total Drug Concentration\")\nplot10\n\n\n\n\nPlot 10 shows that height is not correlated with total drug concentration either. The points appear to be randomly distributed with no clear trend of association.\n\n# scatter plot of weight and total drug concentration\nplot11 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= WT, y=Y)) + \n              geom_point() + \n                labs(x=\"Weight\", y=\"Total Drug Concentration\", title = \"Weight Related to Total Drug Concentration\")\nplot11\n\n\n\n\nPlot 11 contains a lot of variation, but there appears to be weak correlation between these two variables. There seems to be a negative trend between weight and total drug concentration. As weight increases, total drug concentration decreases.\n\n# boxplot of race and total drug concentration\nplot12 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= RACE, y=Y)) + \n              geom_boxplot() + \n                labs(x=\"Race\", y=\"Total Drug Concentration\", title = \"Race Related to Total Drug Concentration\")\nplot12\n\n\n\n\nPlot 12 shows that there is minimal variation in total drug concentration across racial categories because the averages appear to be within 100 units for all 4 categories. The interquartile range for RACE=1 appears to be the largest, and this category has the highest maximum total drug concentration as well. The interquartile range for RACE=7 is the smallest, which means there is the least variation in total drug concentration in this category. There are only 2 outliers according to this plot, one in RACE=1 and another in RACE=88.\n\nplot13 &lt;- mavo5 %&gt;% \n            ggplot(aes(x= SEX, y=Y)) + \n              geom_boxplot() + \n                labs(x=\"Sex\", y=\"Total Drug Concentration\", title = \"Sex Related to Total Drug Concentration\")\nplot13\n\n\n\n\nPlot 13 shows a difference in total drug concentration based on sex. The average total concentration for SEX=1 is higher than in SEX=2. The interquartile range apppears to be the same for both sexes, and there is one outlier in each category."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-fitting",
    "href": "fitting-exercise/fitting-exercise.html#model-fitting",
    "title": "Fitting Exercise",
    "section": "",
    "text": "The first model we are interested in fitting is a linear model with the continuous outcome Y and the main predictor of interest DOSE. I will use simple linear regression with the default lm method for ordinary least squares.\n\n# create a linear model with main predictor only\nlm_mod &lt;- linear_reg() #specify the type of model\nmavo_fit1 &lt;- lm_mod %&gt;% \n            fit(Y ~ DOSE, data = mavo5) #estimate/train the model \ntidy(mavo_fit1) #generate clean output with estimates and p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    323.     199.        1.62 1.07e- 1\n2 DOSE            58.2      5.19     11.2  2.69e-20\n\n\nBased on the linear regression model, the total drug concentration increases by 58.21 units with every 1 unit increase in dosage. There is a statistically significant positive association between the two variables based on the tiny p-value of 2.69e-20.\nThe second model we are interested in is also a linear model, but we want to examine the continuous outcome with all six predictors. I will use multiple linear regression with the same default lm method.\n\n# create a linear model with all predictors\nlm_mod &lt;- linear_reg() #specify the type of model\nmavo_fit2 &lt;- lm_mod %&gt;% \n            fit(Y ~ DOSE + AGE + HT + WT + RACE + SEX, data = mavo5) #estimate/train the model \ntidy(mavo_fit2) #generate clean output with estimates and p-values\n\n# A tibble: 9 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  3387.     1835.       1.85  6.76e- 2\n2 DOSE           59.9       4.88    12.3   2.05e-22\n3 AGE             3.16      7.82     0.403 6.88e- 1\n4 HT           -748.     1104.      -0.678 4.99e- 1\n5 WT            -23.0       6.40    -3.60  4.71e- 4\n6 RACE2         155.      129.       1.21  2.31e- 1\n7 RACE7        -405.      448.      -0.904 3.68e- 1\n8 RACE88        -53.5     245.      -0.219 8.27e- 1\n9 SEX2         -358.      217.      -1.65  1.02e- 1\n\n\nThe multiple linear regression yields a slightly larger coefficient for the DOSE variable than the simple linear regression did. As dosage increases by 1 unit, total drug concentration increases by 59.934 units according to this model. This pattern makes sense because a higher dosage of the drug is expected to create a higher total drug concentration in the body. Age is positively associated with total drug concentration because as age increases by 1 year, total drug concentration increases by 3.155 years. This pattern is expected because metabolism slows as age increases, so it makes sense that the body would retain higher levels of drug concentration if it is metabolizing the drug at a slower rate. Height has a negative correlation with total drug concentration based on a coefficient estimate of -748.487. Weight is also negatively correlated, but it has a much smaller coefficient of -23.047. These trends are intuitive given that larger bodies are able to metabolize quicker than smaller bodies on average, so taller and heavier people would have lower total drug concentration. The model chose RACE=1 as the comparison category, so the total drug concentration increases by 155.034 units when RACE=2 compared to RACE=1. The total drug concentration decreases by 405.32 units when RACE=7 compared to RACE=1. There is also a negative association between RACE=88 and total drug concentration compared to RACE=1 based on the coefficient of -53.505. It is harder to interpet these trends since we do not know which race is associated with each category. Lastly, total drug concentration decreases by 357.734 units when SEX=2 compared to when SEX=1. Males and femaless have completely different metabolism rate, so it makes sense that this trend exists. All of these associations are statistically significant because the p-values are all less than 0.001.\n\n\n\nTo determine how well these models perform, we will compute the RMSE and R squared metrics. To find RMSE, I made predictions using each model to compare to the real data stored in mavo5. I selected the prediction column and bound it to the column with true values of Y from the original data to create a new data frame. I used the rmse() function from the yardstick package in tidymodels to find the RMSE values. R squared is calculated from the same prediction and truth values using the rsq() function.\n\n# create predictions using the simple linear regression model\npredictions1 &lt;- predict(mavo_fit1, new_data = mavo5) %&gt;% \n                  select(.pred)\n\n# create a data frame with the predictions and true values of Y\ndata1 &lt;- bind_cols(predictions1, mavo5$Y) %&gt;%\n            rename(Y = \"...2\")\n\n# find RMSE and R squared to determine model fit\nrmse1&lt;- rmse(data1, truth = Y, estimate = .pred)\nrsq1 &lt;- rsq(data1, truth = Y, estimate = .pred)\nprint(rmse1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        666.\n\nprint(rsq1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.516\n\n# create predictions using the multiple linear regression model\npredictions2 &lt;- predict(mavo_fit2, new_data = mavo5) %&gt;% \n                  select(.pred)\n\n# create a data frame with the predictions and true values of Y\ndata2 &lt;- bind_cols(predictions2, mavo5$Y) %&gt;%\n            rename(Y = \"...2\")\n\n# find RMSE and R squared to determine model fit\nrmse2&lt;- rmse(data2, truth = Y, estimate = .pred)\nrsq2&lt;- rsq(data2, truth = Y, estimate = .pred)\nprint(rmse2)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        591.\n\nprint(rsq2)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.619\n\n\nThe RMSE for the simple linear regression model is 666.4618, which is much higher than the RMSE of 590.8535 for the multiple linear regression model. The model with all of the predictors is a better fit for the data. The R squared value for the multiple linear regression model is higher than the simple linear regression model. All the predictors account for 61.93% of variation in the graph, while the DOSE variable alone only accounts for 51.56% of the variation. Overall, the multiple linear regression model is the better choice.\n\n\n\nWe will know consider SEX as the outcome of interest for the sake of practicing modeling with categorical outcomes. First, we will use logistic regression to model SEX using the main predictor, DOSE.\n\n# create a logistic regression model with main predictor only\nlog_mod &lt;- logistic_reg() #specify the type of model\nmavo_fit3 &lt;- log_mod %&gt;% \n                fit(SEX ~ DOSE, data = mavo5) #estimate/train the model \ntidy(mavo_fit3) #generate clean output with estimates and p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.765     0.854     -0.896   0.370\n2 DOSE         -0.0318    0.0243    -1.31    0.192\n\n#exponentiate coefficient estimate \nexp(-0.03175443)\n\n[1] 0.9687444\n\n\nThe logistic regression model shows a negative relationship between SEX and DOSE. The coefficient estimate is on the log odds scale, so it must be exponentiated before interpretation. The odds of SEX being SEX=2 increase multiplicatively by a factor of 0.9687 for every increase in dosage. This interpretation does not make a ton of sense logically, but it is important to remember that coefficient estimates are on the log odds scale.\nNext, we will fit a model for SEX with all of the predictors.\n\n# create a logistic regression model with all predictors\nlog_mod &lt;- logistic_reg() #specify the type of model\nmavo_fit4 &lt;- log_mod %&gt;% \n                fit(SEX ~ DOSE + Y + AGE + HT + WT + RACE, data = mavo5) #estimate/train the model \ntidy(mavo_fit4) #generate clean output with estimates and p-values\n\n# A tibble: 9 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  60.3     18.0         3.34   0.000824\n2 DOSE         -0.0308   0.0776     -0.396  0.692   \n3 Y            -0.00104  0.000963   -1.08   0.280   \n4 AGE           0.0834   0.0607      1.37   0.170   \n5 HT          -33.2     11.1        -3.00   0.00274 \n6 WT           -0.0628   0.0794     -0.791  0.429   \n7 RACE2        -1.93     1.37       -1.40   0.161   \n8 RACE7         0.118    3.85        0.0306 0.976   \n9 RACE88       -1.50     2.19       -0.683  0.494   \n\n# exponentiate coefficient estimates\nexp(-0.03075591)\n\n[1] 0.9697122\n\nexp(-0.00104044)\n\n[1] 0.9989601\n\nexp(-33.19601486)\n\n[1] 3.829605e-15\n\n\nThis model shows negative associations between most of the variables and the outcome of SEX. The dose coefficient estimate changed only slightly with the addition of the other predictors, and now dosage is associated with a 0.9697 multiplicative effect on the odds of SEX=2. Total drug concentration Y appears to have the smallest effect on the outcome, which is to be expected because the total concentration of the drug in your body will have no effect on your sex. Regardless, for every 1 unit increase in total drug concentration, the odds of SEX=2 increase multiplicatively by a factor of 0.9989, which is essentially null. Height has the strongest relationship with the outcome variable, and that trend is expected because males are taller on average than females, which makes height the best predictor of the binary SEX outcome. As height increases by 1 unit, the odds of SEX=2 increase multiplicatively by a factor of 3.829e-15. Weight has a significantly smaller effect on SEX that is not statistically significant, but the same logic as the height variable applies here. It makes sense that this trend would be weaker because weight fluctuates muchh more than height, regardless of sex. While it is difficult to interpret the change in odds in this situation, the overall trends make sense intuitively.\n\n\n\nFor both models, we will assess the fit using accuracy and ROC-AUC metrics. The setup is similar to calculating RMSE and R squared for linear models. I found the predictions for the SEX variable first. I created a new dataset with the predictions and the true value from the original dataset. The accuracy() function uses the dataset, truth, and estimate arguments.\n\n# make predictions using the logistic model with main predictor only \npredictions3 &lt;- predict(mavo_fit3, new_data = mavo5) %&gt;% \n                  select(.pred_class)\n\n# create a data frame with the predicted and true values \ndata3 &lt;- bind_cols(predictions3, mavo5$SEX) %&gt;% \n            rename(SEX = \"...2\")\n\n# find accuracy of the first logistic model\nacc1 &lt;- accuracy(data3, truth = SEX, estimate = .pred_class)\nacc1\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.867\n\n# make predictions using the logistic model with all predictors\npredictions4 &lt;- predict(mavo_fit4, new_data = mavo5) %&gt;%\n                    select(.pred_class)\n\n# create a data frame with the predicted and true values\ndata4 &lt;- bind_cols(predictions4, mavo5$SEX) %&gt;% \n            rename(SEX = \"...2\")\n\n# find accuracy of the second logistic model\nacc2 &lt;- accuracy(data4, truth = SEX, estimate = .pred_class)\nacc2\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.942\n\n\nThe accuracy for the logistic model with the main predictor only is 0.867, and the accuracy for the model with all predictors is 0.942. Once again,the model with all predictors is the better choice.\nIdeally, we would use the roc_auc() function from the tidymodels package to find the area under the curve. I received persistent errors when I tried this approach about not being able to rename variables in this context. I removed the variable names from my prediction data set and tried to call the columns in multiple formats, but I could not get this function to run. Instead, I used the pROC package to find the ROC curve first and the area under the curve second.\n\n# create data set with truth and prediction variables for model with main predictor only \ndata4 &lt;- data.frame(truth = mavo5$SEX,\n                    estimate = as.numeric(predictions3$.pred_class))\n\n# find the ROC curve\nroc_curve1 &lt;- roc(data4$truth, data4$estimate)\n\n# calculate area under the curve score\nroc_auc1 &lt;- auc(roc_curve1)\nroc_auc1\n\nArea under the curve: 0.5\n\n# create data set with truth and prediction variables for model with main predictor only \ndata5 &lt;- data.frame(truth = mavo5$SEX,\n                    estimate = as.numeric(predictions4$.pred_class))\n\n# find the ROC curve\nroc_curve2 &lt;- roc(data5$truth, data5$estimate)\n\n# calculate area under the curve score\nroc_auc2 &lt;- auc(roc_curve2)\nroc_auc2\n\nArea under the curve: 0.8606\n\n\nThe area under the curve for the logistic regression model with the main predictor only is 0.5 compared to an AOC of 0.861 for the second model containing all the predictors. The second model is the better choice based on this metric.\nAfter receiving feedback from Dr. Handel, he provided us with an example of how to find the area under the curve using the function that I attempted the first time.\n\n# fit the first logistic model \nlogfit1 &lt;- log_mod %&gt;% fit(SEX ~ DOSE, data = mavo5)\n\n# find area under the curve\nm1_auc &lt;-  logfit1 %&gt;%\n  predict(mavo5, type = \"prob\") %&gt;%\n  bind_cols(mavo5) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\nprint(m1_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.592\n\n# fit the second logistic model\nlogfit2 &lt;- log_mod %&gt;% fit(SEX ~ ., data = mavo5)\n\n# find the area under the curve\nm2_auc &lt;-  logfit2 %&gt;%\n  predict(mavo5, type = \"prob\") %&gt;%\n  bind_cols(mavo5) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\nprint(m2_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.980"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "I found this data set from the FiveThirtyEight website about American opinions on the Coronavirus in relation to political events from March 2020 to April 2021: https://data.fivethirtyeight.com/ I chose to use the covid_concern_polls.csv file to recreate the graph about how worried Americans are about Covid infection, which can be found here. I added the original graph in this section for convenience.  I examined the data to discover there are start and end dates for each response and four levels of worry for each topic: very, somewhat, not very, and not at all, which are all represented on the graph I want to replicate. There are 678 observations of 15 variables stored in this dataset.\n\nlibrary(readr)\nlibrary(here)\npollsdata &lt;- read_csv(here(\"presentation-exercise\", \"covid_concern_polls.csv\"))\nstr(pollsdata)\n\nspc_tbl_ [678 × 15] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ start_date : Date[1:678], format: \"2020-01-27\" \"2020-01-31\" ...\n $ end_date   : Date[1:678], format: \"2020-01-29\" \"2020-02-02\" ...\n $ pollster   : chr [1:678] \"Morning Consult\" \"Morning Consult\" \"YouGov\" \"Morning Consult\" ...\n $ sponsor    : chr [1:678] NA NA \"Economist\" NA ...\n $ sample_size: num [1:678] 2202 2202 1500 2200 1000 ...\n $ population : chr [1:678] \"a\" \"a\" \"a\" \"a\" ...\n $ party      : chr [1:678] \"all\" \"all\" \"all\" \"all\" ...\n $ subject    : chr [1:678] \"concern-economy\" \"concern-economy\" \"concern-infected\" \"concern-economy\" ...\n $ tracking   : logi [1:678] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ text       : chr [1:678] \"How concerned are you that the coronavirus will impact the following? U.S. economy\" \"How concerned are you that the coronavirus will impact the following? U.S. economy\" \"Taking into consideration both your risk of contracting it and the seriousness of the illness, how worried are \"| __truncated__ \"How concerned are you that the coronavirus will impact the following? U.S. economy\" ...\n $ very       : num [1:678] 19 26 13 23 11 11 22 22 22 10 ...\n $ somewhat   : num [1:678] 33 32 26 32 24 28 23 35 21 28 ...\n $ not_very   : num [1:678] 23 25 43 24 33 39 37 28 33 42 ...\n $ not_at_all : num [1:678] 11 7 18 9 20 22 19 15 23 19 ...\n $ url        : chr [1:678] \"https://morningconsult.com/wp-content/uploads/2020/02/200167_crosstabs_CORONAVIRUS_Adults_v2_JB-1.pdf\" \"https://morningconsult.com/wp-content/uploads/2020/02/200191_crosstabs_CORONAVIRUS_Adults_v2_JB-1.pdf\" \"https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/73jqd6u5mv/econTabReport.pdf\" \"https://morningconsult.com/wp-content/uploads/2020/02/200214_crosstabs_CORONAVIRUS_Adults_v4_JB.pdf\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   start_date = col_date(format = \"\"),\n  ..   end_date = col_date(format = \"\"),\n  ..   pollster = col_character(),\n  ..   sponsor = col_character(),\n  ..   sample_size = col_double(),\n  ..   population = col_character(),\n  ..   party = col_character(),\n  ..   subject = col_character(),\n  ..   tracking = col_logical(),\n  ..   text = col_character(),\n  ..   very = col_double(),\n  ..   somewhat = col_double(),\n  ..   not_very = col_double(),\n  ..   not_at_all = col_double(),\n  ..   url = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ndim(pollsdata)\n\n[1] 678  15"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#get-the-original-data",
    "href": "presentation-exercise/presentation-exercise.html#get-the-original-data",
    "title": "Presentation Exercise",
    "section": "",
    "text": "I found this data set from the FiveThirtyEight website about American opinions on the Coronavirus in relation to political events from March 2020 to April 2021: https://data.fivethirtyeight.com/ I chose to use the covid_concern_polls.csv file to recreate the graph about how worried Americans are about Covid infection, which can be found here. I added the original graph in this section for convenience.  I examined the data to discover there are start and end dates for each response and four levels of worry for each topic: very, somewhat, not very, and not at all, which are all represented on the graph I want to replicate. There are 678 observations of 15 variables stored in this dataset.\n\nlibrary(readr)\nlibrary(here)\npollsdata &lt;- read_csv(here(\"presentation-exercise\", \"covid_concern_polls.csv\"))\nstr(pollsdata)\n\nspc_tbl_ [678 × 15] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ start_date : Date[1:678], format: \"2020-01-27\" \"2020-01-31\" ...\n $ end_date   : Date[1:678], format: \"2020-01-29\" \"2020-02-02\" ...\n $ pollster   : chr [1:678] \"Morning Consult\" \"Morning Consult\" \"YouGov\" \"Morning Consult\" ...\n $ sponsor    : chr [1:678] NA NA \"Economist\" NA ...\n $ sample_size: num [1:678] 2202 2202 1500 2200 1000 ...\n $ population : chr [1:678] \"a\" \"a\" \"a\" \"a\" ...\n $ party      : chr [1:678] \"all\" \"all\" \"all\" \"all\" ...\n $ subject    : chr [1:678] \"concern-economy\" \"concern-economy\" \"concern-infected\" \"concern-economy\" ...\n $ tracking   : logi [1:678] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ text       : chr [1:678] \"How concerned are you that the coronavirus will impact the following? U.S. economy\" \"How concerned are you that the coronavirus will impact the following? U.S. economy\" \"Taking into consideration both your risk of contracting it and the seriousness of the illness, how worried are \"| __truncated__ \"How concerned are you that the coronavirus will impact the following? U.S. economy\" ...\n $ very       : num [1:678] 19 26 13 23 11 11 22 22 22 10 ...\n $ somewhat   : num [1:678] 33 32 26 32 24 28 23 35 21 28 ...\n $ not_very   : num [1:678] 23 25 43 24 33 39 37 28 33 42 ...\n $ not_at_all : num [1:678] 11 7 18 9 20 22 19 15 23 19 ...\n $ url        : chr [1:678] \"https://morningconsult.com/wp-content/uploads/2020/02/200167_crosstabs_CORONAVIRUS_Adults_v2_JB-1.pdf\" \"https://morningconsult.com/wp-content/uploads/2020/02/200191_crosstabs_CORONAVIRUS_Adults_v2_JB-1.pdf\" \"https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/73jqd6u5mv/econTabReport.pdf\" \"https://morningconsult.com/wp-content/uploads/2020/02/200214_crosstabs_CORONAVIRUS_Adults_v4_JB.pdf\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   start_date = col_date(format = \"\"),\n  ..   end_date = col_date(format = \"\"),\n  ..   pollster = col_character(),\n  ..   sponsor = col_character(),\n  ..   sample_size = col_double(),\n  ..   population = col_character(),\n  ..   party = col_character(),\n  ..   subject = col_character(),\n  ..   tracking = col_logical(),\n  ..   text = col_character(),\n  ..   very = col_double(),\n  ..   somewhat = col_double(),\n  ..   not_very = col_double(),\n  ..   not_at_all = col_double(),\n  ..   url = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ndim(pollsdata)\n\n[1] 678  15"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#ask-ai-to-recreate-the-original-graph",
    "href": "presentation-exercise/presentation-exercise.html#ask-ai-to-recreate-the-original-graph",
    "title": "Presentation Exercise",
    "section": "Ask AI to recreate the original graph",
    "text": "Ask AI to recreate the original graph\nThis is the original prompt that I asked ChatGPT 3.5: R code to generate the graph titled “How worried are Americans about infection?” on this website https://projects.fivethirtyeight.com/coronavirus-polls/ using the covid_concern_polls.csv file from this github repository https://github.com/fivethirtyeight/covid-19-polls\nI used the links to the website and GitHub repository to ensure that it knew which graph I am trying to recreate, which has been saved as Inspiration_graph.png in the presentation-exercise folder. The first step it gave me was to load three packages: readr, dplyr, and ggplot2. The second step was to save the data as an object, which I have already done. Next, it suggested filtering the data for the question regarding infection: “How worried are you about personally being infected with coronavirus?”. I made several modifications to the code because it did not know the object name that I stored my data in, and it did not know the exact verbiage of the question that was asked. This was not successful because I do not have one “estimate” for the percentage of people in each of the worry categories ranging from not_at_all to very. I should have known that this code would not be accurate because it said these steps will produce a polar bar graph when I am trying to produce a line graph. I commented out this code for the sake of rendering my website with this exercise.\n\n# load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# modified filtering code\n# concern_data &lt;- pollsdata %&gt;%\n # filter(text == \"How worried are you about you or someone in your family being infected with the Coronavirus??\") %&gt;%\n # select(starts_with(\"estimate\"))\n\nThis is the second prompt I asked ChatGPT: R code to produce a line graph with percentages of four different categories “very”, “somewhat”, “not very”, and “not at all” on the y-axis and time on the x-axis using the covid_concern_polls.csv file from this github repository https://github.com/fivethirtyeight/covid-19-polls\nI asked for a line graph specifically, and I know that I need to create variables with the percentages of each response category. I filtered for the subject of interest: “concern-infected” to limit the data to information about concern over Coronavrius infection. The graph models how concerned Americans are that they, someone in their family or someone else they know will be infected with Coronavirus, so I filtered for responses to all of the questions asking about concern over Coronavirus infections, which yielded 77 observations.\n\n# filter for all responses about Coronavirus concern\nconcern_data &lt;- pollsdata %&gt;% \n    filter(subject == \"concern-infected\")\n\nThe next suggestion from ChatGPT was to create percentages for each of the response variables. The first variable that had to be changed here is the grouping factor because modeldate was not included in the original dataset. Additionally, there were no “estimate” variables, so I just used the column names instead because each column had count data for that observation.\n\n# group data by date and calculate percentages for each category\nconcern_data2 &lt;- concern_data %&gt;%\n  group_by(end_date) %&gt;%\n  summarise(\n    very_percent = sum(very) / n(),\n    somewhat_percent = sum(somewhat) / n(),\n    not_very_percent = sum(not_very) / n(),\n    not_at_all_percent = sum(not_at_all) / n()\n  )\n\n# check that it worked \nstr(concern_data2)\n\ntibble [226 × 5] (S3: tbl_df/tbl/data.frame)\n $ end_date          : Date[1:226], format: \"2020-02-04\" \"2020-02-09\" ...\n $ very_percent      : num [1:226] 13 11 11 22 16 10 16 19 23 30 ...\n $ somewhat_percent  : num [1:226] 26 24 28 23 24.5 30 31 33 32.5 32 ...\n $ not_very_percent  : num [1:226] 43 33 39 37 37.5 40 27 32.5 27 24 ...\n $ not_at_all_percent: num [1:226] 18 20 22 19 21 20 16 15 17 15 ...\n\n\nChatGPT suggested changing the shape of the data before attempting to visualize it. It did not account for the fact that I need the tidyr package to use the pivot_longer() function, so I loaded that package here. This transformation left me with a data set with 276 observations of 3 variables.\n\n# load packages\nlibrary(tidyr)\n\n# convert shape of the dataset\nconcern_data_long &lt;- concern_data2 %&gt;%\n  pivot_longer(cols = c(very_percent, somewhat_percent, not_very_percent, not_at_all_percent),\n               names_to = \"Concern Level\",\n               values_to = \"Percentage\")\n\nLastly, I attempted to recreate the plot from the original webpage. The first attempt is not bad, but it looks like I have missing data in the “not_at_all_percent” variable which was not the case in the graph on the website.\n\n# attempt to recreate the original plot\ngraph1 &lt;- ggplot(concern_data_long, aes(x = end_date, y = Percentage, color = `Concern Level`)) +\n  geom_line() +\n  labs(title = \"COVID-19 Concern Levels Over Time\",\n       x = \"Date\",\n       y = \"Percentage\",\n       color = \"Concern Level\") +\n  theme_minimal()\ngraph1\n\n\n\n\n\n\n\n\nAfter finding there are 6 missing observations of the Percentage variable, I decided to omit those values because they account for such a minimal percentage of total observations, which fixed the strange gap in the line graph.\n\n# explore for missing data and remove it\nsum(is.na(concern_data_long$Percentage))\n\n[1] 8\n\nconcern_data_long &lt;- na.omit(concern_data_long)\n\n# check that missing data removal fixed the issue\ngraph2 &lt;- ggplot(concern_data_long, aes(x = end_date, y = Percentage, color = `Concern Level`)) +\n  geom_line() +\n  labs(title = \"COVID-19 Concern Levels Over Time\",\n       x = \"Date\",\n       y = \"Percentage\",\n       color = \"Concern Level\") +\n  theme_minimal()\ngraph2\n\n\n\n\n\n\n\n\nI need to make aesthetic changes, so the replicated graph matches the original graph more accurately. The title needs to be changed and centered. I used ChatGPT to create code to separate the title into three lines and change the size of the second title line, so it will fit better. I got a warning message that vectorized input to element_text is not supported in ggplot2, so all three lines of the title are the same size. The original x-axis labels each month, so I created custom labels and dropped the “date” label for the entire axis. While the original graph is interactive, that is slightly out of reach with my coding knowledge right now, so I decided to keep the stagnant legend. I noticed the legend was inverted, so I corrected that. I manually changed the colors of the lines using hex codes.\n\n# create 14 breaks for the x-axis and custom labels for each break\nbreaks &lt;- seq(as.Date(\"2020-02-28\"), by = \"month\", length.out = 14)\ncustom_labels &lt;- c(\"3/1/20\", \"4/1\", \"5/1\", \"6/1\", \"7/1\",\n                   \"8/1\", \"9/1\", \"10/1\", \"11/1\", \"12/1\", \"1/1/21\", \"2/1\", \"3/1\", \"4/1\")\n\n# create updated version of the graph with modifications\ngraph3 &lt;- ggplot(concern_data_long, aes(x = end_date, y = Percentage, color = `Concern Level`)) +\n    geom_line() +\n    scale_color_manual(values = c(\"#800080\",\"#BA55D3\",\"#FFDAB9\",\"#FF0000\")) +\n    labs(title = \"How worried are Americans about infection?\",\n         subtitle = paste(\"How concerned Americans say they are that they, someone in their family or someone else they know will\", \"\\n\", \"become infected with the coronavirus\")) +\n    theme(plot.title = element_text(hjust = 0.5, size = 10), \n          plot.subtitle = element_text(hjust = 0.5, size = 8)) +\n    scale_x_date(breaks = breaks, labels = custom_labels) +\n    xlab(NULL) + \n    guides(color = guide_legend(reverse = TRUE))\ngraph3\n\n\n\n\n\n\n\n\nLastly, I need to add labels on specific dates. I used this simple prompt in ChatGPT: how to add labels to specific dates on a line graph. It suggested adding a geom_text() layer. I had to go back and forth with ChatGPT a couple times to find a date format that worked. Unfortunately, I received a consistent error about the geom_text() layer not being able to find the Percentage variable which prevented it from adding the labels onto my existing graph. I attempted the same format with geom_label() and got the same error. I added an arbitrary Percentage variable to the labels_data just to see if that would fix the error, but it was also unsuccessful. I would appreciate any input on how to solve this error. I commented out the last piece of code for the sake of rendering my website.\n\n# create custom labels to be added as another layer on the original graph \nlabels_data &lt;- data.frame(\n  modeldate = as.Date(c(\"2020-02-29\", \"2020-05-28\", \"2020-10-02\", \"2020-11-07\", \"2021-01-02\")),  \n  label_text = c(\"First US death reported\", \"US deaths surpass 100,000\", \"Trump diagnosed with Covid-19\", \"Biden declared election winner\", \"Biden sworn into office\",\n  Percentage = c(60, 60, 60, 60, 60))\n)\n\n ## graph4 &lt;- graph3 + \n ##  geom_label(data = labels_data, aes(x = modeldate, y=Percentage, label = label_text))\n\nAfter receiving input from a classmate, Erick Mollinedo, I added several geom_vline() and geom_text() functions to add the missing labels. The geom_vline() functions created dashed lines on each of the important dates, which I think solved the previous issue because it provided a location for the new layer to be added to in ggplot(). The geom_text() functions allow the actual descriptions to be added. The labels were a little too large for my graph originally, so I changed the size of each one to make them fit better. This addition brought me much closer to replicating the original graph although my lines appear to be significantly more jagged than the original. I also received a suggestion to use the ggthemes package to make the final replication appear closer to the original by adding the theme_538() function. I received multiple errors about not being able to find this function, so ChatGPT suggested creating my own function to mimic the 538 theme. This change removed the gridlines in the background and adjusted the sizing of the x-axis labels to match the original graph more closely.\n\n# create a function to mimic the 538 style\ntheme_538 &lt;- function() {\n  theme_minimal() +\n    theme(\n      axis.title = element_text(size = 12),\n      axis.text = element_text(size = 10),\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      panel.background = element_blank(),\n      axis.line = element_line(color = \"black\"),\n      axis.ticks = element_line(color = \"black\"),\n      plot.title = element_text(size = 14, hjust = 0.5)\n    )\n}\n\n# apply the 538 style function and add labels to dates of interest\ngraph4 &lt;- ggplot(concern_data_long, aes(x = end_date, y = Percentage, color = `Concern Level`)) +\n    geom_line() +\n    theme_538() +\n    scale_color_manual(values = c(\"#800080\",\"#BA55D3\",\"#FFDAB9\",\"#FF0000\")) +\n    labs(title = \"How worried are Americans about infection?\",\n         subtitle = paste(\"How concerned Americans say they are that they, someone in their family or someone else they know will\", \"\\n\", \"become infected with the coronavirus\")) +\n    theme(plot.title = element_text(hjust = 0.5, size = 10), \n          plot.subtitle = element_text(hjust = 0.5, size = 8)) +\n    scale_x_date(breaks = breaks, labels = custom_labels) +\n    xlab(NULL) + \n    guides(color = guide_legend(reverse = TRUE)) +\n    geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype = \"dashed\") + \n    geom_vline(xintercept = as.Date(\"2020-05-28\"), linetype = \"dashed\") +\n    geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype = \"dashed\") +\n    geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype = \"dashed\") +\n    geom_vline(xintercept = as.Date(\"2021-01-20\"), linetype = \"dashed\") +\n    geom_text(aes(x = as.Date(\"2020-02-29\"), y = 55, label = paste(\"First U.S.\", \"\\n\", \"death reported\")), size = 3, angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") + \n  geom_text(aes(x = as.Date(\"2020-05-28\"), y = 55, label = paste(\"U.S. deaths\", \"\\n\", \"surpass 100,000\")), size = 3, angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2020-10-02\"), y = 48, label = paste(\"Trump diagnosed\", \"\\n\", \"with COVID-19\")), size = 3, angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2020-11-07\"), y = 57, label = paste(\"Biden declared\", \"\\n\", \"election winner\")), size = 3, angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2021-01-20\"), y = 55, label = paste(\"Biden sworn\", \"\\n\", \"into office\")), size = 3, angle = 0, vjust = 0, fontface = \"italic\", color = \"black\")\ngraph4\n\n\n\n\n\n\n\n\nOriginal graph included again for quick comparison. \nSince Dr. Handel recommended that new users begin with the gt package when creating tables, I specifically asked ChatGPT to use that package with this prompt: R code using the gt package to create a table displaying the percentages for four categories (very, somewhat, not very, not at all) for each daily observation. I am using the same data from the exercise above, which is stored as concern_data2.\nThe original code provided by ChatGPT had outdated syntax in the column argument of the tab_spanner() function, so I updated that using the simple c() function. I also had to add the real variable names found in concern_data2.\n\n# load packages\nlibrary(gt)\n\n# generate first attempt at publication quality table\ntable1 &lt;- concern_data2 %&gt;%\n  gt() %&gt;%\n  tab_spanner(\n    label = \"Percentage\",\n    columns = c(very_percent, somewhat_percent, not_very_percent, not_at_all_percent)\n  ) %&gt;%\n  tab_header(\n    title = \"Observations with Percentage Breakdown\"\n  )\ntable1\n\n\n\n\n\n\n\nObservations with Percentage Breakdown\n\n\nend_date\nPercentage\n\n\nvery_percent\nsomewhat_percent\nnot_very_percent\nnot_at_all_percent\n\n\n\n\n2020-02-04\n13.00000\n26.00000\n43.00000\n18.000000\n\n\n2020-02-09\n11.00000\n24.00000\n33.00000\n20.000000\n\n\n2020-02-11\n11.00000\n28.00000\n39.00000\n22.000000\n\n\n2020-02-16\n22.00000\n23.00000\n37.00000\n19.000000\n\n\n2020-02-18\n16.00000\n24.50000\n37.50000\n21.000000\n\n\n2020-02-25\n10.00000\n30.00000\n40.00000\n20.000000\n\n\n2020-02-28\n16.00000\n31.00000\n27.00000\n16.000000\n\n\n2020-03-03\n19.00000\n33.00000\n32.50000\n15.000000\n\n\n2020-03-08\n23.00000\n32.50000\n27.00000\n17.000000\n\n\n2020-03-09\n30.00000\n32.00000\n24.00000\n15.000000\n\n\n2020-03-10\n14.00000\n35.00000\n35.00000\n16.000000\n\n\n2020-03-11\n24.00000\n26.00000\n39.00000\n11.000000\n\n\n2020-03-12\n22.33333\n36.33333\n26.33333\n11.333333\n\n\n2020-03-13\n25.00000\n33.66667\n23.33333\n14.000000\n\n\n2020-03-15\n33.66667\n31.33333\n22.33333\n10.333333\n\n\n2020-03-16\n23.00000\n34.66667\n28.66667\n13.333333\n\n\n2020-03-17\n15.00000\n41.00000\n33.00000\n11.000000\n\n\n2020-03-19\n26.93333\n41.83333\n22.13333\n8.100000\n\n\n2020-03-22\n39.00000\n32.00000\n18.00000\n9.000000\n\n\n2020-03-23\n33.33333\n37.33333\n21.00000\n8.666667\n\n\n2020-03-24\n35.40000\n35.00000\n21.40000\n8.400000\n\n\n2020-03-25\n33.20000\n38.20000\n19.60000\n8.400000\n\n\n2020-03-26\n36.50000\n34.50000\n20.50000\n8.500000\n\n\n2020-03-27\n37.50000\n35.50000\n18.50000\n8.500000\n\n\n2020-03-28\n31.70000\n41.10000\n20.00000\n7.050000\n\n\n2020-03-29\n35.00000\n36.66667\n17.66667\n8.666667\n\n\n2020-03-30\n30.60000\n34.60000\n20.80000\n12.400000\n\n\n2020-03-31\n34.00000\n36.50000\n20.50000\n8.250000\n\n\n2020-04-01\n37.00000\n35.66667\n19.00000\n8.333333\n\n\n2020-04-02\n44.00000\n35.33333\n15.00000\n5.333333\n\n\n2020-04-03\n40.00000\n37.00000\n16.50000\n6.500000\n\n\n2020-04-04\n39.10000\n38.50000\n14.90000\n5.550000\n\n\n2020-04-05\n35.00000\n37.00000\n16.00000\n10.000000\n\n\n2020-04-06\n43.33333\n35.33333\n15.33333\n5.666667\n\n\n2020-04-07\n40.16667\n36.00000\n16.16667\n7.000000\n\n\n2020-04-08\n41.50000\n34.00000\n18.00000\n6.500000\n\n\n2020-04-09\n40.25000\n39.00000\n15.00000\n6.000000\n\n\n2020-04-10\n34.92500\n36.35000\n17.67500\n8.225000\n\n\n2020-04-11\n34.00000\n40.00000\n19.00000\n6.000000\n\n\n2020-04-12\n23.80000\n41.60000\n23.20000\n10.000000\n\n\n2020-04-13\n35.33333\n38.00000\n18.66667\n8.000000\n\n\n2020-04-14\n31.59286\n37.78143\n21.59286\n8.831429\n\n\n2020-04-15\n37.00000\n35.00000\n19.66667\n7.666667\n\n\n2020-04-16\n37.33333\n35.66667\n20.33333\n6.666667\n\n\n2020-04-17\n34.50000\n35.50000\n22.50000\n7.500000\n\n\n2020-04-18\n32.40000\n37.80000\n21.00000\n8.200000\n\n\n2020-04-19\n25.40000\n37.80000\n22.60000\n12.000000\n\n\n2020-04-20\n37.33333\n34.33333\n21.00000\n7.333333\n\n\n2020-04-21\n36.78667\n36.70833\n19.27500\n7.236667\n\n\n2020-04-22\n36.00000\n37.00000\n19.33333\n7.666667\n\n\n2020-04-23\n38.33333\n38.33333\n18.00000\n5.666667\n\n\n2020-04-24\n36.50000\n36.50000\n22.50000\n5.500000\n\n\n2020-04-25\n27.80000\n43.10000\n19.20000\n9.200000\n\n\n2020-04-26\n30.28571\n35.85714\n20.42857\n11.000000\n\n\n2020-04-27\n36.50000\n34.50000\n23.00000\n5.500000\n\n\n2020-04-28\n32.62200\n38.05200\n21.70200\n7.692000\n\n\n2020-04-29\n38.50000\n35.50000\n20.00000\n6.500000\n\n\n2020-04-30\n37.66667\n37.33333\n18.33333\n5.666667\n\n\n2020-05-01\n33.66667\n37.33333\n19.33333\n9.333333\n\n\n2020-05-02\n29.70000\n38.20000\n22.00000\n9.000000\n\n\n2020-05-03\n28.33333\n35.33333\n22.33333\n12.666667\n\n\n2020-05-04\n39.00000\n32.20000\n18.60000\n9.800000\n\n\n2020-05-05\n30.32833\n36.97500\n22.19167\n10.105000\n\n\n2020-05-06\n38.50000\n32.50000\n21.00000\n8.500000\n\n\n2020-05-07\n36.33333\n37.00000\n19.33333\n6.666667\n\n\n2020-05-08\n37.00000\n35.00000\n20.50000\n7.500000\n\n\n2020-05-09\n31.60000\n39.90000\n18.80000\n8.700000\n\n\n2020-05-10\n24.20000\n37.80000\n22.80000\n13.200000\n\n\n2020-05-11\n36.50000\n34.00000\n21.50000\n8.500000\n\n\n2020-05-12\n31.73400\n37.18200\n21.12800\n9.850000\n\n\n2020-05-13\n35.66667\n36.33333\n19.00000\n9.000000\n\n\n2020-05-14\n33.50000\n37.25000\n19.75000\n10.000000\n\n\n2020-05-15\n40.50000\n32.00000\n18.00000\n9.500000\n\n\n2020-05-16\n28.60000\n37.60000\n20.90000\n12.400000\n\n\n2020-05-17\n29.60000\n34.60000\n21.60000\n11.800000\n\n\n2020-05-18\n39.00000\n33.25000\n19.00000\n8.000000\n\n\n2020-05-19\n26.49333\n39.33000\n22.45667\n11.320000\n\n\n2020-05-20\n40.00000\n34.00000\n19.50000\n6.000000\n\n\n2020-05-21\n36.00000\n42.00000\n15.00000\n7.000000\n\n\n2020-05-23\n27.60000\n37.40000\n24.80000\n9.700000\n\n\n2020-05-24\n20.50000\n38.50000\n27.50000\n11.500000\n\n\n2020-05-25\n32.75000\n36.25000\n18.50000\n11.500000\n\n\n2020-05-26\n28.44000\n37.74667\n23.22333\n10.226667\n\n\n2020-05-27\n26.00000\n38.00000\n22.00000\n13.000000\n\n\n2020-05-28\n30.00000\n33.00000\n22.00000\n13.000000\n\n\n2020-05-30\n24.60000\n36.30000\n27.40000\n10.700000\n\n\n2020-05-31\n22.25000\n35.75000\n24.25000\n15.000000\n\n\n2020-06-01\n33.25000\n34.00000\n20.50000\n11.500000\n\n\n2020-06-02\n22.60000\n38.40000\n25.63333\n12.066667\n\n\n2020-06-03\n14.00000\n37.00000\n20.00000\n29.000000\n\n\n2020-06-05\n22.00000\n38.00000\n28.00000\n12.000000\n\n\n2020-06-06\n23.70000\n37.60000\n23.90000\n14.300000\n\n\n2020-06-07\n20.25000\n36.00000\n24.75000\n16.000000\n\n\n2020-06-08\n31.50000\n33.50000\n22.50000\n13.000000\n\n\n2020-06-09\n23.77667\n37.13667\n24.82333\n13.963333\n\n\n2020-06-11\n28.00000\n41.00000\n23.00000\n8.000000\n\n\n2020-06-13\n26.00000\n37.80000\n22.40000\n12.300000\n\n\n2020-06-14\n21.33333\n37.00000\n24.00000\n15.333333\n\n\n2020-06-15\n34.25000\n33.00000\n20.25000\n12.250000\n\n\n2020-06-16\n30.30000\n33.60000\n26.20000\n8.833333\n\n\n2020-06-21\n16.00000\n37.00000\n28.00000\n17.500000\n\n\n2020-06-22\n32.00000\n33.33333\n22.00000\n11.333333\n\n\n2020-06-23\n25.37500\n38.59000\n22.97000\n12.875000\n\n\n2020-06-25\n34.00000\n43.00000\n19.00000\n5.000000\n\n\n2020-06-26\n34.00000\n37.00000\n17.00000\n11.000000\n\n\n2020-06-27\n23.10000\n39.30000\n21.80000\n13.800000\n\n\n2020-06-28\n19.00000\n39.50000\n25.50000\n15.500000\n\n\n2020-06-29\n33.00000\n38.00000\n18.00000\n9.000000\n\n\n2020-06-30\n34.25000\n33.25000\n15.25000\n12.000000\n\n\n2020-07-05\n18.00000\n39.00000\n25.50000\n15.000000\n\n\n2020-07-06\n35.00000\n36.00000\n15.00000\n10.000000\n\n\n2020-07-07\n28.83500\n37.58000\n20.81000\n12.160000\n\n\n2020-07-12\n26.00000\n36.00000\n21.66667\n14.000000\n\n\n2020-07-13\n47.00000\n30.50000\n13.50000\n9.000000\n\n\n2020-07-14\n24.00000\n40.00000\n26.00000\n11.000000\n\n\n2020-07-15\n33.00000\n33.00000\n17.00000\n12.000000\n\n\n2020-07-19\n26.66667\n37.00000\n20.66667\n13.000000\n\n\n2020-07-20\n48.00000\n31.50000\n13.00000\n7.000000\n\n\n2020-07-21\n29.35500\n36.37000\n22.68000\n10.575000\n\n\n2020-07-22\n51.00000\n28.00000\n20.00000\n1.000000\n\n\n2020-07-23\n37.00000\n40.00000\n17.00000\n6.000000\n\n\n2020-07-24\n35.00000\n37.00000\n17.00000\n11.000000\n\n\n2020-07-26\n22.50000\n39.00000\n21.50000\n14.500000\n\n\n2020-07-27\n38.00000\n33.00000\n17.00000\n9.000000\n\n\n2020-07-28\n29.07333\n36.47000\n22.26333\n10.536667\n\n\n2020-08-02\n24.50000\n44.50000\n20.00000\n10.500000\n\n\n2020-08-03\n40.00000\n32.50000\n17.00000\n8.500000\n\n\n2020-08-04\n25.00000\n40.00000\n23.00000\n12.000000\n\n\n2020-08-09\n11.00000\n44.00000\n30.00000\n15.000000\n\n\n2020-08-10\n39.50000\n31.50000\n17.00000\n11.000000\n\n\n2020-08-11\n31.43000\n36.58500\n19.06000\n12.510000\n\n\n2020-08-15\n31.00000\n34.00000\n15.00000\n14.000000\n\n\n2020-08-16\n21.50000\n40.00000\n22.50000\n14.000000\n\n\n2020-08-17\n36.00000\n34.00000\n18.00000\n7.000000\n\n\n2020-08-18\n37.50000\n33.00000\n21.50000\n8.000000\n\n\n2020-08-19\n42.00000\n31.00000\n17.00000\n9.000000\n\n\n2020-08-21\n32.00000\n36.00000\n20.00000\n12.000000\n\n\n2020-08-22\n37.00000\n33.00000\n19.00000\n8.000000\n\n\n2020-08-23\n25.00000\n36.00000\n17.00000\n16.000000\n\n\n2020-08-25\n21.00000\n37.00000\n28.00000\n14.000000\n\n\n2020-08-28\n37.00000\n35.00000\n17.00000\n10.000000\n\n\n2020-08-29\n31.00000\n37.00000\n19.00000\n10.000000\n\n\n2020-08-30\n19.00000\n38.00000\n25.50000\n16.000000\n\n\n2020-08-31\n33.75000\n34.00000\n21.50000\n10.500000\n\n\n2020-09-01\n25.31500\n36.95000\n24.08000\n13.205000\n\n\n2020-09-04\n30.00000\n38.00000\n21.00000\n11.000000\n\n\n2020-09-06\n27.00000\n35.00000\n18.00000\n17.000000\n\n\n2020-09-08\n34.00000\n32.00000\n19.50000\n14.000000\n\n\n2020-09-13\n27.00000\n33.00000\n20.00000\n16.000000\n\n\n2020-09-14\n35.75000\n32.00000\n21.75000\n9.250000\n\n\n2020-09-15\n27.27500\n36.37000\n24.10500\n12.225000\n\n\n2020-09-19\n29.00000\n43.00000\n19.00000\n10.000000\n\n\n2020-09-20\n22.00000\n36.00000\n20.00000\n17.000000\n\n\n2020-09-21\n36.00000\n32.50000\n20.50000\n11.000000\n\n\n2020-09-22\n24.00000\n36.00000\n26.00000\n14.000000\n\n\n2020-09-23\n22.00000\n42.00000\n22.00000\n13.000000\n\n\n2020-09-24\n29.00000\n33.00000\n18.00000\n15.000000\n\n\n2020-09-25\n49.00000\n26.00000\n24.00000\n1.000000\n\n\n2020-09-27\n26.00000\n33.00000\n21.00000\n16.000000\n\n\n2020-09-30\n20.00000\n41.00000\n25.00000\n15.000000\n\n\n2020-10-02\n20.00000\n35.50000\n27.00000\n17.500000\n\n\n2020-10-03\n27.66667\n40.33333\n21.33333\n10.333333\n\n\n2020-10-04\n24.00000\n36.00000\n20.00000\n15.000000\n\n\n2020-10-06\n39.47000\n32.58000\n16.97000\n10.690000\n\n\n2020-10-09\n29.00000\n36.00000\n15.00000\n12.000000\n\n\n2020-10-11\n23.00000\n39.66667\n19.33333\n15.333333\n\n\n2020-10-12\n38.50000\n31.50000\n18.00000\n10.000000\n\n\n2020-10-13\n21.00000\n42.00000\n24.00000\n14.000000\n\n\n2020-10-18\n25.00000\n38.00000\n21.00000\n14.666667\n\n\n2020-10-20\n27.48333\n37.77333\n19.91000\n14.576667\n\n\n2020-10-22\n49.00000\n27.00000\n23.00000\nNA\n\n\n2020-10-24\n36.00000\n41.00000\n15.00000\n7.000000\n\n\n2020-10-25\n28.00000\n35.00000\n18.00000\n14.000000\n\n\n2020-10-27\n28.33333\n36.00000\n22.00000\n13.000000\n\n\n2020-11-01\n30.00000\n35.50000\n19.50000\n14.500000\n\n\n2020-11-02\n24.00000\n38.00000\n24.00000\n14.000000\n\n\n2020-11-08\n25.00000\n35.00000\n17.00000\n17.000000\n\n\n2020-11-10\n23.00000\n36.00000\n23.00000\n18.000000\n\n\n2020-11-15\n25.00000\n34.00000\n16.00000\n16.000000\n\n\n2020-11-16\n38.00000\n31.00000\n17.00000\n12.000000\n\n\n2020-11-20\n47.00000\n28.00000\n23.00000\nNA\n\n\n2020-11-22\n36.00000\n39.00000\n17.00000\n8.000000\n\n\n2020-11-24\n25.00000\n38.00000\n23.00000\n14.000000\n\n\n2020-11-29\n26.00000\n31.00000\n18.00000\n17.000000\n\n\n2020-12-07\n44.50000\n32.50000\n14.50000\n9.000000\n\n\n2020-12-08\n27.50000\n37.50000\n20.50000\n13.000000\n\n\n2020-12-13\n25.00000\n34.00000\n17.00000\n18.000000\n\n\n2020-12-15\n30.75500\n36.20000\n20.60500\n11.770000\n\n\n2020-12-17\n45.00000\n28.00000\n26.00000\nNA\n\n\n2020-12-21\n47.00000\n33.00000\n13.00000\n5.000000\n\n\n2020-12-22\n26.00000\n35.00000\n25.00000\n14.000000\n\n\n2021-01-03\n25.00000\n33.00000\n17.00000\n15.000000\n\n\n2021-01-12\n22.00000\n40.00000\n22.00000\n16.000000\n\n\n2021-01-13\n31.54500\n33.77500\n14.46000\n14.390000\n\n\n2021-01-15\n34.00000\n35.00000\n19.00000\n12.000000\n\n\n2021-01-17\n28.00000\n32.00000\n16.00000\n15.000000\n\n\n2021-01-19\n22.00000\n40.00000\n21.00000\n16.000000\n\n\n2021-01-24\n60.00000\n19.00000\n12.00000\n7.000000\n\n\n2021-01-25\n19.00000\n41.00000\n23.00000\n9.000000\n\n\n2021-01-26\n52.00000\n25.00000\n20.00000\nNA\n\n\n2021-01-31\n22.00000\n33.00000\n18.00000\n16.000000\n\n\n2021-02-02\n29.40500\n34.42000\n21.57500\n12.500000\n\n\n2021-02-07\n32.00000\n33.00000\n19.00000\n11.000000\n\n\n2021-02-09\n26.00000\n35.00000\n23.00000\n16.000000\n\n\n2021-02-14\n25.00000\n33.00000\n16.00000\n15.000000\n\n\n2021-02-15\n35.00000\n34.00000\n14.00000\n11.000000\n\n\n2021-02-16\n24.00000\n37.00000\n24.00000\n16.000000\n\n\n2021-02-18\n50.00000\n29.00000\n20.00000\nNA\n\n\n2021-02-22\n23.00000\n35.00000\n25.00000\n17.000000\n\n\n2021-02-28\n22.00000\n32.00000\n17.00000\n17.000000\n\n\n2021-03-01\n35.50000\n32.00000\n19.00000\n12.500000\n\n\n2021-03-02\n27.08500\n34.20000\n23.23500\n14.280000\n\n\n2021-03-06\n27.00000\n45.00000\n20.00000\n8.000000\n\n\n2021-03-09\n22.00000\n35.00000\n25.00000\n18.000000\n\n\n2021-03-13\n29.00000\n39.00000\n21.00000\n11.000000\n\n\n2021-03-14\n41.00000\nNA\n28.50000\nNA\n\n\n2021-03-18\n22.00000\n38.00000\n28.00000\n12.000000\n\n\n2021-03-21\n35.00000\n28.00000\n32.00000\nNA\n\n\n2021-03-23\n18.00000\n34.00000\n30.00000\n17.000000\n\n\n2021-03-28\n20.00000\n32.00000\n21.00000\n19.000000\n\n\n2021-03-29\n29.00000\n33.33333\n24.66667\n13.333333\n\n\n2021-03-30\n21.33333\n34.66667\n27.00000\n17.333333\n\n\n2021-04-06\n20.17000\n33.08000\n26.82500\n18.125000\n\n\n2021-04-11\n20.00000\n30.00000\n22.00000\n20.000000\n\n\n2021-04-13\n16.00000\n34.00000\n29.00000\n20.000000\n\n\n2021-04-20\n20.00000\n31.00000\n31.00000\n17.000000\n\n\n\n\n\n\n\nThe first table looks decent, but I want to clean up a few things stylistically. I added a more descriptive title and renamed the columns, so the table would not contain variable names. Following my classmate Erick’s suggestion, I created a new variable, month to summarize the data in the original table to make it more useful. I felt like it was unnecessary to have 5 decimal points for each percentage, so I limited the decimals to 1 place when I created the new average percentages by month. The result is a clean and publication-quality table that is much easier to digest than the original.\n\n# load packages required to manipulate dates\nlibrary(lubridate)\n\n# create new month variable to summarize the table and rename columns\ncovid_summary &lt;- concern_data2 %&gt;%\n  mutate(month = floor_date(as.Date(end_date, format = \"%Y-%m-%d\"), \"month\")) %&gt;% \n  group_by(month) %&gt;% #Group by month of the year\n  summarise(avg_very_percent = round(mean(very_percent, na.rm = TRUE), 1),\n            avg_somewhat_percent = round(mean(somewhat_percent, na.rm = TRUE), 1),\n            avg_not_very_percent = round(mean(not_very_percent, na.rm = TRUE), 1),\n            avg_not_at_all_percent = round(mean(not_at_all_percent, na.rm = TRUE), 1)) %&gt;%\n  mutate(across(starts_with(\"avg_\"), ~ as.numeric(format(., nsmall = 2)))) %&gt;% \n  rename(Month = \"month\", \n         Very = \"avg_very_percent\",\n         Somewhat = \"avg_somewhat_percent\",\n         `Not very` = \"avg_not_very_percent\",\n         `Not at all` = \"avg_not_at_all_percent\")\n\n## generate table with proper labels\ntable2 &lt;- covid_summary %&gt;% \n  gt() %&gt;%\n  tab_spanner(label = \"Percentage\",\n              columns = c(\"Very\", \"Somewhat\", \"Not very\", \"Not at all\")) %&gt;%\n  tab_header(title = \"Levels of concern about COVID-19 infections among Americans\") %&gt;% \n  fmt_number(columns = c(\"Very\", \"Somewhat\", \"Not very\", \"Not at all\"),\n             decimals = 1)\ntable2\n\n\n\n\n\n\n\nLevels of concern about COVID-19 infections among Americans\n\n\nMonth\nPercentage\n\n\nVery\nSomewhat\nNot very\nNot at all\n\n\n\n\n2020-02-01\n14.1\n26.6\n36.6\n19.4\n\n\n2020-03-01\n28.7\n35.2\n24.3\n10.9\n\n\n2020-04-01\n35.8\n37.0\n19.2\n7.4\n\n\n2020-05-01\n31.7\n36.3\n21.2\n10.1\n\n\n2020-06-01\n26.4\n37.0\n22.5\n13.0\n\n\n2020-07-01\n33.0\n35.7\n19.4\n10.4\n\n\n2020-08-01\n30.3\n36.2\n20.4\n11.5\n\n\n2020-09-01\n29.0\n35.4\n21.5\n12.8\n\n\n2020-10-01\n29.7\n36.3\n20.0\n12.9\n\n\n2020-11-01\n29.9\n34.5\n19.8\n14.5\n\n\n2020-12-01\n35.1\n33.7\n19.5\n11.8\n\n\n2021-01-01\n31.6\n33.2\n18.2\n13.4\n\n\n2021-02-01\n29.6\n33.6\n20.0\n14.4\n\n\n2021-03-01\n27.2\n35.0\n25.0\n14.2\n\n\n2021-04-01\n19.0\n32.0\n27.2\n18.8"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\nFigure 2 shows a boxplot figure produced by one of the R scripts. Each boxplot represents the distribution of height for a specific race. The average height is highest among Black participants, and average height seems to be approximately the same across the other three races included, which are White, American Indian and Alaska Native, and Asian. The boxplot for American Indian and Alaska Native is not helpful because there is only one observation for this racial category, so there is no additional data to display besides the height of the one AIAN observation.\n\n\n\n\n\nFigure 2: Height statified by race.\n\n\n\n\nFigure 3 shows a scatterplot figure produced by one of the R scripts. The scatterplot has been fitted with a line that displays the relationship between the two numerical variables along with a shaded area that represents the confidence interval around the estimates. As weight increases across the x-axis, age decreases. This observation is counter intuitive because most people gain weight as they age, which suggests there could be an issue with the data.\n\n\n\n\n\nFigure 3: Plot of age by weight."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\nExample Table 3 shows a summary of another linear model fit with Height as the outcome with Race and Age as predictors. The table displays an estimate for each component of the linear model, along with the standard error, test statistic, and p-value for the estimate to provide context for its usefulness. The p-values for each variable are extremely large, which shows we fail to reject the null hypothesis that each of these variables are not statistically significantly associated with height according to this dataset.\n\n\n\n\nTable 3: Second linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n167.5410539\n20.2483086\n8.2743234\n0.0011643\n\n\nRaceAIAN\n-0.5460615\n26.1275182\n-0.0208999\n0.9843265\n\n\nRaceB\n-1.1524256\n18.6903702\n-0.0616588\n0.9537925\n\n\nRaceW\n-2.0799817\n20.6436086\n-0.1007567\n0.9245919\n\n\nAge\n-0.0226135\n0.4010505\n-0.0563856\n0.9577388"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "",
    "text": "Brian McKay1, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nMark Ebell, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nAriella Perry Dale, Colorado Department of Public Health and Environment, Denver, CO, USA\nYe Shen, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nAndreas Handel1, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\n\n1 Corresponding Authors: Brian McKay and Andreas Handel\nAddress: 101 Buck Rd, Miller Hall, Athens, Georgia 30606\nEmail: bmckay52@uga.edu or ahandel@uga.edu"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#data-collection",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#data-collection",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Data Collection",
    "text": "Data Collection\nStudents with a primary complaint related to a respiratory infection who made an appointment at the health center of a large research university from December 2016 to February 2017 filled out an electronic questionnaire (see [27] for more details). The questionnaire collected data about their current symptoms and activity level. A response was required for all symptom-related questions when they scheduled their appointments. We included all symptoms collected by the questionnaire in this analysis. The complete questionnaire is available in the supplementary material.\nFor the symptoms of cough, weakness, and body aches, the patient graded the severity of the symptom as none, mild, moderate, and severe. The patient recorded all other symptom data as present or absent. The patient also reported any changes in their normal behavior. Patients describe their activity level as a number between 0 and 10, with 10 indicating no change in regular activity and 0 being bedridden.\nThe study population includes all patients with a diagnosis of influenza. The data and results presented in the main text includes patients diagnosed with a rapid antigen or rapid PCR test. To address the impact of the influenza diagnosis method, we performed the same analyses for all patients diagnosed with influenza regardless of the method used. These results are shown in the supplementary material.\nThe institutional review board approved the study protocol."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#data-cleaning",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#data-cleaning",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nWe cleaned the data to format the variables and to check for variables with potential errors or missing entries. During the cleaning process, we removed uninformative variables, which we defined as any symptoms found to occur in less than 5% of patients. The symptoms of blurred vision and hearing loss both had a prevalence of less than 5%, so they were not considered for further analysis. To allow easy comparison of morbidity symptoms, we dichotomized weakness and body aches to “absent” or “present”. Patients reported cough as present or absent as well as severity, but only cough absent or present was considered for the main analysis."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#analysis",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#analysis",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Analysis",
    "text": "Analysis\nWe assessed the univariate relationships between activity and each symptom using linear regression treating activity level as a continuous variable. We also performed multiple linear regression. We determined the variables to include in our final model with a sequential forward floating selection, minimizing the root mean square error (RMSE) on test data through a 5-fold cross-validation (20 times repeated) [28].\nNext, we constructed two cumulative scores, one for overall infectiousness and one for overall morbidity. To that end, we divided all symptoms into those related to infectiousness and those related to morbidity (SM Table 1). We defined morbidity symptoms as symptoms that influence overall feelings of well-being but are not associated with infectiousness. Infectiousness symptoms are any symptoms that could plausibly contribute to passing the virus from an infected host to a susceptible host [29,30]. Importantly, the grouping of variables into either the morbidity or infectiousness symptom categories was based on a priori medical and biologic considerations, independently of any observed correlation with activity level. Doing so prevents any circular reasoning since only including symptoms correlated with activity would, of course, generate a score which would match the impact on activity level. These scores are similar to systemic and respiratory scores used in past studies of influenza infection [29,31,32]. The scores are computed as a sum of the symptoms that are present. Our data did not allow us to take into account symptom severity, though a comparison of our scores with cough, weakness, and body aches, for which we have severity, shows that there is a good positive correlation between strength and number of symptoms (see SM). To test the robustness of our results, we completed several sensitivity analyses using different approaches to generate the scores. Results from these analyses are shown in the SM.\nCorrelations between the infectiousness score, morbidity score, and activity were assessed using Spearman correlation [33] and the generalized Mantel–Haenszel procedure [34]. Linear regression is used to estimate the average change in activity, and the lines are included in the plots to help visualize the relationships. All analyses were completed using R (version 3.5.3) [35]. We used the mlr package for cross-validation [36], vcdExtra to compute Yule’s Q and the CHM trend test [37], DescTools to compute Spearman’s rank correlation coefficient and corresponding confidence intervals [38]. All of the code and data required to reproduce the results are available through Dryad."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#study-population",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#study-population",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Study population",
    "text": "Study population\nDuring the study period, 2380 patients had a respiratory complaint and filled out the questionnaire. Among those, 324 had a lab-based diagnosis of influenza (PCR or rapid antigen). The following analyses focus on those patients since they are most likely infected with influenza. For analyses of patients who received a flu diagnosis based on lab tests or empirically from a physician, see the SM. Patients with influenza reported activity levels ranging from 0 to 10 with a median of 4 (SM Figure 1). All of the patients reported symptoms, with only 14% reporting 9 or fewer (max possible 25). The most common symptoms were coughing and weakness while the least common was abdominal pain (SM Table 1)."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#univariate-and-subset-selection",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#univariate-and-subset-selection",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Univariate and subset selection",
    "text": "Univariate and subset selection\nWe assessed correlations between activity level and each symptom in a univariate linear analysis (SM Table 2). All statistically significant symptoms had a negative correlation with activity level (SM Table 2). Next, we considered a multi-variable regression model and performed variable selection based on cross-validated minimization of RMSE. We found that the best performing model was one that included chills/sweats, subjective fever, headache, weakness, sleeplessness, and vomiting (SM Table 2). While vomiting is not a common symptom of influenza, in those patients who did report vomiting lead to major reductions in their activity."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#computation-of-infectiousness-and-morbidity-scores",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#computation-of-infectiousness-and-morbidity-scores",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Computation of infectiousness and morbidity scores",
    "text": "Computation of infectiousness and morbidity scores\nFor the main analysis, we classified 5 symptoms as infectiousness related: coughing, sneezing, runny nose, nasal congestion, and chest congestion. 20 symptoms were classified as morbidity related: subjective fever, having chills and or sweats, body aches, weakness, headache, fatigue, sleeplessness, breathlessness, wheezing, chest pain, sore throat, abdominal pain, diarrhea, nausea, vomiting, ear pain, tooth pain, eye pain, itchy eyes, and swollen lymph nodes. Each symptom present in a patient contributed one point to their respective scores. For those symptoms for which we had severity, we investigated correlations with number of symptoms and found total number of symptoms to be a good proxy (see SM). Analyses using several alternative approaches for computing the score are shown in the SM and summarized below in the Sensitivity Analysis section.\nThe infectiousness score had a possible range of 0 to 5, and the morbidity score had a possible range of 0 to 20. The median infectiousness score was 4. Only 2 patients had an infectiousness score of 0, 20% had a score of 2 or less, and 28% of patients had the maximum possible score of 5. The mean morbidity score was 9.574, and no patients had a morbidity score of 0, 1, 19 or 20. The centered distribution is assumed to be the result of how patients were included in the study that is all the patients felt sick enough to seek medical care, but none were sick enough to require urgent care or hospitalization. Results presented in the SM show plots of the score distributions (SM Figure 2 A-B)."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#impact-of-infectiousness-score-on-activity",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#impact-of-infectiousness-score-on-activity",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Impact of infectiousness score on activity",
    "text": "Impact of infectiousness score on activity\nAnalysis of the association between the infectiousness score and the patient’s self-reported activity level suggests that the value of this score has a small impact on the activity level of a patient, with higher infectiousness correlating with reduced activity. Spearman’s rank correlation indicates a negative relationship (\\(r=\\) -0.15 (95% CI: -0.25, -0.04)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 6.363, \\(df =\\) 1, \\(p\\) 0.01) (Figure @ref(fig:LabMIvAFig)A). Note however that the data suggest a non-linear relationship between infectiousness and activity. We cannot think of a biological mechanism that might lead to this pattern. Given that the observed negative trend is small and doesn’t show a monotone decline, it is most reasonable to assume based on this data that there is no meaningful relationship between infectiousness score and activity level. Results presented in the SM show that the overall pattern remains the same if details of the analysis approach are changed (SM Figures 6-8B, 13)."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#impact-of-morbidity-score-on-activity",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#impact-of-morbidity-score-on-activity",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Impact of morbidity score on activity",
    "text": "Impact of morbidity score on activity\nAnalysis of the association between the morbidity score and the patient’s self-reported activity level suggests that higher morbidity score is associated with reduced activity levels. Spearman’s rank correlation indicates negative relationship (\\(r=\\) -0.32 (95% CI: -0.42, -0.22)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 38.577, \\(df =\\) 1, \\(p\\) &lt; 0.01) (Figure @ref(fig:LabMIvAFig)B). The observed pattern is consistent and clear, with a mean 4.7-fold reduction in activity level going from the lowest to the highest morbidity score. The strong negative relationship is preserved if details of the analysis approach are changed results are shown in SM (SM Figures 9-10B, 14)."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#impact-of-morbidity-score-on-infectiousness-score",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#impact-of-morbidity-score-on-infectiousness-score",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Impact of morbidity score on infectiousness score",
    "text": "Impact of morbidity score on infectiousness score\nAnalysis of the relationship between the morbidity and infectiousness scores show a positive correlation. Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.28 (95% CI: 0.18, 0.38)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 26.093, \\(df =\\) 1, \\(p\\) &lt; 0.01) (Figure @ref(fig:LabMIvAFig)C). Apart from the mean activity levels for very low morbidity score values (with very small sample sizes), the pattern is consistent and clear, with a mean 1.8-fold increase in the infectiousness score going from the lowest to the highest morbidity score. A positive relationship is observed regardless of how the infectiousness or morbidity scores are calculated (SM Figures 6-10C) and in the analysis of empirically-diagnosed patients (SM Figure 15).\n\n\n\n\n\nFor all plots the red diamonds indicate the mean and the solid blue line is the linear regression fit. The shaded area is the 95% confidence interval for the linear regression. (A) Activity level for each level of the infectiousness score. (B) Activity level for each level of the morbidity score. There are no patients with a score of morbidity 0, 1, 19, and 20. (C) Infectiousness score for each level of the morbidity score."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#sensitivity-analysis",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#sensitivity-analysis",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Sensitivity analysis",
    "text": "Sensitivity analysis\nWe performed several sensitivity analyses of our results. In one type of analysis, we computed both the morbidity and infectiousness scores in different ways and showed that overall results remain the same. In a different analysis, we considered all patients diagnosed with influenza, not just those that had a positive lab test. Again, overall results remained robust. Details of all these analyses are presented in the SM."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#conceptualizing-our-results",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/6 Manuscript/Manuscript.html#conceptualizing-our-results",
    "title": "Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of influenza patients",
    "section": "Conceptualizing our results",
    "text": "Conceptualizing our results\nWe can place our data into the conceptual framework introduced in the introduction with the morbidity score as a proxy of virulence, \\(v\\), the infectiousness score as a proxy of per-contact transmission potential, \\(p\\), and patient-reported activity as a proxy for the contact rate, \\(c\\) (Figure @ref(fig:HypoFigDATA)). Since our data is measured in units with indirect and uncertain mapping to actual per-contact transmission potential and actual contact rate, we standardize the data and manually place it on top of the conceptual lines. This should not be considered a quantitative mapping.\nOur study population consisted of individuals who felt sick enough to seek medical care, but none were ill enough to require emergency care. It is thus a reasonable assumption to expect them to be somewhere in the middle of the virulence spectrum. Based on our data, this range is characterized by infectiousness levels that are increasing as morbidity (virulence) increases (Figure @ref(fig:HypoFigDATA)). Activity is more strongly impacted and decreases as morbidity increases. We speculate that a study population that included asymptomatic and mildly symptomatic infected persons would be on the left side of our data, while severely ill and hospitalized individuals would fall to the right side of our data.\n\n\n\n\n\nTheoretical framework and data for virulence mediated transmission trade-off. Morbidity score is a proxy for virulence, infectiousness score is a proxy for per-contact transmission potential, and activity level is a proxy for contact rate. The values for infectiousness and activity are re-scaled to allow better visualization. Lines are adjusted to pass through data. Thus, this figure does not show a fit but instead a conceptual framework in which our data can be placed and interpreted."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/taylorglass/Documents/MADA /taylorglass-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/taylorglass/Documents/MADA /taylorglass-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/taylorglass/Documents/MADA /taylorglass-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                         `Allowed Values`\n  &lt;chr&gt;           &lt;chr&gt;                                         &lt;chr&gt;           \n1 Height          height in centimeters                         numeric value &gt;…\n2 Weight          weight in kilograms                           numeric value &gt;…\n3 Gender          identified gender (male/female/other)         M/F/O/NA        \n4 Race            Identified  race (black/white/American India… B/W/AIAN/A      \n5 Age             age in years                                  numeric value &gt;…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n$ Race   &lt;chr&gt; \"B\", \"W\", \"AIAN\", \"A\", \"B\", \"B\", \"A\", \"AIAN\", \"W\", \"W\", \"W\", \"B…\n$ Age    &lt;dbl&gt; 16, 23, 19, 45, 56, 77, 20, 44, 32, 21, 39, 47, 51, 62\n\nsummary(rawdata)\n\n    Height              Weight          Gender              Race          \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n      Age       \n Min.   :16.00  \n 1st Qu.:21.50  \n Median :41.50  \n Mean   :39.43  \n 3rd Qu.:50.00  \n Max.   :77.00  \n                \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender Race    Age\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 180        80 M      B        16\n2 175        70 O      W        23\n3 sixty      60 F      AIAN     19\n4 178        76 F      A        45\n5 192        90 NA     B        56\n6 6          55 F      B        77\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n1\n4\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.0\n70.0\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n39.43\n18.50\n16\n21.5\n41.5\n50\n77\n▇▃▆▃▂\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n1\n4\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n41.00\n18.25\n16\n23.00\n44\n51\n77\n▇▃▇▃▂\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n1\n4\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n41.00\n18.25\n16\n23.00\n44\n51\n77\n▇▃▇▃▂\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nRace\n0\n1\n1\n4\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n43.00\n18.84\n16\n27.5\n45\n53.5\n77\n▆▂▇▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nRace\n0\n1\n1\n4\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n43.00\n18.84\n16\n27.5\n45\n53.5\n77\n▆▂▇▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nRace\n0\n1\n1\n4\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n42.78\n20.19\n16\n23\n45\n51\n77\n▆▁▇▂▂\n\n\n\n\n\n Additional cleaning on updated raw data. Age variable treated as numeric (good). No glaring issues with values. Notice variable Race is categorical, but is treated as character by R. Let’s fix that. \n\nd4$Race &lt;- as.factor(d4$Race)  \nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nRace\n0\n1\nFALSE\n4\nA: 3, B: 3, W: 2, AIA: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n42.78\n20.19\n16\n23\n45\n51\n77\n▆▁▇▂▂\n\n\n\n\n\n Race variable is now treated as categorical (good). No glaring issues with values. \nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-predictions-and-uncertainty",
    "href": "fitting-exercise/fitting-exercise.html#model-predictions-and-uncertainty",
    "title": "Fitting Exercise",
    "section": "Model Predictions and Uncertainty",
    "text": "Model Predictions and Uncertainty\nUsually, linear models lend themselves to nice metrics which demosntrate the uncertainty in their estimates of the parameters. More comples and nonlinear models, however, often aren’t so generous. Another way to estimate uncertainty, however, is the “manual” or “brute-force” sort of way. Is it the most elegant? Ask a statistician. Does it work well? Absolutely - most of the time. This method is called “bootstrapping.”\nWe are going to give it a try on our linear model here, to provide us a nice framework for how bootstrapping works and what it tell us.\nThe logic behind bootstrapping is simple - we take a certain number of sample of the data, fit our models to the data, and examine the distributions of the estimated parameters. It’s somewhat similar to cross-validation, expect that cross-validation splits the entire dataset into k-subsets and trains/predicts models on each combination of \\(k-1\\) subsets to train and \\(k\\) subset to predict, while bootstrapping fits estimates to n randomly generated subsets (which can overlap). Thus, cross validation shows us how well a model can perform on unseen data, while boostrapping does not make predictions on unseen data and rather provides an estimate of how widely parameters estimated from subsets of the same data vary. (CV is good for model performance; bootstrapping is good for uncertainty. Obviously, the two goals are not mutually exclusive.)\nAnd now that I’ve written all of this I see that Andreas also include a much nicer and briefer explanation of the difference on his MADA assignment page. Ah, c’est la vie.\nAll that said, let’s go ahead an bootstrap our multiple regression model. We will be using hte bootstraps function from the rsample package.\n\n# we are taking random samples again, so lets set our seed\nset.seed(rngseed)\n\n# create 100 bootstraps from the training data\nbootstraps = train_data %&gt;%\n  rsample::bootstraps(times = 100)\n\n# estimating multiple regression model parameters from each of these bootstraps, and generating predictions for the original training data\n# define your model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\n# creating function to fit our bootstrap to multiple regression model\nmulti_reg.preds  = function(i){ # i is the index of our bootstrap\n  bs = rsample::analysis(bootstraps$splits[[i]])\n  mod_preds = lm_spec %&gt;% fit(Y ~ ., data = bs) %&gt;% predict(new_data = train_data) %&gt;% select(.pred) %&gt;% as.vector() %&gt;% unlist()\n  \n  return(mod_preds)\n}\n\n# create an array in which to store \nbs_preds = matrix(ncol = nrow(bootstraps), nrow = nrow(train_data))\n\n# generate predictions\nfor(i in 1:nrow(bootstraps)){\n  bs_preds[,i] = multi_reg.preds(i)\n}\n\n# median and 89% confidence intervals\npreds &lt;- bs_preds %&gt;% apply(1, quantile,  c(0.055, 0.5, 0.945)) %&gt;%  t() %&gt;% cbind(\"observed_Y\" = train_data$Y) %&gt;% as.data.frame() \ncolnames(preds) = c(\"lower\", \"median\", \"upper\", \"observed_Y\")\n\n# I might do something with bootstrap predictions late, so don't mind me here\nfor_plot = as.data.frame(cbind(train_data$Y, trainpred2, bs_preds))\ncolnames(for_plot) = c(\"observed_Y\", \"all_Y\", paste0(\"bs_\", 1:nrow(bootstraps)))\n\n# finally, plot!\nplot = ggplot() + geom_point(data = for_plot, aes(x = observed_Y, y = all_Y)) +\n  geom_point(data = preds, aes(x = observed_Y, y = median), col = \"blue\") +\n  geom_errorbar(data = preds, aes(ymin = lower, ymax = upper, x = observed_Y), color = \"blue\") +\n  geom_abline(linetype = \"dashed\", col = \"red\") + \n  xlim(0, 5000) +\n  ylim(0,5000) +\n  labs(x = \"Observed Y\", y = \"Model Prediction\", title = \"Uncertainty in multiple regrerssion model predictions of outcome Y\")\n\n# Save Figure\nfigure_file = here(\"fitting-exercise\", \"mod2_bootstrap_scatter.png\")\nggsave(filename = figure_file, plot=plot) \n\nSaving 7 x 5 in image\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nFigure 3: Correlation between multiple regression model residuals and predicted outcome. Blue points show median prediction from bootstrap with 100 samples, blue lines show 89% confidence interval in model prediction.\n\n\n\n\nWe can see from Figure 3 that our 89% confidence intervals for the predictions which fall off the 1:1 correlation with observation line do not intersect this line; we can be fairly confident that at high Y observations, our model underestimates the value of Y. There may thus be some underlying non-linear relationship between our predictors and the outcome variable Y which would result in a greater value of Y than is predicted linearly as Y gets larger."
  }
]